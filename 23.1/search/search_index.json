{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"<p>Nextflow Tower is the centralized command post for the management of Nextflow data pipelines. It brings monitoring, logging, and observability to distributed workflows and simplifies the deployment of pipelines on any cloud, cluster, or laptop.</p> <p>Users can launch pre-configured pipelines with ease, while the flexible API provides programmatic integration to meet the needs of organizations building on Nextflow Tower. Workflow developers can publish pipelines to shared workspaces and administrators can set up and manage the infrastructure required to run data analysis at scale.</p> <p></p>","title":"Home"},{"location":"#what-is-nextflow","text":"<p>Nextflow is a framework for the development of data workflows. It enables engineers and data scientists to create and securely deploy custom, parallel data applications to the cloud or traditional on-premises infrastructure. Nextflow is characterized by its powerful dataflow programming paradigm and execution engines that allow for transparent deployment.</p> <p>Nextflow is both a programming workflow language and an execution runtime that supports a wide range of execution platforms, including popular traditional grid scheduling systems such as Slurm and IBM LSF, and cloud services such as AWS Batch and Google Cloud Life Sciences.</p> <p></p>","title":"What is Nextflow?"},{"location":"#why-nextflow-tower","text":"<p>We created Nextflow in 2013 to deliver the most seamless experience for executing data workflows at scale. Tower is the continuation of that mission. Using the latest technologies, we have built the solution to easily execute and monitor pipelines across every stage. Tower brings the cloud closer than ever before with automated resource provisioning and role-based access control (RBAC).</p> <p>Tower is designed to be easily configurable in any environment \u2014 data and compute never leave your organization's security boundary. It has been extensively tested with over 500 million jobs, achieving 99.99% uptime.</p> <p>As mandated by healthcare industries to ensure compliance, the Tower platform is regularly submitted to penetration tests and security scanning. These tests meet the compliance standards set by ISO-27001, HIPAA, and HITRUST.</p>   <p>Tip</p> <p>Sign up to try Tower for free, or request a demo for deployments in your own on-premises or cloud environment.</p>","title":"Why Nextflow Tower?"},{"location":"_todo/","text":"<p>FAQ TO DOs:</p>","title":"todo"},{"location":"_todo/#general","text":"<ul> <li>$TOWER_AGENT_WORKDIR</li> <li>What does <code>NXF_PLUGINS_DEFAULT</code> environment variable do?</li> <li>Where is the analysis running?</li> <li>What about security of my data?</li> <li>Identity via LDAP/Active Directory</li> <li>Difference between free and paid Tower?</li> <li>Can I have Service-Account-type and Agent-type credentials in the same Workspace?<ul> <li>Not right now. Must choose. https://github.com/seqeralabs/nf-tower-cloud/issues/2879#issuecomment-1072646557</li> </ul> </li> </ul>","title":"GENERAL"},{"location":"_todo/#amazon","text":"<ul> <li>Can I have Nextflow automatically retry Tasks that fail due to Spot instance reclamation?</li> <li> <p>Can I have Nextflow automatically retry Tasks that fail due to Spot instance reclamation? </p> <p>Yes. As of Tower version ?????, any Spot-based AWS Batch Compute Environment created by Tower Forge will be automatically configured to retry each process 3 times. ??? If a retry policy is not defined???</p> <p>Given that Spots can be reclaimed during the execution of a job, it is a recommended practice that pipeline authors always include retry logic in their logic. ???See HERE for examples??? https://github.com/seqeralabs/nf-tower-cloud/pull/2820/files</p> <ul> <li>Why won't Secrets work with my legacy Tower Forge-created AWS Batch Compute Environment?</li> <li>Why won't Secrets work with my legacy Tower Forge-created AWS Batch Compute Environment? </li> </ul> <p>The Secrets feature requires new permissions to be added to existing IAM Roles: * The User/Role used by your Tower implementation must have the <code>secretsmanager:CreateSecret</code>. * Your Batch EC2 Instance Role must have ??? execution role ??? * Added details from here: https://github.com/seqeralabs/nf-tower-cloud/pull/2820</p> <p>Add Tower Agent blurb re: Rijkzwaan as per https://git.seqera.io/rijkzwaan/nf-support/issues/15#issuecomment-8438</p> <p>Meeting summary for 31-03-2021</p> </li> </ul> <p>Adding quick summary for the meeting today, please feel free to add/correct anything I might have missed.</p> <pre>1\n2\n3\n4\n5\n6\n7</pre><pre><code>With Jordi's guidance, the $TW_USER_AGENT was successfully used on an agent running in Kim's account to launch a pipeline from Daniel's user on Tower UI\n\nThe slight nuance on the RKZ cluster was that the home directories seem to be following a non-standard pattern i.e. with all-caps usernames (for example /home/DCR) and we had to append USER=DCR tw-agent ... to enable the agent.\n\nAn upgrade to the latest version of Tower would enable the use of $TW_AGENT_WORK variable with the agent\n\nWe also discussed the usage of pipeline reports feature\n</code></pre> <p>Follow-ups</p> <pre>1\n2\n3</pre><pre><code>@daniel-cruz-rijkzwaan to follow up here with an independent experiment to get up and running with tw-agent\n\nAbhinav to request the addition of Daniel's account in the community/showcase workspace in tower.nf\n</code></pre> <p>Warmly, Abhinav</p> <p>AZURE Batch - SSL problem as per https://git.seqera.io/eagle/nf-support/issues/10#issuecomment-8523</p>","title":"Amazon"},{"location":"_todo/#determine-if-this-is-relevant-to-google-faq-section","text":"<p>https://github.com/nf-core/configs/blob/master/conf/google.config as per this ticket: https://git.seqera.io/pluto/nf-support/issues/6</p> <p>TO DO: As per Ben, document profile injection behaviour into Tower docs (e.g. nf-core automagically injecting google profile if executing on GLS)</p>","title":"Determine if this is relevant to Google FAQ section:"},{"location":"agent/","tags":["agent"],"text":"","title":"Tower Agent"},{"location":"agent/#overview","tags":["agent"],"text":"<p>Tower Agent enables Tower to launch pipelines on HPC clusters that do not allow direct access through an SSH client.</p> <p>Tower Agent is a standalone process that runs on a node that can submit jobs to the cluster (e.g. login node). It establishes an authenticated secure reverse connection with Tower, allowing Tower to submit and monitor new jobs. The jobs are submitted on behalf of the user running the agent.</p>","title":"Overview"},{"location":"agent/#installation","tags":["agent"],"text":"<p>Tower Agent is distributed as a single executable file to simply download and execute.</p> <ol> <li>Download the latest release from Github:</li> </ol> <pre>1</pre><pre><code>curl -fSL https://github.com/seqeralabs/tower-agent/releases/latest/download/tw-agent-linux-x86_64 &gt; tw-agent\n</code></pre> <ol> <li>Make it executable:</li> </ol> <pre>1</pre><pre><code>chmod +x ./tw-agent\n</code></pre> <ol> <li>(Optional) Move it into a folder that is in your path.</li> </ol>","title":"Installation"},{"location":"agent/#quickstart","tags":["agent"],"text":"<p>Before running the Agent:</p> <ol> <li> <p>Create a personal access token in Tower.</p> </li> <li> <p>Create Tower Agent credentials in a Tower workspace. See here for more instructions.</p> </li> </ol>   <p>Tower Agent sharing</p> <p>To share a single Tower Agent instance with all members of a workspace, create a Tower Agent credential with Shared agent enabled.</p>  <p>When you create the credentials you'll get an Agent Connection ID. You can use the default ID or enter a custom ID \u2014 the connection ID in the workspace credentials must match the ID entered when you run the agent.</p> <p></p> <p>The agent should always be running in order to accept incoming requests from Tower. We recommend that you use a terminal multiplexer like tmux or GNU Screen, so that it keeps running even if you close your SSH session.</p> <pre>1\n2</pre><pre><code>export TOWER_ACCESS_TOKEN=&lt;YOUR TOKEN&gt;\n./tw-agent &lt;YOUR CONNECTION ID&gt;\n</code></pre> <p></p>","title":"Quickstart"},{"location":"agent/#tips","tags":["agent"],"text":"<ul> <li>If you are using the agent with Tower Enterprise (on-prem) you can set the API url using the <code>TOWER_API_ENDPOINT</code> environment variable or the <code>--url</code> option.</li> <li>By default, the Agent uses the folder <code>${HOME}/work</code> as the Nextflow work directory. You can change it using the <code>--work-dir</code> option.</li> <li>The work directory must exist before running the agent.</li> <li>You can also change the work directory in Tower when you create a compute environment or launch a pipeline.</li> </ul>","title":"Tips"},{"location":"agent/#usage","tags":["agent"],"text":"<pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13</pre><pre><code>Usage: tw-agent [OPTIONS] AGENT_CONNECTION_ID\n\nNextflow Tower Agent\n\nParameters:\n*     AGENT_CONNECTION_ID    Agent connection ID to identify this agent.\n\nOptions:\n* -t, --access-token=&lt;token&gt; Tower personal access token. If not provided TOWER_ACCESS_TOKEN variable will be used.\n  -u, --url=&lt;url&gt;            Tower server API endpoint URL. If not provided TOWER_API_ENDPOINT variable will be used [default: https://api.tower.nf].\n  -w, --work-dir=&lt;workDir&gt;   Default path where the pipeline scratch data is stored. It can be changed when launching a pipeline from Tower [default: ~/work].\n  -h, --help                 Show this help message and exit.\n  -V, --version              Print version information and exit.\n</code></pre>","title":"Usage"},{"location":"cli/","tags":["cli","tower"],"text":"<p><code>tw</code> is Tower on the command line. It brings Tower concepts including Pipelines, Actions and Compute Environments to the terminal.</p> <p>Tower is a full-stack application for the management of data pipelines and compute resources. It enables collaborative data analysis at scale, on-premises or in any cloud.</p> <p>The Tower CLI interacts with Tower, providing an interface to launch pipelines, manage cloud resources and administer your analysis.</p> <p></p>","title":"Nextflow Tower CLI"},{"location":"cli/#key-features","tags":["cli","tower"],"text":"<ul> <li> <p>A Nextflow-like experience: Tower CLI provides a developer-friendly environment. Pipelines can be launched with the CLI similar to Nextflow but with the benefits of Tower such as monitoring, logging, resource provisioning, dataset management, and collaborative sharing.</p> </li> <li> <p>Infrastructure as Code: All Tower resources including Pipelines and Compute Environments can be described in a declarative manner. This allows a complete definition of an analysis environment that can be versioned and treated as code. It greatly simplifies sharing and re-use of configuration as well as routine administration.</p> </li> <li> <p>Built on OpenAPI: Tower CLI interacts with Tower via the Tower API which is created using the latest OpenAPI 3.0 specification. Tower CLI provides full control of the Tower application allowing users to get maximum insights into their pipeline submissions and execution environments.</p> </li> </ul>","title":"Key features"},{"location":"cli/#availability","tags":["cli","tower"],"text":"<p>Tower CLI can be installed on macOS, Windows, and Linux.</p> <p>Visit the Tower CLI page on GitHub for installation and configuration details.</p>","title":"Availability"},{"location":"faqs/","tags":["faq","help"],"text":"","title":"FAQs and troubleshooting"},{"location":"faqs/#general-questions","tags":["faq","help"],"text":"","title":"General Questions"},{"location":"faqs/#administration-console","tags":["faq","help"],"text":"<p><p>Q: How do I access the Administration Console?</p></p> <p>The Administration Console allows Tower instance administrators to interact with all users and organizations registered with the platform. Administrators must be identified in your Tower instance configuration files prior to instantiation of the application.</p> <ol> <li>Create a <code>TOWER_ROOT_USERS</code> environment variable (e.g. via tower.env or Kubernetes ConfigMap).</li> <li>Populate the variable with a sequence of comma-delimited email addresses (no spaces).Example: <code>TOWER_ROOT_USERS=foo@foo.com,bar@bar.com</code></li> <li>If using a Tower version earlier than 21.12:<ol> <li>Add the following configuration to tower.yml: <pre>1\n2\n3</pre><pre><code>tower:\n    admin:\n        root-users: \"${TOWER_ROOT_USERS:[]}\"\n</code></pre></li> </ol> </li> <li>Restart the <code>cron</code> and <code>backend</code> containers/Deployments.</li> <li>The console will now be available via your Profile drop-down menu.</li> </ol>","title":"Administration Console"},{"location":"faqs/#api","tags":["faq","help"],"text":"<p><p>Q:I am trying to query more results than the maximum return size allows. Can I do pagination?</p></p> <p>Yes. We recommend using pagination to fetch the results in smaller chunks through multiple API calls with the help of <code>max</code> and subsequent <code>offset</code> parameters. You will receive an error like below if you run into the maximum result limit.</p> <p><code>{object} length parameter cannot be greater than 100 (current value={value_sent})</code></p> <p>We have laid out an example below using the workflow endpoint.</p> <pre>1\n2\n3\n4\n5\n6\n7</pre><pre><code>curl -X GET \"https://$TOWER_SERVER_URL/workflow/$WORKFLOW_ID/tasks?workspaceId=$WORKSPACE_ID&amp;max=100\" \\\n    -H \"Accept: application/json\" \\\n    -H \"Authorization: Bearer $TOWER_ACCESS_TOKEN\"\n\ncurl -X GET \"https://$TOWER_SERVER_URL/workflow/$WORKFLOW_ID/tasks?workspaceId=$WORKSPACE_ID&amp;max=100&amp;offset=100\" \\\n    -H \"Accept: application/json\" \\\n    -H \"Authorization: Bearer $TOWER_ACCESS_TOKEN\"\n</code></pre> <p><p>Q: Why am I receiving a 403 HTTP Response when trying to launch a pipeline via the <code>/workflow/launch</code> API endpoint?</p></p> <p>Launch users have more restricted permissions within a Workspace than Power users. While both can launch pipelines via API calls, Launch users must specify additional values that are optional for a Power user.</p> <p>One such value is <code>launch.id</code>; attempting to launch a pipeline without specifying a <code>launch.id</code> in the API payload is equivalent to using the \"Start Quick Launch\" button within a workspace (a feature only available to Power users).</p> <p>If you have encountered the 403 error as a result of being a Launch user who did not provide a <code>launch.id</code>, please try resolving the problem as follows:</p> <ol> <li> <p>Provide the launch ID to the payload sent to the tower using the same endpoint. To do this;</p> <ol> <li>Query the list of pipelines via the <code>/pipelines</code> endpoint. Find the <code>pipelineId</code> of the pipeline you intend to launch.</li> <li>Once you have the <code>pipelineId</code>, call the <code>/pipelines/{pipelineId}/launch</code> API to retrieve the pipeline's <code>launch.id</code>.</li> <li>Include the <code>launch.id</code> in your call to the <code>/workflow/launch</code> API endpoint (see example below).     <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10</pre><pre><code>{\n    \"launch\": {\n        \"id\": \"Q2kVavFZNVCBkC78foTvf\",\n        \"computeEnvId\": \"4nqF77d6N1JoJrVrrgB8pH\",\n        \"runName\": \"sample-run\",\n        \"pipeline\": \"https://github.com/sample-repo/project\",\n        \"workDir\": \"s3://myBucketName\",\n        \"revision\": \"main\"\n    }\n}\n</code></pre></li> </ol> </li> <li> <p>If a launch id remains unavailable to you, upgrade your user role to 'Maintain' or higher. This will allow you to execute quick launch-type pipeline invocations.</p> </li> </ol>","title":"API"},{"location":"faqs/#common-errors","tags":["faq","help"],"text":"<p><p>Q: After following the log-in link, why is my screen frozen at <code>/auth?success=true</code>?</p></p> <p>Starting with v22.1, Tower Enterprise implements stricter cookie security by default and will only send an auth cookie if the client is connected via HTTPS. The lack of an auth token will cause HTTP-only log-in attempts to fail (thereby causing the frozen screen).</p> <p>To remediate this problem, set the following environment variable <code>TOWER_ENABLE_UNSAFE_MODE=true</code>.</p> <p><p>Q: \"Unknown pipeline repository or missing credentials\" error when pulling from a public Github repository?</p></p> <p>Github imposes rate limits on repository pulls (including public repositories), where unauthenticated requests are capped at 60 requests/hour and authenticated requests are capped at 5000/hour. Tower users tend to encounter this error due to the 60 request/hour cap.</p> <p>To resolve the problem, please try the following:</p> <ol> <li>Ensure there is at least one Github credential in your Workspace's Credentials tab.</li> <li>Ensure that the Access token field of all Github Credential objects is populated with a Personal Access Token value, NOT a user password. (_Github PATs are typically several dozen characters long and begin with a <code>ghp_</code>prefix; example:<code>ghp*IqIMNOZH6zOwIEB4T9A2g4EHMy8Ji42q4HA</code>*)</li> <li> <p>Confirm that your PAT is providing the elevated threshold and transactions are being charged against it:</p> <p><code>curl -H \"Authorization: token ghp_LONG_ALPHANUMERIC_PAT\" -H \"Accept: application/vnd.github.v3+json\" https://api.github.com/rate_limit</code></p> </li> </ol> <p><p>Q: \"Row was updated or deleted by another transaction (or unsaved-value mapping was incorrect)\" error.</p> <p>This error can occur if incorrect configuration values are assigned to the <code>backend</code> and <code>cron</code> containers' <code>MICRONAUT_ENVIRONMENTS</code> environment variable. You may see other unexpected system behaviour like two exact copies of the same Nextflow job be submitted to the Executor for scheduling.</p> <p>Please verify the following:</p> <ol> <li>The <code>MICRONAUT_ENVIRONMENTS</code> environment variable associated with the <code>backend</code> container:<ul> <li>Contains <code>prod,redis,ha</code></li> <li>Does not contain <code>cron</code></li> </ul> </li> <li>The <code>MICRONAUT_ENVIRONMENTS</code> environment variable associated with the <code>cron</code> container:<ul> <li>Contains <code>prod,redis,cron</code></li> <li>Does not contain <code>ha</code></li> </ul> </li> <li>You do not have another copy of the <code>MICRONAUT_ENVIRONMENTS</code> environment variable defined elsewhere in your application (e.g. a tower.env file or Kubernetes ConfigMap).</li> <li>If you are using a separate container/pod to execute migrate-db.sh, there is no <code>MICRONAUT_ENVIRONMENTS</code> environment variable assigned to it.</li> </ol> <p><p>Q: Why do I get a <code>chmod: cannot access PATH/TO/bin/*: No such file or directory</code> exception?</p></p> <p>This error will be thrown if you attempt to run <code>chmod</code> against an S3/fusion-backed workdir which contains only hidden files.</p> <p>The behaviour is patched in Nextflow v22.09.7-edge. If you are unable to upgrade please see the original bug report for alternative workarounds.</p> <p><p>Q: \"No such variable\" error.</p></p> <p>This error can occur if you execute a DSL 1-based Nextflow workflow using Nextflow 22.03.0-edge or later.</p> <p><p>Q: Does the sleep command work the same way across my entire script?</p></p> <p>The <code>sleep</code> commands within your Nextflow workflows may differ in behaviour depending on where they are:</p> <ul> <li>If used within an <code>errorStrategy</code> block, the Groovy sleep function will be used (which takes its value in milliseconds).</li> <li>If used within a process script block, that language's sleep binary/method will be used. Example: this BASH script uses the BASH sleep binary, which takes its value in seconds.</li> </ul> <p><p>Q: Why does re-launching/resuming a run fail with <code>field revision is not writable</code>?</p></p> <p>A known issue with Tower versions prior to 22.3 caused resuming runs to fail for users with the launch role. This issue was fixed in Tower 22.3. Upgrade to the latest version of Tower to allow launch users to resume runs.</p>","title":"Common Errors"},{"location":"faqs/#compute-environments","tags":["faq","help"],"text":"<p><p>Q: Can the name of a Compute Environment created in Tower contain special characters?</p> <p>No. Tower version 21.12 and later do not support the inclusion of special characters in the name of Compute Environment objects.</p> <p><p>Q: How do I set NXF_OPTS values for a Compute Environment?</p> <p>This depends on your Tower version:</p> <ul> <li>For v22.1.1+, specify the values via the Environment variables section of the \"Add Compute Environment\" screen.</li> <li> <p>For versions earlier than v22.1.1, specify the values via the Staging options &gt; Pre-run script textbox on the \"Add Compute Environment\" screen. Example:</p> <p><code>export NXF_OPTS=\"-Xms64m -Xmx512m\"</code></p> </li> </ul>","title":"Compute Environments"},{"location":"faqs/#containers","tags":["faq","help"],"text":"<p><p>Q: Can I use rootless containers in my Nextflow pipelines?</p></p> <p>Most containers use the root user by default. However, some users prefer to define a non-root user in the container in order to minimize the risk of privilege escalation. Because Nextflow and its tasks use a shared work directory to manage input and output data, using rootless containers can lead to file permissions errors in some environments:</p> <pre>1</pre><pre><code>touch: cannot touch '/fsx/work/ab/27d78d2b9b17ee895b88fcee794226/.command.begin': Permission denied\n</code></pre> <p>As of Tower 22.1.0 or later, this issue should not occur when using AWS Batch. In other situations, you can avoid this issue by forcing all task containers to run as root. To do so, add one of the following snippets to your Nextflow configuration:</p> <pre>1\n2\n3\n4\n5\n6\n7\n8</pre><pre><code>// cloud executors\nprocess.containerOptions = \"--user 0:0\"\n\n// Kubernetes\nk8s.securityContext = [\n  \"runAsUser\": 0,\n  \"runAsGroup\": 0\n]\n</code></pre>","title":"Containers"},{"location":"faqs/#databases","tags":["faq","help"],"text":"<p><p>Q: Help! I upgraded to Tower Enterprise 22.2.0 and now my database connect is failing.</p></p> <p>Tower Enterprise 22.2.0 introduced a breaking change whereby the <code>TOWER_DB_DRIVER</code> is now required to be <code>org.mariadb.jdbc.Driver</code>.</p> <p>Clients who use Amazon Aurora as their database solution may encounter a <code>java.sql.SQLNonTransientConnectionException: ... could not load system variables</code> error, likely due to a known error tracked within the MariaDB project.</p> <p>Please modify Tower Enterprise configuration as follows to try resolving the problem:</p> <ol> <li>Ensure your <code>TOWER_DB_DRIVER</code> uses the specified MariaDB URI.</li> <li>Modify your <code>TOWER_DB_URL</code> to: <code>TOWER_DB_URL=jdbc:mysql://YOUR_DOMAIN:YOUR_PORT/YOUR_TOWER_DB?usePipelineAuth=false&amp;useBatchMultiSend=false</code></li> </ol>","title":"Databases"},{"location":"faqs/#datasets","tags":["faq","help"],"text":"<p><p>Q: Why are uploads of Datasets via direct calls to the Tower API failing?</p></p> <p>When uploading Datasets via the Tower UI or CLI, some steps are automatically done on your behalf. To upload Datasets via the TOwer API, additional steps are required:</p> <ol> <li>Explicitly define the MIME type of the file being uploaded.</li> <li>Make two calls to the API:<ol> <li>Create a Dataset object</li> <li>Upload the samplesheet to the Dataset object.</li> </ol> </li> </ol> <p>Example:</p> <pre>1\n2\n3\n4\n5</pre><pre><code># Step 1: Create the Dataset object\n$ curl -X POST \"https://api.tower.nf/workspaces/$WORKSPACE_ID/datasets/\" -H \"Content-Type: application/json\" -H \"Authorization: Bearer $TOWER_ACCESS_TOKEN\" --data '{\"name\":\"placeholder\", \"description\":\"A placeholder for the data we will submit in the next call\"}'\n\n# Step 2: Upload the datasheet into the Dataset object\n$ curl -X POST \"https://api.tower.nf/workspaces/$WORKSPACE_ID/datasets/$DATASET_ID/upload\"  -H \"Accept: application/json\"  -H \"Authorization: Bearer $TOWER_ACCESS_TOKEN\"  -H \"Content-Type: multipart/form-data\" -F \"file=@samplesheet_full.csv; type=text/csv\"\n</code></pre>   <p>Tip</p>  <p>You can also use the tower-cli to upload the dataset to a particular workspace.</p> <pre>1\n2\n3</pre><pre><code>```console\ntw datasets add --name \"cli_uploaded_samplesheet\" ./samplesheet_full.csv\n```\n</code></pre> <p><p>Q: Why is my uploaded Dataset not showing in the Tower Launch screen input field drop-down?</p></p> <p>When launching a Nextflow workflow from the Tower GUI, the <code>input</code> field drop-down will only show Datasets whose mimetypes match the rules specified in the associated <code>nextflow_schema.json</code> file. If your Dataset has a different mimetype than specified in the pipeline schema, Tower will not present the file.</p> <p>Note that a known issue in Tower 22.2 which caused TSV datasets to be unavailable in the drop-down has been fixed in version 22.4.1.</p> <p>Example: The default nf-core RNASeq pipeline specifies that only files with a <code>csv</code> mimetype should be provided as an input file. If you created a Dataset of mimetype <code>tsv</code>, it would not appear as an input filed dropdown option.</p> <p><p>Q: Can an input file mimetype restriction be added to the nextflow_schema.json file generated by the nf-core pipeline schema builder tool?</p></p> <p>As of August 2022, it is possible to add a mimetype restriction to the nextflow_schema.json file generated by the nf-core schema builder tool but this must occur manually after generation, not during. Please refer to this RNASeq example to see how the <code>mimetype</code> key-value pair should be specified.</p> <p><p>Q: Why are my datasets converted to 'application/vnd.ms-excel' data type when uploading on a browser using Windows OS?</p></p> <p>This is a known issue when using Firefox browser with the Tower version prior to 22.2.0. You can either (a) upgrade the Tower version to 22.2.0 or higher or (b) use Chrome.</p> <p>For context, the Tower will prompt the message below if you encountered this issue.</p> <pre>1</pre><pre><code>\"Given file is not a dataset file. Detected media type: 'application/vnd.ms-excel'. Allowed types: 'text/csv, text/tab-separated-values'\"\n</code></pre> <p><p>Q: Why are TSV-formatted datasets not shown in the Tower launch screen input field drop-down menu?</p></p> <p>An issue was identified in Tower version 22.2 which caused TSV datasets to be unavailable in the input data drop-down menu on the launch screen. This has been fixed in Tower version 22.4.1.</p>","title":"Datasets"},{"location":"faqs/#email-and-tls","tags":["faq","help"],"text":"<p><p>Q: How do I solve TLS errors when attempting to send email? </p></p> <p>Nextflow and Nextflow Tower both have the ability to interact with email providers on your behalf. These providers often require TLS connections, with many now requiring at least TLSv1.2.</p> <p>TLS connection errors can occur due to variability in the default TLS version specified by your underlying JDK distribution. If you encounter any of the following errors, there is likely a mismatch between your default TLS version and what is expected by the email provider:</p> <ul> <li><code>Unexpected error sending mail ... TLS 1.0 and 1.1 are not supported. Please upgrade/update your client to support TLS 1.2\" error</code></li> <li><code>ERROR nextflow.script.WorkflowMetadata - Failed to invoke 'workflow.onComplete' event handler ... javax.net.ssl.SSLHandshakeException: No appropriate protocol (protocol is disabled or cipher suites are inappropriate)</code></li> </ul> <p>To fix the problem, you can either:</p> <ol> <li>Set a JDK environment variable to force Nextflow and/or the Tower containers to use TLSv1.2 by default:</li> </ol> <pre>1</pre><pre><code>export JAVA_OPTIONS=\"-Dmail.smtp.ssl.protocols=TLSv1.2\"\n</code></pre> <ol> <li>Add the following parameter to your nextflow.config file:</li> </ol> <pre>1\n2\n3</pre><pre><code>mail {\n    smtp.ssl.protocols = 'TLSv1.2'\n}\n</code></pre> <p>In both cases, please ensure these values are also set for Nextflow and/or Tower:</p> <ul> <li><code>mail.smtp.starttls.enable=true</code></li> <li><code>mail.smtp.starttls.required=true</code></li> </ul>","title":"Email and TLS"},{"location":"faqs/#git-integration","tags":["faq","help"],"text":"<p><p>Q: Tower authentication to BitBucket fails, with the Tower backend log containing a warning: \"Can't retrieve revisions for pipeline - https://my.bitbucketserver.com/path/to/pipeline/repo - Cause: Get branches operation not support by BitbucketServerRepositoryProvider provider\"</p></p> <p>If you have supplied correct BitBucket credentials and URL details in your tower.yml, but experience this error, update your Tower version to at least v22.3.0. This version addresses SCM provider authentication issues and is likely to resolve the retrieval failure described here.</p>","title":"Git integration"},{"location":"faqs/#healthcheck","tags":["faq","help"],"text":"<p><p>Q: Does Tower offer a healthcheck API endpoint?</p></p> <p>Yes. Customers wishing to implement automated healtcheck functionality should use Tower's <code>service-info</code> endpoint.</p> <p>Example:</p> <pre>1\n2\n3</pre><pre><code># Run a healthcheck and extract the HTTP response code:\n$ curl -o /dev/null -s -w \"%{http_code}\\n\" --connect-timeout 2  \"https://api.tower.nf/service-info\"  -H \"Accept: application/json\"\n200\n</code></pre>","title":"Healthcheck"},{"location":"faqs/#logging","tags":["faq","help"],"text":"<p><p>Q: Can Tower enable detailed logging related to sign-in activity?</p></p> <p>Yes. For more detailed logging related to login events, set the following environment variable: <code>TOWER_SECURITY_LOGLEVEL=DEBUG</code>.</p> <p><p>Q: Can Tower enable detailed logging related to application activites?</p></p> <p>Yes. For more detailed logging related to application activities, set the following environment variable: <code>TOWER_LOG_LEVEL=TRACE</code>.</p> <p><p>Q: Version 22.3.1: My downloaded Nextflow log file is broken.</p></p> <p>A Tower Launcher issue has been identified which affects the Nextflow log file download in Tower version 22.3.1. A patch was released in version 22.3.2 that addresses this behavior. Update Tower to version 22.3.2 or later.</p>","title":"Logging"},{"location":"faqs/#login","tags":["faq","help"],"text":"<p><p>Q: Can I completely disable Tower's email login feature?</p></p> <p>The email login feature cannot be completely removed from the Tower login screen.</p> <p><p>Q: How can I restrict Tower access to only a subset of email addresses?</p></p> <p>You can restrict which emails are allowed to have automatic access to your Tower implementation via a configuration in tower.yml.</p> <p>Users without automatic access will receive an acknowledgment of their login request but be unable to access the platform until approved by a Tower administration via the Administrator Console.</p> <pre>1\n2\n3\n4\n5</pre><pre><code># This any email address that matches a pattern here will have automatic access.\ntower:\n  trustedEmails:\n    - '*@seqera.io`\n    - 'named_user@example.com'\n</code></pre> <p><p>Q: Why am I receiving login errors stating that admin approval is required when using Azure AD OIDC?</p></p> <p>The Azure AD app integrated with Tower must have user consent settings configured to \"Allow user consent for apps\" to ensure that admin approval is not required for each application login. See User consent settings.</p> <p><p>Q: Why is my OIDC redirect_url set to http instead of https?</p></p> <p>This can occur for several reasons. Please verify the following:</p> <ol> <li>Your <code>TOWER_SERVER_URL</code> environment variable uses the <code>https://</code> prefix.</li> <li>Your <code>tower.yml</code> has <code>micronaut.ssl.enabled</code> set to <code>true</code>.</li> <li>Any Load Balancer instance that sends traffic to the Tower application is configured to use HTTPS as its backend protocol rather than TCP.</li> </ol> <p><p>Q: Why isn't my OIDC callback working?</p></p> <p>Callbacks could fail for many reasons. To more effectively investigate the problem:</p> <ol> <li>Set the Tower environment variable to <code>TOWER_SECURITY_LOGLEVEL=DEBUG</code>.</li> <li>Ensure your <code>TOWER_OIDC_CLIENT</code>, <code>TOWER_OIDC_SECRET</code>, and <code>TOWER_OIDC_ISSUER</code> environment variables all match the values specified in your OIDC provider's corresponding application.</li> <li>Ensure your network infrastructure allow necessary egress and ingress traffic.</li> </ol> <p><p>Q: Why did Google SMTP start returning <code>Username and Password not accepted</code> errors?</p> Previously functioning Tower Enterprise email integration with Google SMTP are likely to encounter errors as of May 30, 2022 due to a security posture change implemented by Google.</p> <p>To reestablish email connectivity, please follow the instructions at https://support.google.com/accounts/answer/3466521 to provision an app password. Update your <code>TOWER_SMTP_PASSWORD</code> environment variable with the app password, and restart the application.</p>","title":"Login"},{"location":"faqs/#logging_1","tags":["faq","help"],"text":"<p><p>Q: Can Tower enable detailed logging related to sign-in activity?</p></p> <p>Yes. For more detailed logging related to login events, set the following environment variable: <code>TOWER_SECURITY_LOGLEVEL=DEBUG</code>.</p> <p><p>Q: Can Tower enable detailed logging related to application activities?</p></p> <p>Yes. For more detailed logging related to application activities, set the following environment variable: <code>TOWER_LOG_LEVEL=TRACE</code>.</p>","title":"Logging"},{"location":"faqs/#miscellaneous","tags":["faq","help"],"text":"<p><p>Q: Is my data safe?</p></p> <p>Yes, your data stays strictly within your infrastructure itself. When you launch a workflow through Tower, you need to connect your infrastructure (HPC/VMs/K8s) by creating the appropriate credentials and compute environment in a workspace.</p> <p>Tower then uses this configuration to trigger a Nextflow workflow within your infrastructure similar to what is done via the Nextflow CLI, therefore Tower does not manipulate any data itself and no data is transferred to the infrastructure where Tower is running.</p>","title":"Miscellaneous"},{"location":"faqs/#monitoring","tags":["faq","help"],"text":"<p><p>Q: Can Tower integrate with 3rd party Java-based Application Performance Monitoring (APM) solutions?</p></p> <p>Yes. You can mount the APM solution's JAR file in the <code>backend</code> container and set the agent JVM option via the <code>JAVA_OPTS</code> env variable.</p> <p><p>Q: Is it possible to retrieve the trace file for a Tower-based workflow run?</p> Yes. Although it is not possible to directly download the file via Tower, you can configure your workflow to export the file to persistent storage:</p> <ol> <li>Set the following block in your <code>nextflow.config</code>:</li> </ol> <pre>1\n2\n3</pre><pre><code>trace {\n    enabled = true\n}\n</code></pre> <ol> <li>Add a copy command to your pipeline's Advanced options &gt; Post-run script field:</li> </ol> <pre>1\n2\n3</pre><pre><code># Example: Export the generated trace file to an S3 bucket\n# Ensure that your Nextflow head job has the necessary permissions to interact with the target storage medium!\naws s3 cp ./trace.txt s3://MY_BUCKET/trace/trace.txt\n</code></pre> <p><p>Q: When monitoring pipeline execution via the Runs tab, why do I occasionally see Tower reporting \"Live events sync offline\"?</p></p> <p>Nextflow Tower uses server-sent events to push real-time updates to your browser. The client must establish a connection to the Nextflow Tower server's <code>/api/live</code> endpoint to initiate the stream of data, and this connection can occasionally fail due to factors like network latency.</p> <p>To resolve the issue, please try reloading the UI to reinitiate the client's connection to the server. If reloading fails to resolve the problem, please contact Seqera Support for assistance with webserver timeout settings adjustments.</p>","title":"Monitoring"},{"location":"faqs/#nextflow-configuration","tags":["faq","help"],"text":"<p><p>Q: How can I specify Nextflow CLI run arguments when launching from Tower?</p></p> <p>As of Nextflow v22.09.1-edge, when invoking a pipeline from Tower, you can specify Nextflow CLI run arguments by setting the <code>NXF_CLI_OPTS</code> environment variable via pre-run script:</p> <pre>1\n2</pre><pre><code># Example:\nexport NXF_CLI_OPTS='-dump-hashes'\n</code></pre> <p><p>Q: Can a repository's <code>nextflow_schema.json</code> support multiple input file mimetypes?</p></p> <p>No. As of April 2022, it is not possible to configure an input field (example) to support different mime types (e.g. a <code>text/csv</code>-type file during one execution, and a <code>text/tab-separated-values</code> file in a subsequent run).</p> <p><p>Q: Why are my <code>--outdir</code> artefacts not available when executing runs in a cloud environment?</p></p> <p>As of April 2022, Nextflow resolves relative paths against the current working directory. In a classic grid HPC, this normally corresponds to a subdirectory of the user's $HOME directory. In a cloud execution environment, however, the path will be resolved relative to the container file system meaning files will be lost when the container is termination. See here for more details.</p> <p>Tower Users can avoid this problem by specifying the following configuration in the Advanced options &gt; Nextflow config file configuration textbox: <code>params.outdir = workDir + '/results</code>. This will ensure the output files are written to your stateful storage rather than ephemeral container storage.</p> <p><p>Q: Can Nextflow be configured to ignore a Singularity cache?</p></p> <p>Yes. To ignore the Singularity cache, add the following configuration item to your workflow: <code>process.container = 'file:///some/singularity/image.sif'</code>.</p> <p><p>Q: Why does Nextflow fail with a <code>WARN: Cannot read project manifest ... path=nextflow.config</code> error message?</p></p> <p>This error can occur when executing a pipeline where the source git repository's default branch is not populated with <code>main.nf</code> and <code>nextflow.config</code> files, regardless of whether the invoked pipeline is using a non-default revision/branch (e.g. <code>dev</code>).</p> <p>Current as of May 16, 2022, there is no solution for this problem other than to create blank <code>main.nf</code> and <code>nextflow.config</code> files in the default branch. This will allow the pipeline to run, using the content of the <code>main.nf</code> and <code>nextflow.config</code> in your target revision.</p> <p><p>Q: Is it possible to maintain different Nextflow configuration files for different environments?</p></p> <p>Yes. The main <code>nextflow.config</code> file will always be imported by default. Instead of managing multiple <code>nextflow.config</code> files (each customized for an environment), you can create unique environment config files and import them as their own profile in the main <code>nextflow.config</code>.</p> <p>Example:</p> <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11</pre><pre><code>// nextflow.config\n\n&lt;truncated&gt;\n\nprofiles {\n    test { includeConfig 'conf/test.config' }\n    prod { includeConfig 'conf/prod.config' }\n    uat  { includeConfig 'conf/uat.config'  }\n}\n\n&lt;truncated&gt;\n</code></pre> <p><p>Q: Is there a limitation to the size of the BAM files that can be uploaded to the S3 bucket?</p></p> <p>You will see this on your log file if you encountered an error related to this: <code>WARN: Failed to publish file: s3://[bucket-name]</code></p> <p>AWS have a limitation on the size of the object that can be uploaded to S3 when using the multipart upload feature. You may refer to this documentation for more information. For this specific instance, it is hitting the maximum number of parts per upload.</p> <p>The following configuration are suggested to work with the above stated AWS limitation:</p> <ul> <li>Head Job CPUs = 16</li> <li>Head Job Memory = 60000</li> <li>Pre-run script = export NXF_OPTS=\"-Xms20G -Xmx40G\"</li> <li>Update the <code>nextflow.config</code> to increase the chunk size and slow down the number of transfers.     <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15</pre><pre><code>aws {\n  batch {\n      maxParallelTransfers = 5\n      maxTransferAttempts = 3\n      delayBetweenAttempts = 30\n  }\n  client {\n      uploadChunkSize = '200MB'\n      maxConnections = 10\n      maxErrorRetry = 10\n      uploadMaxThreads = 10\n      uploadMaxAttempts = 10\n      uploadRetrySleep = '10 sec'\n  }\n}\n</code></pre></li> </ul> <p><p>Q: Why is Nextflow forbidden to retrieve a params file from Nextflow Tower? </p></p> <p>Ephemeral endpoints can only be consumed once. Nextflow versions older than <code>22.04</code> may try to call the same endpoint more than once, resulting in an error similar to the following: <code>Cannot parse params file: /ephemeral/example.json - Cause: Server returned HTTP response code: 403 for URL: https://api.tower.nf/ephemeral/example.json</code>.</p> <p>To resolve this problem, please upgrade your Nextflow version to version <code>22.04.x</code> or later.</p> <p><p>Q: How can I prevent Nextflow from uploading intermediate files from local scratch to my S3 work directory? </p></p> <p>Nextflow will only unstage files/folders that have been explicitly defined as process outputs. If your workflow has processes that generate folder-type outputs, please ensure that the process also purges any intermediate files that reside within. Failure to do so will result in the intermediate files being copied as part of the task unstaging process, resulting in additional storage costs and lengthened pipeline execution times.</p> <p><p>Q: Why do some values specified in my git repository's nextflow.config change when the pipeline is launched via Tower? </p> You may notice that some values specified in your pipeline repository's nextflow.config have changed when the pipeline is invoked via Tower. This occurs because Tower is configured with a set of default values that are superimposed on the pipeline configuration (with the Tower defaults winning).</p> <p>Example: The following code block is specified in your nextflow.config:</p> <pre>1\n2\n3\n4\n5\n6\n7</pre><pre><code>aws {\n  region = 'us-east-1'\n  client {\n    uploadChunkSize = 209715200 // 200 MB\n  }\n  ...\n}\n</code></pre> <p>When the job instantiates on the AWS Batch Compute Environment, you will see that the <code>uploadChunkSize</code> changed:</p> <pre>1\n2\n3\n4\n5\n6\n7</pre><pre><code>aws {\n   region = 'us-east-1'\n   client {\n      uploadChunkSize = 10485760 // 10 MB\n   }\n   ...\n}\n</code></pre> <p>This change occurred because Tower superimposes its 10 MB default value rather than using the value specified in the nextflow.config file.</p> <p>To force the Tower-invoked job to use your desired value, please add the configuration setting in the Tower Workspace Launch screen's Advanced options &gt; Nextflow config file textbox. In the case of our example above, you would simply need to add <code>aws.client.uploadChunkSize = 209715200 // 200 MB</code> .</p> <p>Nextflow configuration values that are affected by this behaviour include:</p> <ul> <li>aws.client.uploadChunkSize</li> <li>aws.client.storageEncryption</li> </ul> <p><p>Q: <code>Missing output file(s) [X] expected by process [Y]</code> error during task execution in an environment using Fusion v1 </p></p> <p>Fusion v1 has a limitation which causes tasks that run for less than 60 seconds to fail as the output file generated by the task is not yet detected by Nextflow. This is a limitation inherited from a Goofys driver used by the Fusion v1 implementation. Fusion v2 (to be made available to Tower Enterprise users during Q1 of 2023) resolves this issue.</p> <p>If Fusion v2 is not yet available, or updating to v2 is not feasible, this issue can be addressed by instructing Nextflow to wait for 60 seconds after the task completes.</p> <p>From Advanced options &gt; Nextflow config file in Pipeline settings, add the following line to your Nextflow configuration:</p> <pre>1</pre><pre><code>process.afterScript = 'sleep 60'\n</code></pre> <p><p>Q: Why are jobs in RUNNING status not terminated when my pipeline run is canceled?</p></p> <p>The behavior of Tower when canceling a run depends on the <code>errorStrategy</code> defined in your process script. If the process <code>errorStrategy</code> is set to <code>finish</code>, an orderly pipeline shutdown is initiated when you cancel (or otherwise interrupt) a run. This instructs Nextflow to wait for the completion of any submitted jobs. To ensure that all jobs are terminated when your run is canceled, set <code>errorStrategy</code> to <code>terminate</code> in your Nextflow config. For example:</p> <pre>1\n2\n3\n4\n5\n6</pre><pre><code>process ignoreAnyError {\n  errorStrategy 'ignore'\n\n  script:\n  &lt;your command string here&gt;\n}\n</code></pre> <p><p>Q: Why do some cached tasks run from scratch when I re-launch a pipeline?</p></p> <p>When re-launching a pipeline, Tower relies on Nextflow's <code>resume</code> functionality for the continuation of a workflow execution. This skips previously completed tasks and uses a cached result in downstream tasks, rather than running the completed tasks again. The unique ID (hash) of the task is calculated using a composition of the task's:</p> <ul> <li>Input values</li> <li>Input files</li> <li>Command line string</li> <li>Container ID</li> <li>Conda environment</li> <li>Environment modules</li> <li>Any executed scripts in the bin directory</li> </ul> <p>A change in any of these values results in a changed task hash. Changing the task hash value means that the task will be run again when the pipeline is re-launched. To aid debugging efforts when a re-launch behaves unexpectedly, run the pipeline twice with <code>dumpHashes=true</code> set in your Nextflow config file (from Advanced options -&gt; Nextflow config file in the Pipeline settings). This will instruct Nextflow to dump the task hashes for both executions in the <code>nextflow.log</code> file. You can compare the log files to determine the point at which the hashes diverge in your pipeline when it is resumed.</p> <p>See here for more information on the Nextflow <code>resume</code> mechanism.</p> <p><p>Q: Why does my run fail with an \"o.h.e.jdbc.spi.SqlExceptionHelper - Incorrect string value\" error?</p></p>  <pre>1\n2\n3</pre><pre><code> [scheduled-executor-thread-2] - WARN  o.h.e.jdbc.spi.SqlExceptionHelper - SQL Error: 1366, SQLState: HY000\n [scheduled-executor-thread-2] - ERROR o.h.e.jdbc.spi.SqlExceptionHelper - (conn=34) Incorrect string value: '\\xF0\\x9F\\x94\\x8D |...' for column 'error_report' at row 1\n [scheduled-executor-thread-2] - ERROR i.s.t.service.job.JobSchedulerImpl - Oops .. unable to save status of job id=18165; name=nf-workflow-26uD5XXXXXXXX; opId=nf-workflow-26uD5XXXXXXXX; status=UNKNOWN\n</code></pre>  <p>Runs will fail if your Nextflow script or Nextflow config contain illegal characters (such as emojis or other non-UTF8 characters). Validate your script and config files for any illegal characters before atttempting to run again.</p>","title":"Nextflow Configuration"},{"location":"faqs/#nextflow-launcher","tags":["faq","help"],"text":"<p><p>Q: There are several nf-launcher images available in the Seqera image registry. How can I tell which one is most appropriate for my implementation?</p></p> <p>Your Tower implementation knows the nf-launcher image version it needs and will specify this value automatically when launching a pipeline.</p> <p>If you are restricted from using public container registries, please see Tower Enterprise Release Note instructions (example) for the specific image you should use and how to set this as the default when invoking pipelines.</p> <p><p>Q: The nf-launcher is pinned to a specific Nextflow version. How can I make it use a different release? </p></p> <p>Each Nextflow Tower release uses a specific nf-launcher image by default. This image is loaded with a specific Nextflow version, meaning that any workflow run in the container uses this Nextflow version by default. You can force your jobs to use a newer/older version of Nextflow with any of the following strategies:</p> <ol> <li>Use the Pre-run script advanced launch option to set the desired Nextflow version. Example: <code>export NXF_VER=22.08.0-edge</code></li> <li>For jobs executing in an AWS Batch compute environment, create a custom job definition which references a different nf-launcher image.</li> </ol>","title":"Nextflow Launcher"},{"location":"faqs/#oidc","tags":["faq","help"],"text":"<p><p>Q: Can I have users seamlessly log in to Tower if they already have an active session with their OpenId Connect (OIDC) Identity Provider (IDP)?</p></p> <p>Yes. If you are using OIDC as your authentication method, it is possible to implement a seamless login flow for your users.</p> <p>Rather than directing your users to <code>http(s)://YOUR_TOWER_HOSTNAME</code> or <code>http(s)://YOUR_TOWER_HOSTNAME/login</code>, point the user-initiated login URL here instead: <code>http(s)://YOUR_TOWER_HOSTNAME/oauth/login/oidc</code>.</p> <p>If your user already has an active session established with the IDP, they will be automatically logged into Tower rather than having to manually choose their authentication method.</p>","title":"OIDC"},{"location":"faqs/#optimization","tags":["faq","help"],"text":"<p><p>Q: When using optimization, why are tasks failing with an <code>OutOfMemoryError: Container killed due to memory usage</code> error?</p></p> <p>Improvements are being made to the way Nextflow calculates the optimal memory needed for containerized tasks, which will resolve issues with underestimating memory allocation in an upcoming release.</p> <p>A temporary workaround for this issue is to implement a <code>retry</code> error strategy in the failing process that will increase the allocated memory each time the failed task is retried. Add the following <code>errorStrategy</code> block to the failing process:</p> <pre>1\n2\n3\n4\n5</pre><pre><code>process {\n    errorStrategy = 'retry'\n    maxRetries    = \u200b3\n    memory  = 1.GB * task.attempt\n}\n</code></pre>","title":"Optimization"},{"location":"faqs/#plugins","tags":["faq","help"],"text":"<p><p>Q: Is it possible to use the Nextflow SQL DB plugin to query AWS Athena?</p></p> <p>Yes. As of Nextflow 22.05.0-edge, your Nextflow pipelines can query data from AWS Athena. You must add the following configuration items to your <code>nextflow.config</code> (Note: the use of secrets is optional):</p> <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13</pre><pre><code>plugins {\n  id 'nf-sqldb@0.4.0'\n}\n\nsql {\n    db {\n        'athena' {\n              url = 'jdbc:awsathena://AwsRegion=YOUR_REGION;S3OutputLocation=s3://YOUR_S3_BUCKET'\n              user = secrets.ATHENA_USER\n              password = secrets.ATHENA_PASSWORD\n            }\n    }\n}\n</code></pre> <p>You can then call the functionality from within your workflow.</p> <pre>1\n2\n3</pre><pre><code>// Example\n  channel.sql.fromQuery(\"select * from test\", db: \"athena\", emitColumns:true).view()\n}\n</code></pre> <p>For more information on the implementation, please see https://github.com/nextflow-io/nf-sqldb/discussions/5.</p>","title":"Plugins"},{"location":"faqs/#repositories","tags":["faq","help"],"text":"<p><p>Q: Can Tower integrate with private docker registries like JFrog Artifactory?</p></p> <p>Yes. Tower-invoked jobs can pull container images from private docker registries. The method to do so differs depending on platform, however:</p> <ul> <li>If using AWS Batch, modify your EC2 Launch Template as per these directions from AWS.Note:<ul> <li>This solution requires that your Docker Engine be at least 17.07 to use <code>--password-stdin</code>.</li> <li>You may need to add the following additional commands to your Launch Template depending on your security posture: <code>cp /root/.docker/config.json /home/ec2-user/.docker/config.json &amp;&amp; chmod 777 /home/ec2-user/.docker/config.json</code></li> </ul> </li> <li>If using Azure Batch, please create a Container Registry-type credential in your Tower Workspace and associate it with the Azure Batch object also defined in the Workspace.</li> <li>If using Kubernetes, please use an <code>imagePullSecret</code> as per https://github.com/nextflow-io/nextflow/issues/2827.</li> </ul> <p><p>Q: Why does my Nextflow log have a <code>Remote resource not found</code> error when trying to contact the workflow repository? </p></p> <p>This error can occur if the Nextflow head job fails to retrieve the necessary repository credentials from Nextflow Tower.</p> <p>To determine if this is the case, please do the following:</p> <ol> <li>Check your Nextflow log for an entry like <code>DEBUG nextflow.scm.RepositoryProvider - Request [credentials -:-]</code>.</li> <li>If the above is true, please check the protocol of the string that was assigned to your Tower instance's <code>TOWER_SERVER_URL</code> configuration value. It is possible this has been erroneously set to <code>http</code> rather than <code>https</code>.</li> </ol>","title":"Repositories"},{"location":"faqs/#secrets","tags":["faq","help"],"text":"<p><p>Q: When using secrets in Tower workflow run, the process executed with an error <code>Missing AWS execution role arn</code> </p></p> <p>The ECS Agent must be empowered to retrieve Secrets from the AWS Secrets Manager. Secrets-using pipelines that are launched from Nextflow Tower and execute in an AWS Batch Compute Environment will encounter this error if an IAM Execution Role is not provided. Please see the Pipeline Secrets for remediation steps.</p> <p><p>Q: Why do work tasks which use Secrets fail when running in AWS Batch?</p></p> <p>Users may encounter a few different errors when executing pipelines that use Secrets, via AWS Batch:</p> <ul> <li> <p>If you use <code>nf-sqldb</code> version 0.4.1 or earlier and have Secrets in your <code>nextflow.config</code>, you may see following error in your Nextflow Log: <code>nextflow.secret.MissingSecretException: Unknown config secret {SECRET_NAME}</code>.     You can resolve this error by explicitly defining the <code>xpack-amzn</code> plugin in your configuration.     Example:</p> <pre>1\n2\n3\n4</pre><pre><code>plugins {\n  id 'xpack-amzn'\n  id 'nf-sqldb'\n}\n</code></pre> </li> <li> <p>If you have two or more processes that use the same container image, but only a subset of these processes use Secrets, your Secret-using processes may fail during the initial run but succeed when resumed. This is due to an bug in how Nextflow (22.07.1-edge and earlier) registers jobs with AWS Batch.</p> <p>To resolve the issue, please upgrade your Nextflow version to 22.08.0-edge. If you cannot upgrade, you can use the following as workarounds:</p> <ol> <li>Use a different container image for each process.</li> <li>Define the same set of Secrets in each process that uses the same container image.</li> </ol> </li> </ul>","title":"Secrets"},{"location":"faqs/#tower-agent","tags":["faq","help"],"text":"<p><p>Q:Tower Agent closes a session with \"Unexpected Exception in WebSocket [io.seqera.tower.agent.AgentClientSocket$Intercepted@698514a]: Operation timed out java.io.IOException: Operation timed out\"</p></p> <p>The reconnection logic of Tower Agent has been improved with the release of version 0.5.0. Update your Tower Agent version before relaunching your pipeline.</p>","title":"Tower Agent"},{"location":"faqs/#tower-configuration","tags":["faq","help"],"text":"<p><p>Q: Can I customize menu items on the Tower navigation menu?</p></p> <p>Yes. Using the <code>navbar</code> snippet in the tower.yml configuration file, you can specify custom navigation menu items for your Tower installation. See here for more details.</p> <p><p>Q: Can a custom path be specified for the <code>tower.yml</code> configuration file?</p></p> <p>Yes. Provide a POSIX-compliant path to the <code>TOWER_CONFIG_FILE</code> environment variable.</p> <p><p>Q: Why do parts of <code>tower.yml</code> not seem to work when I run my Tower implementation?</p></p> <p>There are two reasons why configurations specified in <code>tower.yml</code> are not being expressed by your Tower instance:</p> <ol> <li>There is a typo in one of the key value pairs.</li> <li> <p>There is a duplicate key present in your file.</p> <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11</pre><pre><code># EXAMPLE\n# This block will not end up being enforced because there is another `tower` key below.\ntower:\n  trustedEmails:\n    - user@example.com\n\n# This block will end up being enforced because it is defined last.\ntower:\n  auth:\n    oidc:\n      - \"*@foo.com\"\n</code></pre> </li> </ol> <p><p>Q: Do you have guidance on how to create custom Nextflow containers?</p></p> <p>Yes. Please see https://github.com/seqeralabs/gatk4-germline-snps-indels/tree/master/containers.</p> <p><p>Q: What DSL version does Nextflow Tower set as default for Nextflow head jobs?</p> <p>As of Nextflow 22.03.0-edge, DSL2 is the default syntax.</p> <p>To minimize disruption on existing pipelines, Nextflow Tower version 22.1.x and later are configured to default Nextflow head jobs to DSL 1 for a transition period (ending TBD).</p> <p>You can force your Nextflow head job to use DSL2 syntax via any of the following techniques:</p> <ul> <li>Adding <code>export NXF_DEFAULT_DSL=2</code> in the Advanced Features &gt; Pre-run script field of Tower Launch UI.</li> <li>Specifying <code>nextflow.enable.dsl = 2</code> at the top of your Nextflow workflow file.</li> <li>Providing the <code>-dsl2</code> flag when invoking the Nextflow CLI (e.g. <code>nextflow run ... -dsl2</code>)</li> </ul> <p><p>Q: Can Tower to use a Nextflow workflow stored in a local git repository?</p></p> <p>Yes. As of v22.1, Nextflow Tower Enterprise can link to workflows stored in \"local\" git repositories. To do so:</p> <ol> <li>Volume mount your repository folder into the Tower Enterprise <code>backend</code> container.</li> <li>Update your <code>tower.yml</code> with the following configuration:</li> </ol> <pre>1\n2\n3\n4</pre><pre><code>tower:\n    pipeline:\n        allow-local-repos:\n            - /path/to/repo\n</code></pre> <p>Note: This feature is not available to Tower Cloud users.</p> <p><p>Q: Am I forced to define sensitive values in <code>tower.env</code>?</p> No. You can inject values directly into <code>tower.yml</code> or - in the case of a Kubernetes deployment - reference data from a secrets manager like Hashicorp Vault.</p> <p>Please contact Seqera Labs for more details if this is of interest.</p>","title":"Tower Configuration"},{"location":"faqs/#tower-forge","tags":["faq","help"],"text":"<p><p>Q: What does the <code>Enable GPU</code> option do when building an AWS Batch cluster via Tower Forge?</p></p> <p>Activating the Enable GPU field while creating an AWS Batch environment with Tower Forge will result in an AWS-recommended GPU-optimized ECS AMI being used as your Batch cluster's default image.</p> <p>Note:</p> <ol> <li>Activation does not cause GPU-enabled instances to automatically spawn in your Batch cluster. You must still specify these in the Forge screen's Advanced options &gt; Instance types field.</li> <li>Population of the Forge screen's Advanced options &gt; AMI Id field will supersede the AWS-recommended AMI.</li> <li>Your Nextflow script must include accelerator directives to use the provisioned GPUs.</li> </ol>","title":"Tower Forge"},{"location":"faqs/#tw-cli","tags":["faq","help"],"text":"<p><p>Q: Can a custom run name be specified when launch a pipeline via the <code>tw</code> CLI?</p></p> <p>Yes. As of <code>tw</code> v0.6.0, this is possible. Example: <code>tw launch --name CUSTOM_NAME ...</code></p> <p><p>Q: Why are tw cli commands resulting in segfault errors?</p></p> <p><code>tw</code> cli versions 0.6.1 through 0.6.4 were compiled using glibc instead of MUSL. This change was discovered to cause segfaults in certain operating systems and has been rolled back in tw cli 0.6.5.</p> <p>To resolve this error, please try using the MUSL-based binary first. If this fails to work on your machine, an alternative Java JAR-based solution is available for download and use.</p> <p><p>Q: Can <code>tw cli</code> communicate with hosts using http?</p></p> <p>This error indicates that your Tower host accepts connections using http (insecure), rather than https. If your host cannot be configured to accept https connections, run your tw cli command with the <code>--insecure</code> flag.</p> <pre>1\n2</pre><pre><code> ERROR: You are trying to connect to an insecure server: http://hostname:port/api\n        if you want to force the connection use '--insecure'. NOT RECOMMENDED!\n</code></pre> <p>To do this, add the <code>--insecure</code> flag before your cli command (see below). Note that, although this approach is available for use in deployments that do not accept <code>https:</code> connections, it is not recommended. Best practice is to use <code>https:</code> wherever possible.</p> <pre>1</pre><pre><code>$ tw --insecure info\n</code></pre> <p>Note: The <code>${TOWER_API_ENDPOINT}</code> is equivalent to the <code>${TOWER_SERVER_URL}/api</code>.</p> <p><p>Q: Can a user resume/relaunch a pipeline using the tw cli?</p></p> <p>Yes, it is possible with <code>tw runs relaunch</code>.</p> <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13</pre><pre><code>$ tw runs relaunch -i 3adMwRdD75ah6P -w 161372824019700\n\n  Workflow 5fUvqUMB89zr2W submitted at [org / private] workspace.\n\n\n$ tw runs list -w 161372824019700\n\n  Pipeline runs at [org / private] workspace:\n\n     ID             | Status    | Project Name   | Run Name        | Username    | Submit Date\n    ----------------+-----------+----------------+-----------------+-------------+-------------------------------\n     5fUvqUMB89zr2W | SUBMITTED | nf/hello       | magical_darwin  | seqera-user | Tue, 10 Sep 2022 14:40:52 GMT\n     3adMwRdD75ah6P | SUCCEEDED | nf/hello       | high_hodgkin    | seqera-user | Tue, 10 Sep 2022 13:10:50 GMT\n</code></pre>","title":"tw CLI"},{"location":"faqs/#workspaces","tags":["faq","help"],"text":"<p><p>Q: Why is my Tower-invoked pipeline trying to contact a different Workspace than the one it was launched from?</p></p> <p>This problem will express itself with the following entry in your Nextflow log: <code>Unexpected response for request http://YOUR_TOWER_URL/api/trace/TRACE_ID/begin?workspaceId=WORKSPACE_ID</code>.</p> <p>This can occur due to the following reasons:</p> <ol> <li>An access token value has been hardcoded in the <code>tower.accessToken</code> block of your <code>nextflow.config</code> (either via the git repository itself or override value in the launch form).</li> <li>In cases where your compute environment is an HPC cluster, the credentialized user's home directory contains a stateful <code>nextflow.config</code> with a hardcoded token (e.g. `~/.nextflow/config).</li> </ol> <p><p>Q: What privilege level is granted to a user assigned to a Workspace both as a Participant and Team member?</p></p> <p>It is possible for a user to be concurrently assigned to a Workspace both as a named Participant and member of a Team. In such cases, Tower will grant the higher of the two privilege sets.</p> <p>Example:</p> <ul> <li>If the Participant role is Launch and the Team role is Admin, the user will have Admin rights.</li> <li>If the Participant role is Admin and the Team role is Launch, the user will have Admin rights.</li> <li>If the Participant role is Launch and the Team role is Launch, the user will have Launch rights.</li> </ul> <p>As a best practice, Seqera suggests using Teams as the primary vehicle for assigning rights within a Workspace and only adding named Participants when one-off privilege escalations are deemed necessary.</p>","title":"Workspaces"},{"location":"faqs/#amazon","tags":["faq","help"],"text":"","title":"Amazon"},{"location":"faqs/#ebs","tags":["faq","help"],"text":"<p><p>Q: EBS Autoscaling: Why do some EBS volumes remain active after their associated jobs have completed?</p></p> <p>The EBS autoscaling solution relies on an AWS-provided script running on each container host. This script performs AWS EC2 API requests to delete EBS volumes when the jobs using those volumes have been completed. When running large Batch clusters (hundreds of compute nodes or more), EC2 API rate limits may cause the deletion of unattached EBS volumes to fail. Volumes that remain active after Nextflow jobs have been completed will incur additional costs and should therefore be manually deleted. You can monitor your AWS account for any orphaned EBS volumes via the EC2 console or with a Lambda function. See here for more information.</p>","title":"EBS"},{"location":"faqs/#ec2-instances","tags":["faq","help"],"text":"<p><p>Q: Can I run a Nextflow head job on AWS Graviton instances?</p></p> <p>Yes, Nextflow supports Graviton architecture \u2014 use AWS Batch queues with Graviton-based instance types.</p>","title":"EC2 Instances"},{"location":"faqs/#ecs","tags":["faq","help"],"text":"<p><p>Q:How often are Docker images pulled by the ECS Agent?</p></p> <p>As part of the AWS Batch creation process, Tower Forge will set ECS Agent parameters in the EC2 Launch Template that is created for your cluster's EC2 instances:</p> <ul> <li>For clients using Tower Enterprise v22.01 or later:<ul> <li>Any AWS Batch environment created by Tower Forge will set the ECS Agent's <code>ECS_IMAGE_PULL_BEHAVIOUR</code> set to <code>once</code>.</li> </ul> </li> <li>For clients using Tower Enterprise v21.12 or earlier:<ul> <li>Any AWS Batch environment created by Tower Forge will set the ECS Agent's <code>ECS_IMAGE_PULL_BEHAVIOUR</code> set to <code>default</code>.</li> </ul> </li> </ul> <p>Please see the AWS ECS documentation for an in-depth explanation of this difference.</p> <p></p>Note: This behaviour cannot be changed within the Tower Application. <p><p>Q: We encountered an error saying unable to parse HTTP 429 response body.</p></p> <p><code>CannotPullContainerError: Error response from daemon: error parsing HTTP 429 response body: invalid character 'T' looking for beginning of value: \"Too Many Requests (HAP429)\"</code></p> <p>This is because of the dockerhub rate limit of 100 anonymous pulls per 6 hours. We suggest to use the following on your launch template in order to avoid this issue:</p> <p><code>echo ECS_IMAGE_PULL_BEHAVIOR=once &gt;&gt; /etc/ecs/ecs.config</code></p> <p><p>Q: Help! My job failed due to a CannotInspectContainerError error.</p></p> <p>There are multiple reasons why your pipeline could fail with an <code>Essential container in task exited - CannotInspectContainerError: Could not transition to inspecting; timed out after waiting 30s</code> error.</p> <p>Please try the following:</p> <ol> <li>Upgrade your ECS Agent to 1.54.1 or newer (instructions for checking your ECS Agent version);</li> <li>Provision more storage space for your EC2 instance (preferably via ebs-autoscaling to ensure scalability).</li> <li>If the error is accompanied by <code>command exit status: 123</code> and a <code>permissions denied</code> error tied to a system command, please ensure that the binary is set to be executable (i.e. <code>chmod u+x</code>).</li> </ol>","title":"ECS"},{"location":"faqs/#queues","tags":["faq","help"],"text":"<p><p>Q: Does Nextflow Tower support the use of multiple AWS Batch queues during a single job execution?</p></p> <p>Yes. Even though you can only create/identify a single work queue during the definition of your AWS Batch Compute Environment within Nextflow Tower, you can spread tasks across multiple queues when your job is sent to Batch for execution via your pipeline configuration.</p> <p>Adding the following snippet to either your nextflow.config or the Advanced Features &gt; Nextflow config gile field of Tower Launch UI, will cause processes to be distributed across two AWS Batch queues, depending on the assigned named.</p> <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13</pre><pre><code># nextflow.config\n\nprocess {\n  withName: foo {\n    queue: `TowerForge-1jJRSZmHyrrCvCVEOhmL3c-work`\n  }\n}\n\nprocess {\n  withName: bar {\n    queue: `custom-second-queue`\n  }\n}\n</code></pre>","title":"Queues"},{"location":"faqs/#security","tags":["faq","help"],"text":"<p><p>Q: Can Tower connect to an RDS instance using IAM credentials instead of username/password?</p></p> <p>No. Nextflow Tower must be supplied with a username &amp; password to connect to its associated database.</p>","title":"Security"},{"location":"faqs/#storage","tags":["faq","help"],"text":"<p><p>Q: Can I use EFS as my work directory?</p></p> <p>As of Nextflow Tower v21.12, you can specify an Amazon Elastic File System instance as your Nextflow work directory when creating your AWS Batch Compute Environment via Tower Forge.</p> <p><p>Q: Can I use FSX for Luster as my work directory?</p></p> <p>As of Nextflow Tower v21.12, you can specify an Amazon FSX for Lustre instance as your Nextflow work directory when creating your AWS Batch Compute Environment via Tower Forge.</p> <p><p>Q: How do I configure my Tower-invoked pipeline to be able to write to an S3 bucket that enforces AES256 server-side encryption?</p> <p>If you need to save files to an S3 bucket protected by a bucket policy which enforces AES256 server-side encryption, additional configuration settings must be provided to the nf-launcher script which invokes the Nextflow head job:</p> <ol> <li> <p>Add the following configuration to the Advanced options &gt; Nextflow config file textbox of the Launch Pipeline screen:</p> <pre>1\n2\n3\n4\n5</pre><pre><code>aws {\n  client {\n    storageEncryption = 'AES256'\n  }\n}\n</code></pre> </li> <li> <p>Add the following configuration to the Advanced options &gt; Pre-run script textbox of the Launch Pipeline screen:     <pre>1</pre><pre><code>export TOWER_AWS_SSE=AES256\n</code></pre></p> </li> </ol> <p>Note: This solution requires at least Tower v21.10.4 and Nextflow 22.04.0.</p>","title":"Storage"},{"location":"faqs/#azure","tags":["faq","help"],"text":"","title":"Azure"},{"location":"faqs/#aks","tags":["faq","help"],"text":"<p><p>Q: Why is Nextflow returning a \"... /.git/HEAD.lock: Operation not supported\" error?</p></p> <p>This problem can occur if your Nextflow pod uses an Azure Files-type (SMB) Persistent Volume as its storage medium. By default, the <code>jgit</code> library used by Nextflow attempts a filesystem link operation which is not supported by Azure Files (SMB).</p> <p>To avoid this problem, please add the following code snippet in your pipeline's pre-run script field:</p> <pre>1\n2\n3\n4</pre><pre><code>cat &lt;&lt;EOT &gt; ~/.gitconfig\n[core]\n    supportsatomicfilecreation = true\nEOT\n</code></pre>","title":"AKS"},{"location":"faqs/#batch","tags":["faq","help"],"text":"<p><p>Q: Why is my Azure Batch VM quota set to 0?</p></p> <p>In order to manage capacity during the global health pandemic, Microsoft has reduced core quotas for new Batch accounts. Depending on your region and subscription type, a newly-created account may not be entitled to any VMs without first making a service request to Azure.</p> <p>Please see Azure's Batch service quotas and limits page for further details.</p>","title":"Batch"},{"location":"faqs/#ssl","tags":["faq","help"],"text":"<p><p>Q: \"Problem with the SSL CA cert (path? access rights?)\" error</p></p> <p>This can occur if a tool/library in your task container requires SSL certificates to validate the identity of an external data source.</p> <p>You may be able to solve the issue by:</p> <ol> <li>Mounting host certificates into the container (example).</li> </ol> <p><p>Q: Why is my deployment using Azure SQL database returning an error about <code>Connections using insecure transport are prohibited while --require_secure_transport=ON.</code></p></p> <p>This is due to Azure's default MySQL behavior of enforcing the SSL connections between your server and client application, as detailed here. In order to fix this, append the following to your <code>TOWER_DB_URL</code> connection string: <code>useSSL=true&amp;enabledSslProtocolSuites=TLSv1.2&amp;trustServerCertificate=true</code></p> <p>eg, <code>TOWER_DB_URL=jdbc:mysql://azuredatabase.com/tower?serverTimezone=UTC&amp;useSSL=true&amp;enabledSslProtocolSuites=TLSv1.2&amp;trustServerCertificate=true</code></p>","title":"SSL"},{"location":"faqs/#google","tags":["faq","help"],"text":"","title":"Google"},{"location":"faqs/#retry","tags":["faq","help"],"text":"<p><p>Q: How do I make my Nextflow pipelines more resilient to VM preemption?</p></p> <p>Running your pipelines on preemptible VMs provides significant cost savings but increases the likelihood that a task will be interrupted before completion. It is a recommended best practice to implement a retry strategy when you encounter exit codes that are commonly related to preemption. Example:</p> <pre>1\n2\n3\n4\n5</pre><pre><code>process {\n  errorStrategy = { task.exitStatus in [8,10,14] ? 'retry' : 'finish' }\n  maxRetries    = 3\n  maxErrors     = '-1'\n}\n</code></pre> <p><p>Q: What are the minimum Tower Service account permissions needed for GLS and GKE?</p></p> <p>The following roles are needed to be granted to the <code>nextflow-service-account</code>.</p> <ol> <li>Cloud Life Sciences Workflows Runner</li> <li>Service Account User</li> <li>Service Usage Consumer</li> <li>Storage Object Admin</li> </ol> <p>For detailed information, please refer to this guide.</p>","title":"Retry"},{"location":"faqs/#kubernetes","tags":["faq","help"],"text":"<p><p>Q: Pod failing with 'Invalid value: \"xxx\": must be less or equal to memory limit' error</p></p> <p>This error may be encountered when you specify a value in the Head Job memory field during the creation of a Kubernetes-type Compute Environment.</p> <p>If you receive an error that includes <code>field: spec.containers[x].resources.requests</code> and <code>message: Invalid value: \"xxx\": must be less than or equal to memory limit</code>, your Kubernetes cluster may be configured with system resource limits which deny the Nextflow head job's resource request. To isolate which component is causing the problem, try to launch a Pod directly on your cluster via your Kubernetes administration solution. Example:</p> <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16</pre><pre><code>---\napiVersion: v1\nkind: Pod\nmetadata:\n    name: debug\n    labels:\n        app: debug\nspec:\n    containers:\n        - name: debug\n          image: busybox\n          command: [\"sh\", \"-c\", \"sleep 10\"]\n          resources:\n              requests:\n                  memory: \"xxxMi\" # or \"xxxGi\"\n    restartPolicy: Never\n</code></pre>","title":"Kubernetes"},{"location":"faqs/#on-prem-hpc","tags":["faq","help"],"text":"<p><p>Q: \"java: command not found\"</p></p> <p>When submitting jobs to your on-prem HPC (regardless of whether using SSH or Tower-Agent authentication), the following error may appear in your Nextflow logs even though you have Java on your $PATH environment variable:</p> <pre>1\n2\n3\n4</pre><pre><code>java: command not found\nNextflow is trying to use the Java VM defined for the following environment variables:\n  JAVA_CMD: java\n  NXF_OPTS:\n</code></pre> <p>Possible reasons for this error:</p> <ol> <li>The queue where the Nextflow head job runs in a different environment/node than your login node userspace.</li> <li>If your HPC cluster uses modules, the Java module may not be loaded by default.</li> </ol> <p>To troubleshoot:</p> <ol> <li>Open an interactive session with the head job queue.</li> <li>Launch the Nextflow job from the interactive session.</li> <li>If you cluster used modules:<ol> <li>Add <code>module load &lt;your_java_module&gt;</code> in the Advanced Features &gt; Pre-run script field when creating your HPC Compute Environment within Nextflow Tower.</li> </ol> </li> <li>If you cluster does not use modules:<ol> <li>Source an environment with java and Nextflow using the Advanced Features &gt; Pre-run script field when creating your HPC Compute Environment within Nextflow Tower.</li> </ol> </li> </ol>","title":"On-Prem HPC"},{"location":"administration/overview/","tags":["administration","user","workspace","organization"],"text":"","title":"User and organization administration"},{"location":"administration/overview/#administration-of-users-organizations-teams-and-memberships","tags":["administration","user","workspace","organization"],"text":"<p>As a Root user, you can access a comprehensive overview of the users, workspaces, and organizations in the system from the Admin panel.</p> <p>The Admin panel menu entry will only be accessible in the user top-right menu if you are logged in as a Root user. This role should only be assigned to a system administrator, since it enables several high level and potentially risky operations.</p>","title":"Administration of users, organizations, teams, and memberships"},{"location":"administration/overview/#user-administration","tags":["administration","user","workspace","organization"],"text":"<p>The User administration page lists all the users in the Tower database. From this page, you can:</p>","title":"User administration"},{"location":"administration/overview/#search-users","tags":["administration","user","workspace","organization"],"text":"<p>The user search function allows you to find a specific user by name or email and perform various operations with that user.</p>","title":"Search users"},{"location":"administration/overview/#create-a-user","tags":["administration","user","workspace","organization"],"text":"<p>The Add user button above the table allows you to create a new user. If the new user email already exists in the system, the user creation will fail. Once the new user has been created, inform them that access has been granted.</p>","title":"Create a user"},{"location":"administration/overview/#edit-a-user","tags":["administration","user","workspace","organization"],"text":"<p>By selecting a username from the table, you can edit the user's details, or delete the user.</p>","title":"Edit a user"},{"location":"administration/overview/#membership-administration","tags":["administration","user","workspace","organization"],"text":"<p>Available from version 22.3.X</p> <p>From the user list, you have an overview of all the memberships for the selected user. The Membership administration page is reached by selecting the Edit organizations button. From here, you can list and search for all the organizations the user belongs to (as a member or as an owner), change the role of the user for a given membership, remove the user from an organization, or add the user to a new organization.</p> <p>Note: You can only add users to an existing organization, and you cannot remove the last owner of an organization.</p>","title":"Membership administration"},{"location":"administration/overview/#organization-administration","tags":["administration","user","workspace","organization"],"text":"<p>The Organization administration page lists all the organizations in the Tower database. From this page, you can:</p>","title":"Organization administration"},{"location":"administration/overview/#search-organizations","tags":["administration","user","workspace","organization"],"text":"<p>The organization search function allows you to find a specific organization by its name or email and perform various operations with that organization.</p>","title":"Search organizations"},{"location":"administration/overview/#create-an-organization","tags":["administration","user","workspace","organization"],"text":"<p>The Add organization button above the table allows you to create a new organization from scratch.</p>","title":"Create an organization"},{"location":"administration/overview/#edit-an-organization","tags":["administration","user","workspace","organization"],"text":"<p>By selecting an organization name from the table, you can edit the organization's details, or delete it.</p>","title":"Edit an organization"},{"location":"administration/overview/#membership-administration_1","tags":["administration","user","workspace","organization"],"text":"<p>Available from version 22.3.X</p> <p>From the organizations list, you have an overview of all the memberships for the selected organization. Select the Manage users button to access the Membership administration page. From here, you can list and search for all the users that are members or owners of the selected organization, change the role of the user for the given membership, remove the member from the organization, or add a new user to the organization.</p> <p>Note: You can only add existing users to an organization, and you cannot remove a membership if the user being removed is the last owner of the selected organization. To overcome this, promote another user to Owner before removing or demoting the last owner.</p>","title":"Membership administration"},{"location":"administration/overview/#team-administration","tags":["administration","user","workspace","organization"],"text":"<p>Available from version 23.1.X</p> <p>The Team administration page lists all the teams in the Tower database. From this page, you can:</p>","title":"Team administration"},{"location":"administration/overview/#search-teams","tags":["administration","user","workspace","organization"],"text":"<p>The team search function allows you to find a specific team, by name or description, and perform various operations.</p>","title":"Search teams"},{"location":"administration/overview/#create-a-team","tags":["administration","user","workspace","organization"],"text":"<p>The Add team button above the table allows you to create a new team from scratch.</p>","title":"Create a team"},{"location":"administration/overview/#edit-a-team","tags":["administration","user","workspace","organization"],"text":"<p>By selecting Edit next to your team of choice, you can edit the team's details or delete it.</p>","title":"Edit a team"},{"location":"administration/overview/#membership-administration_2","tags":["administration","user","workspace","organization"],"text":"<p>From the teams list, you have an overview of the number of members and the unique ID of each team. Select the Edit button to view a team's page, or select the number next to Members: to navigate directly to the Members tab of the team page. From the Members of team tab, you can list and search for all the users that are members of the selected team, change the role of the user for the given membership, remove the member from the team, or add a new member to the team.</p>","title":"Membership administration"},{"location":"api/overview/","tags":["api"],"text":"<p>Tower exposes a public API with all the necessary endpoints to manage Nextflow workflows programmatically, allowing organizations to incorporate Tower seamlessly into their existing processes.</p>","title":"API"},{"location":"api/overview/#overview","tags":["api"],"text":"<p>The Tower API can be accessed from <code>https://api.tower.nf</code>. All API endpoints use HTTPS, and all request and response payloads use JSON encoding. All timestamps use the ISO 8601 date-time standard format: <code>YYYY-MM-DDTHH:MM:SSZ</code>.</p>","title":"Overview"},{"location":"api/overview/#openapi","tags":["api"],"text":"<p>The Tower API uses the OpenAPI standard. The current OpenAPI schema for Tower can be found here.</p>","title":"OpenAPI"},{"location":"api/overview/#endpoints","tags":["api"],"text":"<p>You can find a detailed list of all Tower endpoints here. This page also includes request and response payload examples, and the ability to test each endpoint interactively.</p>","title":"Endpoints"},{"location":"api/overview/#programmatic-api","tags":["api"],"text":"<p>You can use tools such as openapi-python-client to generate a programmatic API for a particular language (e.g. Python) based on the OpenAPI schema. However, we do not guarantee that any OpenAPI client generator will work with Tower API; use them at your own risk.</p>","title":"Programmatic API"},{"location":"api/overview/#authentication","tags":["api"],"text":"<p>Tower API requires an authentication token to be specified in each API request using the Bearer HTTP header.</p> <p>Your personal authorization token can be found in the user top-right menu under Your tokens.</p> <p>To create a new access token, just provide a name for the token. This will help to identify it later.</p> <p></p> <p>The token is only displayed once. Store your token in a safe place.</p> <p>Once created, use the token to authenticate to the Nextflow API via cURL, Postman, or within your code to requests.</p>","title":"Authentication"},{"location":"api/overview/#curl-example","tags":["api"],"text":"<pre>1</pre><pre><code>curl -H \"Authorization: Bearer eyJ...YTk0\" https://tower.nf/api/workflow\n</code></pre>   <p>Use your token in every API call</p> <p>Your token must be included in every API call. See Bearer token authentication for more information on bearer token authentication.</p>","title":"cURL example"},{"location":"api/overview/#parameters","tags":["api"],"text":"<p>Some API <code>GET</code> methods will accept standard <code>query</code> parameters, which are defined in the documentation; <code>querystring</code> optional parameters such as page size, number (when available) and file name; and body parameters, mostly used for <code>POST</code>, <code>PUT</code> and <code>DELETE</code> requests.</p> <p>Additionally, several head parameters are accepted such as <code>Authorization</code> for bearer access token or <code>Accept-Version</code> to indicate the desired API version to use (default to version 1)</p> <pre>1\n2\n3\n4</pre><pre><code>curl -H \"Authorization: Bearer QH..E5M=\"\n     -H \"Accept-Version:1\"\n     -X POST https://tower.nf/api/domain/{item_id}?queryString={value}\n     -d { params: { \"key\":\"value\" } }\n</code></pre>","title":"Parameters"},{"location":"api/overview/#client-errors","tags":["api"],"text":"<p>There exists two typical standard errors, or non <code>200</code> or <code>204</code> status responses, to expect from the API.</p>","title":"Client errors"},{"location":"api/overview/#bad-request","tags":["api"],"text":"<p>The request payload is not properly defined or the query parameters are invalid.</p> <pre>1\n2\n3</pre><pre><code>{\n    \"message\": \"Oops... Unable to process request - Error ID: 54apnFENQxbvCr23JaIjLb\"\n}\n</code></pre>","title":"Bad Request"},{"location":"api/overview/#forbidden","tags":["api"],"text":"<p>Your access token is invalid or expired. This response may also imply that the entry point you are trying to access is not available; in such a case, it is recommended you check your request syntax.</p> <pre>1</pre><pre><code>Status: 403 Forbidden\n</code></pre>","title":"Forbidden"},{"location":"api/overview/#rate-limiting","tags":["api"],"text":"<p>For all API requests, there is a limit of 20 calls per second (72000 calls per hour) and access key.</p>","title":"Rate limiting"},{"location":"compute-envs/altair-grid-engine/","tags":["grid engine","compute environment"],"text":"<p>Altair Grid Engine is a workload manager maintained by Altair Engineering, Inc.</p> <p>Tower streamlines the deployment of Nextflow pipelines into both cloud-based and on-prem Grid Engine clusters.</p>","title":"Grid Engine"},{"location":"compute-envs/altair-grid-engine/#requirements","tags":["grid engine","compute environment"],"text":"<p>To launch pipelines into a Grid Engine cluster from Tower, the following requirements must be satisfied:</p> <ul> <li>The cluster should allow outbound connections to the Tower web service.</li> <li>The cluster queue used to run the Nextflow head job must be able to submit cluster jobs.</li> <li>The Nextflow runtime version 21.02.0-edge (or later) must be installed on the cluster.</li> </ul>","title":"Requirements"},{"location":"compute-envs/altair-grid-engine/#compute-environment","tags":["grid engine","compute environment"],"text":"<p>To create a new compute environment for Grid Engine in Tower:</p> <ol> <li> <p>In a workspace, select Compute environments and then New environment.</p> </li> <li> <p>Enter a descriptive name for this environment, e.g., \"Grid Engine\".</p> </li> <li> <p>Select Altair Grid Engine as the target platform.</p> </li> <li> <p>Select your credentials, or select + and SSH or Tower Agent to add new credentials.</p> </li> <li> <p>Enter a name for the credentials.</p> </li> <li> <p>Enter the absolute path of the Work directory to be used on the cluster.</p> </li> <li> <p>Enter the absolute path of the Launch directory to be used on the cluster. If omitted, it will be the same as the work directory.</p> </li> <li> <p>Enter the Login hostname. This is usually the hostname or public IP address of the cluster's login node.</p> </li> <li> <p>Enter the Head queue name. This is the cluster queue to which the Nextflow job will be submitted.</p> </li> <li> <p>Enter the Compute queue name. This is the cluster queue to which the Nextflow job will submit tasks.</p>  <p>Tip</p> <p>The compute queue can be overridden by the Nextflow pipeline configuration. See the Nextflow documentation for more details.</p>  </li> <li> <p>Expand Staging options to include optional pre- or post-run Bash scripts that execute before or after the Nextflow pipeline execution in your environment. </p> </li> <li> <p>You can use the Environment variables option to specify custom environment variables for the Head job and/or Compute jobs.</p> </li> <li> <p>Configure any advanced options described below, as needed.</p> </li> <li> <p>Select Create to finalize the creation of the compute environment.</p> </li> </ol> <p>Jump to the documentation for launching pipelines.</p>","title":"Compute environment"},{"location":"compute-envs/altair-grid-engine/#advanced-options","tags":["grid engine","compute environment"],"text":"<ul> <li> <p>Use the Nextflow queue size to limit the number of jobs that Nextflow can submit to the scheduler at the same time.</p> </li> <li> <p>Use the Head job submit options to specify Grid Engine options for the head job. You can optionally apply these options to compute jobs as well:</p> </li> </ul> <p></p>","title":"Advanced options"},{"location":"compute-envs/altair-pbs-pro/","tags":["pbs pro","altair","compute environment"],"text":"<p>Altair PBS Pro is a workload manager and job scheduling tool provided by Altair Engineering, Inc.</p> <p>Tower streamlines the deployment of Nextflow pipelines into both cloud-based and on-prem PBS Pro clusters.</p>","title":"Altair PBS Pro"},{"location":"compute-envs/altair-pbs-pro/#requirements","tags":["pbs pro","altair","compute environment"],"text":"<p>To launch pipelines into a PBS Pro cluster from Tower, the following requirements must be satisfied:</p> <ul> <li>The cluster should allow outbound connections to the Tower web service.</li> <li>The cluster queue used to run the Nextflow head job must be able to submit cluster jobs.</li> <li>The Nextflow runtime version 21.02.0-edge (or later) must be installed on the cluster.</li> </ul>","title":"Requirements"},{"location":"compute-envs/altair-pbs-pro/#compute-environment","tags":["pbs pro","altair","compute environment"],"text":"<p>To create a new compute environment for PBS Pro in Tower:</p> <ol> <li> <p>In a workspace, select Compute environments and then New environment.</p> </li> <li> <p>Enter a descriptive name for this environment, e.g., \"PBS Pro cluster\".</p> </li> <li> <p>Select Altair PBS Pro as the target platform.</p> </li> <li> <p>Select your credentials, or select + and SSH or Tower Agent to add new credentials.</p> </li> <li> <p>Enter a name for the credentials.</p> </li> <li> <p>Enter the absolute path of the Work directory to be used on the cluster.</p> </li> <li> <p>Enter the absolute path of the Launch directory to be used on the cluster. If omitted, it will be the same as the work directory.</p> </li> <li> <p>Enter the Login hostname. This is usually the hostname or public IP address of the cluster's login node.</p> </li> <li> <p>Enter the Head queue name. This is the cluster queue to which the Nextflow job will be submitted.</p> </li> <li> <p>Enter the Compute queue name. This is the cluster queue to which the Nextflow job will submit tasks.</p>  <p>Tip</p> <p>The compute queue can be overridden by the Nextflow pipeline configuration. See the Nextflow documentation for more details.</p>  </li> <li> <p>Expand Staging options to include optional pre- or post-run Bash scripts that execute before or after the Nextflow pipeline execution in your environment. </p> </li> <li> <p>Use the Environment variables option to specify custom environment variables for the Head job and/or Compute jobs.</p> </li> <li> <p>Configure any advanced options described below, as needed.</p> </li> <li> <p>Select Create to finalize the creation of the compute environment.</p> </li> </ol> <p>Jump to the documentation for launching pipelines.</p>","title":"Compute environment"},{"location":"compute-envs/altair-pbs-pro/#advanced-options","tags":["pbs pro","altair","compute environment"],"text":"<ul> <li> <p>Use the Nextflow queue size to limit the number of jobs that Nextflow can submit to the scheduler at the same time.</p> </li> <li> <p>Use the Head job submit options to specify PBS options for the head job. You can optionally apply these options to compute jobs as well:</p> </li> </ul> <p></p>","title":"Advanced options"},{"location":"compute-envs/aws-batch/","tags":["aws","batch","compute environment"],"text":"<p>Requirements</p> <p>This guide assumes you have an existing Amazon Web Service (AWS) account.</p>  <p>There are two ways to create a compute environment for AWS Batch with Tower:</p> <ol> <li> <p>Tower Forge: This option automatically creates the AWS Batch resources in your AWS account. This eliminates the need to set up your AWS Batch infrastructure manually. </p> </li> <li> <p>Manual: This option allows Tower to use existing AWS Batch resources.</p> </li> </ol>","title":"AWS Batch"},{"location":"compute-envs/aws-batch/#tower-forge","tags":["aws","batch","compute environment"],"text":"<p>Tower Forge automates the configuration of an AWS Batch compute environment and the queues required for deploying Nextflow pipelines. Note that this option will automatically create resources in your AWS account that you may be charged for by AWS.</p>","title":"Tower Forge"},{"location":"compute-envs/aws-batch/#iam","tags":["aws","batch","compute environment"],"text":"<p>To use the Tower Forge feature, Tower requires an Identity and Access Management (IAM) user with the permissions listed in this policy file. These authorizations are more permissive than those required to only launch a pipeline, since Tower needs to manage AWS resources on your behalf. Note that launch permissions also require the S3 storage write permissions in this policy file.</p> <p>We recommend creating separate IAM policies for Tower Forge and Tower launch permissions using the policy files linked above. These policies can then be assigned to the Tower IAM user.</p>","title":"IAM"},{"location":"compute-envs/aws-batch/#create-tower-iam-policies","tags":["aws","batch","compute environment"],"text":"<ol> <li> <p>Open the AWS IAM console.</p> </li> <li> <p>From the left navigation menu, select Policies under Access management.</p> </li> <li> <p>Select Create policy.</p> </li> <li> <p>On the Create policy page, select the JSON tab.</p> </li> <li> <p>Copy the contents of your policy JSON file (Forge or Launch, depending on the policy being created) and replace the default text in the policy editor area under the JSON tab. To create a Launch user, you must also create the S3 bucket write policy separately to attach to your Launch user.</p> </li> <li> <p>Select Next: Tags.</p> </li> <li> <p>Select Next: Review.</p> </li> <li> <p>Enter a name and description for the policy on the Review policy page, then select Create policy.</p> </li> <li> <p>Repeat these steps for both the <code>forge-policy.json</code> and <code>launch-policy.json</code> files. For a Launch user, also create the <code>s3-bucket-write-policy.json</code> listed in step 5 above.</p> </li> </ol>","title":"Create Tower IAM policies"},{"location":"compute-envs/aws-batch/#create-an-iam-user","tags":["aws","batch","compute environment"],"text":"<ol> <li> <p>From the AWS IAM console, select Users in the left navigation menu, then select Add User at the top rigt of the page.</p> </li> <li> <p>Enter a name for your user (e.g. <code>tower</code>) and select the Programmatic access type.</p> </li> <li> <p>Select Next: Permissions.</p> </li> <li> <p>Select Next: Tags, then Next: Review and Create User.</p>  <p>This user has no permissions</p> <p>For the time being, you can ignore the warning. Permissions will be applied using the IAM Policy.</p>  </li> <li> <p>Save the Access key ID and Secret access key in a secure location as we will use these in the next section.</p> </li> <li> <p>Once you have saved the keys, select Close.</p> </li> <li> <p>Back in the users table, select the newly created user,then select Add permissions under the Permissions tab.</p> </li> <li> <p>Select Attach existing policies, search for the policies created previously, and select each one.</p> </li> <li> <p>Select Next: Review.</p> </li> <li> <p>Select Add permissions.</p> </li> </ol>","title":"Create an IAM user"},{"location":"compute-envs/aws-batch/#s3-bucket","tags":["aws","batch","compute environment"],"text":"<p>S3 (Simple Storage Service) is a type of object storage. To access files and store the results for our pipelines, we have to create an S3 bucket and grant our new Tower IAM user access to it.</p> <ol> <li> <p>Navigate to the S3 service.</p> </li> <li> <p>Select Create New Bucket.</p> </li> <li> <p>Enter a unique name for your bucket and select a region.</p>  <p>Which AWS region should I use?</p> <p>To maximize data transfer resilience and minimize cost, storage should be in the same region as compute.</p>  </li> <li> <p>Select the default options for Configure options.</p> </li> <li> <p>Select the default options for Set permissions.</p> </li> <li> <p>Review and select Create bucket.</p>  <p>S3 Storage Costs</p> <p>S3 is used by Nextflow for the storage of intermediate files. In production pipelines, this can amount to a large quantity of data. To reduce costs, consider using a retention policy when creating a bucket, such as automatically deleting intermediate files after 30 days. See here for more information.</p>  </li> </ol>","title":"S3 Bucket"},{"location":"compute-envs/aws-batch/#compute-environment","tags":["aws","batch","compute environment"],"text":"<p>Tower Forge automates the configuration of an AWS Batch compute environment and queues required for the deployment of Nextflow pipelines.</p> <p>Once the AWS resources are set up, we can add a new AWS Batch environment in Tower. To create a new compute environment:</p> <ol> <li> <p>In a workspace, select Compute environments and then New environment.</p> </li> <li> <p>Enter a descriptive name for this environment, e.g., \"AWS Batch Spot (eu-west-1)\".</p> </li> <li> <p>Select Amazon Batch as the target platform.</p> </li> <li> <p>From the Credentials drop-down, select existing AWS credentials, or add new credentials by selecting the + button. If you select to use existing credentials, skip to step 7.</p> </li> <li> <p>Enter a name, e.g. \"AWS Credentials\".</p> </li> <li> <p>Add the Access key and Secret key. These are the keys you saved previously when you created the AWS IAM user.</p>  <p>Multiple credentials</p> <p>You can create multiple credentials in your Tower environment.</p>  </li> <li> <p>Select a Region, e.g., \"eu-west-1 - Europe (Ireland)\".</p> </li> <li> <p>Enter the S3 bucket path created in the previous section to the Pipeline work directory field, e.g. <code>s3://unique-tower-bucket</code>.</p>  <p>Warning</p> <p>The bucket should be in the same region selected in the previous step.</p>  </li> <li> <p>Select Enable Wave containers to facilitate access to private container repositories and provision containers in your pipelines using the Wave containers service. See Wave containers for more information.</p> </li> <li> <p>Select Enable Fusion v2 to allow access to your S3-hosted data via the Fusion v2 virtual distributed file system. This speeds up most data operations. The Fusion v2 file system requires Wave containers to be enabled (see above). See Fusion file system for configuration details.</p>  <p>Fusion v2 defaults</p> <p>When using Fusion v2 without fast instance storage (see below), the following EBS settings are applied to optimize file system performance:</p> <ul> <li>EBS autoscaling is disabled</li> <li>EBS boot disk size is increased to 100 GB</li> <li>EBS boot disk type GP3 is selected</li> <li>EBS boot disk throughput is increased to 325 MB/s  </li> </ul> <p>Extensive benchmarking of Fusion v2 has demonstrated that the increased cost associated with these settings are generally outweighed by the costs saved due to decreased run time. </p>  </li> <li> <p>Select Enable fast instance storage to allow the use of NVMe instance storage to speed up I/O and disk access operations. NVMe instance storage requires Fusion v2 to be enabled (see above).</p>  <p>Note</p> <p>Fast instance storage requires an EC2 instance type that uses NVMe disks. Tower validates any instance types you specify (from Advanced options &gt; Instance types) during compute environment creation. If you do not specify an instance type, a standard EC2 instance with NVMe disks will be used (<code>'c5ad', 'c5d', 'c6id', 'i3', 'i4i', 'm5ad', 'm5d', 'm6id', 'r5ad', 'r5d', 'r6id'</code> EC2 instance families) for fast storage.</p>  </li> <li> <p>Set the Config mode to Tower Forge.</p> </li> <li> <p>Select a Provisioning model. In most cases this will be Spot.</p>  <p>Spot or On-demand?</p> <p>You can choose to create a compute environment that launches either Spot or On-demand instances. Spot instances can cost as little as 20% of on-demand instances, and with Nextflow's ability to automatically relaunch failed tasks, Spot is almost always the recommended provisioning model.</p> <p>Note, however, that when choosing Spot instances, Tower will also create a dedicated queue for running the main Nextflow job using a single on-demand instance in order to prevent any execution interruptions.</p>  </li> <li> <p>Enter the Max CPUs e.g. <code>64</code>. This is the maximum number of combined CPUs (the sum of all instances CPUs) AWS Batch will provision at any time.</p> </li> <li> <p>Select EBS Auto scale to allow the EC2 virtual machines to dynamically expand the amount of available disk space during task execution.</p>  <p>EBS autoscaling may cause unattached volumes on large clusters</p> <p>When running large AWS Batch clusters (hundreds of compute nodes or more), EC2 API rate limits may cause the deletion of unattached EBS volumes to fail. Volumes that remain active after Nextflow jobs have completed will incur additional costs, and should be manually deleted. Monitor your AWS account for any orphaned EBS volumes via the EC2 console, or with a Lambda function. See here for more information.</p>  </li> <li> <p>With the optional Enable Fusion mounts (deprecated) feature enabled, S3 buckets specified in Pipeline work directory and Allowed S3 Buckets are mounted as file system volumes in the EC2 instances carrying out the Batch job execution. These buckets can then be accessed at <code>/fusion/s3/&lt;bucket-name&gt;</code>. For example, if the bucket name is <code>s3://imputation-gp2</code>, your pipeline will access it using the file system path <code>/fusion/s3/imputation-gp2</code>. Note: This feature has been deprecated. Consider using Fusion v2 (see above) for enhanced performance and stability.</p>  <p>Tip</p> <p>You do not need to modify your pipeline or files to take advantage of this feature. Nextflow will automatically recognize and replace any reference to files prefixed with <code>s3://</code> with the corresponding Fusion mount paths.</p>  </li> <li> <p>Select Enable GPUs if you intend to run GPU-dependent workflows in the compute environment. See GPU usage for more information.</p> </li> <li> <p>Enter any additional Allowed S3 buckets that your workflows require to read input data or write output data. The Pipeline work directory bucket above is added by default to the list of Allowed S3 buckets.</p> </li> <li> <p>To use EFS, you can either select Use existing EFS file system and specify an existing EFS instance, or select Create new EFS file system to create one. If you intend to use the EFS file system as your work directory, you will need to specify <code>&lt;your_EFS_mount_path&gt;/work</code> in the Pipeline work directory field (step 8 of this guide).</p> <ul> <li>To use an existing EFS file system, enter the EFS file system id and EFS mount path. This is the path where the EFS volume is accessible to the compute environment. For simplicity, we advise that you use <code>/mnt/efs</code> as the EFS mount path.</li> <li>To create a new EFS file system, enter the EFS mount path. We advise that you specify <code>/mnt/efs</code> as the EFS mount path.</li> <li>EFS file systems created by Tower Forge are automatically tagged in AWS with <code>Name=TowerForge-&lt;id&gt;</code>, with <code>&lt;id&gt;</code> being the compute environment ID. Any manually added Tower resource label with the key <code>Name</code> (capital N) will override the automatically-assigned <code>TowerForge-&lt;id&gt;</code> label. </li> </ul> </li> <li> <p>To use FSx for Lustre, you can either select Use existing FSx file system and specify an existing FSx instance, or select Create new FSx file system to create one. If you intend to use the FSx file system as your work directory, you will need to specify <code>&lt;your_FSx_mount_path&gt;/work</code> in the Pipeline work directory field (step 8 of this guide).</p> <ul> <li>To use an existing FSx file system, enter the FSx DNS name and FSx mount path. The FSx mount path is the path where the FSx volume is accessible to the compute environment. For simplicity, we advise that you use <code>/mnt/fsx</code> as the FSx mount path.</li> <li>To create a new FSx file system, enter the FSx size (in GB) and the FSx mount path. We advise that you specify <code>/mnt/fsx</code> as the FSx mount path.</li> <li>FSx file systems created by Tower Forge are automatically tagged in AWS with <code>Name=TowerForge-&lt;id&gt;</code>, with <code>&lt;id&gt;</code> being the compute environment ID. Any manually added Tower resource label with the key <code>Name</code> (capital N) will override the automatically-assigned <code>TowerForge-&lt;id&gt;</code> label. </li> </ul> </li> <li> <p>Select Dispose resources if you want Tower to automatically delete these AWS resources if you delete the compute environment in Tower.</p> </li> <li> <p>Apply Resource labels to the cloud resources consumed by this compute environment. Workspace default resource labels are prefilled. </p> </li> <li> <p>Expand Staging options to include optional pre- or post-run Bash scripts that execute before or after the Nextflow pipeline execution in your environment. </p> </li> <li> <p>You can use the Environment variables option to specify custom environment variables for the Head job and/or Compute jobs.</p> </li> <li> <p>Configure any advanced options described below, as needed.</p> </li> <li> <p>Select Create to finalize the compute environment setup. It will take a few seconds for all the resources to be created, and then you will be ready to launch pipelines.</p> </li> </ol> <p>Jump to the documentation for launching pipelines.</p>","title":"Compute environment"},{"location":"compute-envs/aws-batch/#advanced-options","tags":["aws","batch","compute environment"],"text":"<ul> <li> <p>Specify the Allocation strategy and indicate any preferred Instance types.</p> </li> <li> <p>Configure a custom networking setup using the VPC ID, Subnets, and Security groups fields.</p> </li> <li> <p>You can specify a custom AMI ID.</p>  <p>Requirements for custom AMI</p> <p>To use a custom AMI, make sure the AMI is based on an Amazon Linux-2 ECS optimized image that meets the Batch requirements. To learn more about approved versions of the Amazon ECS optimized AMI, see this AWS guide</p>   <p>GPU-enabled AMI</p> <p>If a custom AMI is specified and the Enable GPU option is also selected, the custom AMI will be used instead of the AWS-recommended GPU-optimized AMI.</p>  </li> <li> <p>If you need to debug the EC2 instance provisioned by AWS Batch, specify a Key pair to log in to the instance via SSH.</p> </li> <li> <p>You can set Min CPUs to be greater than <code>0</code>, in which case some EC2 instances will remain active. An advantage of this is that pipeline executions will initialize faster.</p>  <p>Increasing Min CPUs may increase AWS costs</p> <p>Keeping EC2 instances running may result in additional costs. You will be billed for these running EC2 instances regardless of whether you are executing pipelines or not.</p>  </li> <li> <p>Use Head Job CPUs and Head Job Memory to specify the hardware resources allocated for the Head Job.</p> </li> <li> <p>Use Head Job role and Compute Job role to grant fine-grained IAM permissions to the Head Job and Compute Jobs.</p> </li> <li> <p>Add an execution role ARN to the Batch execution role field to grant permissions to make API calls on your behalf to the ECS container used by Batch. This is required if the pipeline launched with this compute environment needs access to the secrets stored in this workspace. This field can be ignored if you are not using secrets.</p> </li> <li> <p>Specify an EBS block size (in GB) in the EBS auto-expandable block size field to control the initial size of the EBS auto-expandable volume. New blocks of this size are added when the volume begins to run out of free space.</p> </li> <li> <p>Enter the Boot disk size (in GB) to specify the size of the boot disk in the VMs created by this compute environment.</p> </li> <li> <p>If you're using Spot instances, you can also specify the Cost percentage, which is the maximum allowed price of a Spot instance as a percentage of the On-Demand price for that instance type. Spot instances will not be launched until the current spot price is below the specified cost percentage.</p> </li> <li> <p>Use AWS CLI tool path to specify the location of the <code>aws</code> CLI.</p> </li> <li> <p>Specify a CloudWatch Log group for the <code>awslogs</code> driver to stream the logs entry to an existing Log group in Cloudwatch.</p> </li> <li> <p>Specify a custom ECS agent configuration for the ECS agent parameters used by AWS Batch. This is appended to the <code>/etc/ecs/ecs.config</code> file in each cluster node.</p>  <p>Note</p> <p>Altering this file may result in a malfunctioning Tower Forge compute environment. See Amazon ECS container agent configuration to learn more about the available parameters.</p>  </li> </ul>","title":"Advanced options"},{"location":"compute-envs/aws-batch/#manual","tags":["aws","batch","compute environment"],"text":"<p>This section is for users with a pre-configured AWS environment. You will need a Batch queue, a Batch compute environment, an IAM user and an S3 bucket already set up.</p> <p>To enable Tower within your existing AWS configuration, you need to have an IAM user with the following IAM permissions:</p> <ul> <li><code>AmazonS3ReadOnlyAccess</code></li> <li><code>AmazonEC2ContainerRegistryReadOnly</code></li> <li><code>CloudWatchLogsReadOnlyAccess</code></li> <li>A custom policy to grant the ability to submit and control Batch jobs.</li> <li>Write access to any S3 bucket used by pipelines with the following policy template. See below for details.</li> </ul> <p>With these permissions set, we can add a new AWS Batch compute environment in Tower.</p>","title":"Manual"},{"location":"compute-envs/aws-batch/#access-to-s3-buckets","tags":["aws","batch","compute environment"],"text":"<p>Tower can use S3 to store intermediate and output data generated by pipelines. You need to create a policy for your Tower IAM user that grants access to specific buckets.</p> <ol> <li> <p>Go to the IAM User table in the IAM service</p> </li> <li> <p>Select the IAM user.</p> </li> <li> <p>Select Add inline policy.</p> </li> <li> <p>Copy the contents of this policy into the JSON tab. Replace <code>YOUR-BUCKET-NAME</code> (lines 10 and 21) with your bucket name.</p> </li> <li> <p>Name your policy and select Create policy.</p> </li> </ol>","title":"Access to S3 Buckets"},{"location":"compute-envs/aws-batch/#compute-environment_1","tags":["aws","batch","compute environment"],"text":"<p>To create a new compute environment for AWS Batch (without Forge):</p> <ol> <li> <p>In a workspace, select Compute environments and then New environment.</p> </li> <li> <p>Enter a descriptive name for this environment, e.g., \"AWS Batch Manual (eu-west-1)\".</p> </li> <li> <p>Select Amazon Batch as the target platform.</p> </li> <li> <p>Add new credentials by selecting the + button.</p> </li> <li> <p>Enter a name for the credentials, e.g., \"AWS Credentials\".</p> </li> <li> <p>Enter the Access key and Secret key for your IAM user.</p>  <p>Multiple credentials</p> <p>You can create multiple credentials in your Tower environment. See the Credentials section.</p>  </li> <li> <p>Select a Region, e.g., \"eu-west-1 - Europe (Ireland)\".</p> </li> <li> <p>Enter an S3 bucket path for the Pipeline work directory, e.g., <code>s3://tower-bucket</code>.</p> </li> <li> <p>Set the Config mode to Manual.</p> </li> <li> <p>Enter the Head queue, which is the name of the AWS Batch queue that the Nextflow driver job will run.</p> </li> <li> <p>Enter the Compute queue, which is the name of the AWS Batch queue that tasks will be submitted to.</p> </li> <li> <p>Use the Environment variables option to specify custom environment variables for the Head job and/or Compute jobs.</p> </li> <li> <p>Configure any advanced options described below, as needed.</p> </li> <li> <p>Select Create to finalize the compute environment setup.</p> </li> </ol> <p>Jump to the documentation for launching pipelines.</p>","title":"Compute environment"},{"location":"compute-envs/aws-batch/#advanced-options_1","tags":["aws","batch","compute environment"],"text":"<ul> <li> <p>Use Head Job CPUs and Head Job Memory to specify the hardware resources allocated for the Head Job.</p> </li> <li> <p>Use Head Job role and Compute Job role to grant fine-grained IAM permissions to the Head Job and Compute Jobs</p> </li> <li> <p>Add an execution role ARN to the Batch execution role field to grant permissions to make API calls on your behalf to the ECS container used by Batch. This is required if the pipeline launched with this compute environment needs access to the secrets stored in this workspace. This field can be ignored if you are not using secrets.</p> </li> <li> <p>Use AWS CLI tool path to specify the location of the <code>aws</code> CLI.</p> </li> <li> <p>Specify a CloudWatch Log group for the <code>awslogs</code> driver to stream the logs entry to an existing Log group in Cloudwatch.</p> </li> </ul>","title":"Advanced options"},{"location":"compute-envs/azure-batch/","tags":["batch","azure","compute environment"],"text":"<p>Warning</p> <p>The Tower support for Azure Batch is currently in beta. Any feedback and suggestions are welcome.</p> <p>Depending on your region and subscription type, a newly-created account may not be entitled to any VMs without first making a service request to Azure. See Azure Batch service quotas and limits for more information.</p>   <p>Note</p> <p>This guide assumes you have an existing Azure account.</p>  <p>There are two ways to create an Azure Batch compute environment in Tower:</p> <ul> <li> <p>Tower Forge: This option automatically creates the Azure Batch resources needed for your Tower compute environment. This eliminates the need to set up your Azure Batch infrastructure manually.</p> </li> <li> <p>Manual: This option allows Tower to use existing Azure Batch resources.</p> </li> </ul>","title":"Azure Batch"},{"location":"compute-envs/azure-batch/#tower-forge","tags":["batch","azure","compute environment"],"text":"<p>Tower Forge creates the Azure Batch resources needed for your compute environment, recommended if you do not yet have an Azure Batch environment fully set up. Note that this will create resources that may have associated costs in your Azure account.</p>","title":"Tower Forge"},{"location":"compute-envs/azure-batch/#resource-group","tags":["batch","azure","compute environment"],"text":"<p>To create Azure Batch and Azure Storage accounts, first create a resource group in the region of your choice.</p> <p>If you are logged in to your Azure account, select Create new resource group on this page.</p> <ol> <li> <p>Enter a name for the resource group (e.g. <code>towerrg</code>).</p> </li> <li> <p>Select the preferred region for this resource group.</p> </li> <li> <p>Select Review and Create to proceed to the review screen.</p> </li> <li> <p>Select Create to create the resources.</p> </li> </ol>","title":"Resource group"},{"location":"compute-envs/azure-batch/#storage-account","tags":["batch","azure","compute environment"],"text":"<p>The next step is to create the necessary Azure Storage.</p> <p>If you are logged in to your Azure account, select Create a storage account on this page.</p> <ol> <li> <p>Enter a name for the storage account (e.g., <code>towerrgstorage</code>).</p> </li> <li> <p>Select the preferred region for this storage account.</p> </li> <li> <p>Select Review and Create to proceed to the review screen.</p> </li> <li> <p>Select Create to create the Azure Storage account.</p> </li> <li> <p>Navigate to your new storage account and select Container.</p> </li> <li> <p>Create a new Blob container by selecting + Container.</p> <p>A new container dialogue will open. Enter a suitable name (e.g. <code>towerrgstorage-container</code>).</p> </li> <li> <p>Once the new Blob container is created, navigate to the Access Keys section of the storage account (<code>towerrgstorage</code> in this example).</p> </li> <li> <p>Store the access keys for the newly created Azure Storage account.</p>  <p>Note</p> <p>Blob container storage credentials are associated with the Batch pool configuration when it is created. Once your compute environment has been created with Tower Forge, these credentials should not be changed in Tower.</p>  </li> </ol>","title":"Storage account"},{"location":"compute-envs/azure-batch/#batch-account","tags":["batch","azure","compute environment"],"text":"<p>The next step is to create the necessary Batch account.</p> <p>If you are logged in to your Azure account, select Create a batch account on this page.</p> <ol> <li> <p>Enter a name for the Batch account (e.g. <code>towerrgbatch</code>).</p> </li> <li> <p>Select the preferred region for this Batch account.</p> </li> <li> <p>Select Review and Create to proceed to the review screen.</p> </li> <li> <p>Select Create to create the Azure Batch account.</p> </li> </ol>","title":"Batch account"},{"location":"compute-envs/azure-batch/#compute-environment","tags":["batch","azure","compute environment"],"text":"<p>Tower Forge automates the configuration of an Azure Batch compute environment and the queues required for the deployment of Nextflow pipelines.</p> <p>Once the Azure resources are set up, add a new compute environment in Tower:</p> <ol> <li> <p>In a workspace, select Compute Environments and then New Environment.</p> </li> <li> <p>Enter a descriptive name for this environment, e.g. \"Azure Batch (east-us)\"</p> </li> <li> <p>Select Azure Batch as the target platform.</p> </li> <li> <p>From the Credentials drop-down, select existing Azure credentials, or select + to add new credentials. If you have existing credentials, skip to step 7.</p> </li> <li> <p>Enter a name, e.g. \"Azure Credentials\".</p> </li> <li> <p>Add the Batch account and Blob Storage credentials that we created previously.</p>  <p>Multiple credentials</p> <p>You can create multiple credentials in your Tower environment.</p>  </li> <li> <p>Select a Region, for example \"eastus (East US)\".</p> </li> <li> <p>In the Pipeline work directory field, enter the Azure blob container created previously, e.g., <code>az://towerrgstorage-container/work</code>.</p>  <p>Warning</p> <p>The blob container must be in the same Region from step 7.</p>  </li> <li> <p>Set the Config mode to Batch Forge.</p> </li> <li> <p>Enter the default VM type, depending on your quota limits. The default is <code>Standard_D4_v3</code>.</p> </li> <li> <p>Enter the VMs count. This is the number of VMs you wish to deploy.</p> </li> <li> <p>Enable Autoscale if you'd like to scale up and down automatically based on the number of pipeline tasks. The number of VMs will vary from 0 to VMs count.</p> </li> <li> <p>Enable Dispose resources if you'd like Tower to automatically delete the Batch pool once the workflow is complete.</p> </li> <li> <p>Select or create a Container registry credential to authenticate to an Azure registry (used by the Wave containers service). </p> </li> <li> <p>Apply Resource labels to the cloud resources consumed by this compute environment. Workspace default resource labels are prefilled. </p> </li> <li> <p>Expand Staging options to include optional pre- or post-run Bash scripts that execute before or after the Nextflow pipeline execution in your environment. </p> </li> <li> <p>You can use the Environment variables option to specify custom environment variables for the Head job and/or Compute jobs.</p> </li> <li> <p>Configure any advanced options (see below), as needed.</p> </li> <li> <p>Select Create to finalize the compute environment setup. It will take a few seconds for all the resources to be created before the compute environment is ready to launch pipelines.</p> </li> </ol> <p>Jump to the documentation for launching pipelines.</p>","title":"Compute environment"},{"location":"compute-envs/azure-batch/#advanced-options","tags":["batch","azure","compute environment"],"text":"<ul> <li> <p>Use Jobs cleanup policy to control how jobs are deleted upon workflow completion.</p> </li> <li> <p>Use Token duration to control the duration of the SAS token generated by Nextflow.</p> </li> </ul>","title":"Advanced options"},{"location":"compute-envs/azure-batch/#manual","tags":["batch","azure","compute environment"],"text":"<p>This section is for users with a pre-configured Azure environment. You will need an Azure Batch account and Storage account already set up.</p> <p>To create a new compute environment for AWS Batch (without Forge):</p> <ol> <li> <p>In a workspace, select Compute Environments, then New Environment.</p> </li> <li> <p>Enter a descriptive name for this environment, e.g. \"Azure Batch (east-us)\".</p> </li> <li> <p>Select Azure Batch as the target platform.</p> </li> <li> <p>Select your existing Azure credentials or add new credentials by selecting the + button. If you are using existing credentials, skip to step 7.</p> </li> <li> <p>Enter a name, e.g. \"Azure Credentials\".</p> </li> <li> <p>Add the Batch account and Blob Storage credentials that we created previously.</p>  <p>Multiple credentials</p> <p>You can create multiple credentials in your Tower environment.</p>  </li> <li> <p>Select a Region, for example \"eastus (East US)\".</p> </li> <li> <p>In the Pipeline work directory field, add the Azure blob container created previously, e.g. <code>az://towerrgstorage-container/work</code>.</p>  <p>Warning</p> <p>The Blob container must be in the same Region specified in step 7 above.</p>  </li> <li> <p>Set the Config mode to Manual.</p> </li> <li> <p>Enter the Compute Pool name. This is the name of the Azure Batch pool provided to you by your Azure administrator.</p> </li> <li> <p>Use the Environment variables option to specify custom environment variables for the Head job and/or Compute jobs.</p> </li> <li> <p>Configure any advanced options described below, as needed.</p> </li> <li> <p>Select Create to finalize the compute environment setup. It will take a few seconds for all the resources to be created, and then you will be ready to launch pipelines.</p> </li> </ol> <p>Jump to the documentation for launching pipelines.</p>","title":"Manual"},{"location":"compute-envs/azure-batch/#advanced-options_1","tags":["batch","azure","compute environment"],"text":"<ul> <li> <p>Use Jobs cleanup policy to control how jobs are deleted upon workflow completion.</p> </li> <li> <p>Use Token duration to control the duration of the SAS token generated by Nextflow.</p> </li> </ul>","title":"Advanced options"},{"location":"compute-envs/eks/","tags":["eks","amazon","compute environment"],"text":"<p>Amazon EKS is a managed Kubernetes cluster that enables the execution of containerized workloads in the AWS cloud at scale.</p> <p>Tower offers native support for Amazon EKS clusters to streamline the deployment of Nextflow pipelines.</p>","title":"Amazon EKS"},{"location":"compute-envs/eks/#requirements","tags":["eks","amazon","compute environment"],"text":"<p>You must have an EKS cluster up and running. Follow the cluster preparation instructions to create the resources required by Tower. In addition to the generic Kubernetes instructions, you must make a number of EKS-specific modifications.</p>","title":"Requirements"},{"location":"compute-envs/eks/#assign-service-account-role-to-iam-user","tags":["eks","amazon","compute environment"],"text":"<p>You will need to assign the service role to an AWS user that will be used by Tower to access the EKS cluster.</p> <p>First, modify the EKS auth configuration:</p> <pre>1</pre><pre><code>kubectl edit configmap -n kube-system aws-auth\n</code></pre> <p>Once the editor is open, add this entry:</p> <pre>1\n2\n3\n4\n5</pre><pre><code>mapUsers: |\n  - userarn: &lt;AWS USER ARN&gt;\n    username: tower-launcher-user\n    groups:\n      - tower-launcher-role\n</code></pre> <p>Retrieve your user ARN from the AWS IAM console, or with the AWS CLI:</p> <pre>1</pre><pre><code>aws sts get-caller-identity\n</code></pre>  <p>Note</p> <p>The same user must be used when specifying the AWS credentials in the Tower compute environment configuration.</p>  <p>The AWS user must have the following IAM policy:</p>  eks-iam-policy.json <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14</pre><pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n      {\n        \"Sid\": \"TowerEks0\",\n        \"Effect\": \"Allow\",\n        \"Action\": [\n          \"eks:ListClusters\",\n          \"eks:DescribeCluster\"\n        ],\n        \"Resource\": \"*\"\n      }\n    ]\n  }\n</code></pre>  <p>See the AWS documentation for more details.</p>","title":"Assign service account role to IAM user"},{"location":"compute-envs/eks/#compute-environment","tags":["eks","amazon","compute environment"],"text":"<ol> <li> <p>In a workspace, select Compute environments and then New environment.</p> </li> <li> <p>Enter a descriptive name for this environment, e.g., \"Amazon EKS (eu-west-1)\".</p> </li> <li> <p>Select Amazon EKS as the target platform.</p> </li> <li> <p>From the Credentials drop-down, select existing AWS credentials, or add new credentials by selecting the + button. If you select to use existing credentials, skip to step 7.</p>  <p>Note</p> <p>The user must have the IAM permissions required to describe and list EKS clusters as explained here.</p>  </li> <li> <p>Select a Region, e.g., \"eu-west-1 - Europe (Ireland)\".</p> </li> <li> <p>Select a Cluster name from the list of available EKS clusters in the selected region.</p> </li> <li> <p>Specify the Namespace created in the cluster preparation instructions, which is <code>tower-nf</code> by default.</p> </li> <li> <p>Specify the Head service account created in the cluster preparation instructions, which is <code>tower-launcher-sa</code> by default.</p> </li> <li> <p>Specify the Storage claim created in the cluster preparation instructions, which serves as a scratch filesystem for Nextflow pipelines. The storage claim is called <code>tower-scratch</code> in each of the provided examples.</p> </li> <li> <p>Apply Resource labels to the cloud resources consumed by this compute environment. Workspace default resource labels are prefilled. </p> </li> <li> <p>Expand Staging options to include optional pre- or post-run Bash scripts that execute before or after the Nextflow pipeline execution in your environment. </p> </li> <li> <p>Use the Environment variables option to specify custom environment variables for the Head job and/or Compute jobs.</p> </li> <li> <p>Configure any advanced options described below, as needed.</p> </li> <li> <p>Select Create to finalize the compute environment setup.</p> </li> </ol> <p>Jump to the documentation for launching pipelines.</p>","title":"Compute environment"},{"location":"compute-envs/eks/#advanced-options","tags":["eks","amazon","compute environment"],"text":"<ul> <li> <p>The Storage mount path is the file system path where the Storage claim is mounted (default: <code>/scratch</code>).</p> </li> <li> <p>The Work directory is the file system path used as a working directory by Nextflow pipelines. This must be the storage mount path (default) or a subdirectory of it.</p> </li> <li> <p>The Compute service account is the service account used by Nextflow to submit tasks (default: the <code>default</code> account in the given namespace).</p> </li> <li> <p>The Pod cleanup policy determines when to delete terminated pods.</p> </li> <li> <p>Use Custom head pod specs to provide custom options for the Nextflow workflow pod (<code>nodeSelector</code>, <code>affinity</code>, etc). For example:</p> </li> </ul> <pre>1\n2\n3</pre><pre><code>spec:\n  nodeSelector:\n    disktype: ssd\n</code></pre> <ul> <li> <p>Use Custom service pod specs to provide custom options for the compute environment pod. See above for an example.</p> </li> <li> <p>Use Head Job CPUs and Head Job memory to specify the hardware resources allocated for the Nextflow workflow pod.</p> </li> </ul>","title":"Advanced options"},{"location":"compute-envs/gke/","tags":["gke","google","compute environment"],"text":"<p>Google Kubernetes Engine (GKE) is a managed Kubernetes cluster that allows the execution of containerized workloads in Google Cloud at scale.</p> <p>Tower offers native support for GKE clusters and streamlines the deployment of Nextflow pipelines in such environments.</p>","title":"Google GKE"},{"location":"compute-envs/gke/#requirements","tags":["gke","google","compute environment"],"text":"<p>See here for instructions to set up your Google Cloud account and any other services (such as Cloud Storage) that you intend to use.</p> <p>You need to have a GKE cluster up and running. Make sure you have followed the cluster preparation instructions to create the cluster resources required by Tower. In addition to the generic Kubernetes instructions, you will need to make a few modifications specific to GKE.</p>","title":"Requirements"},{"location":"compute-envs/gke/#assign-service-account-role-to-iam-user","tags":["gke","google","compute environment"],"text":"<p>You must grant the cluster access to the service account used to authenticate the Tower compute environment. This can be done by updating the role binding:</p> <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16</pre><pre><code>cat &lt;&lt; EOF | kubectl apply -f -\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: tower-launcher-userbind\nsubjects:\n  - kind: User\n    name: &lt;IAM SERVICE ACCOUNT&gt;\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: tower-launcher-role\n  apiGroup: rbac.authorization.k8s.io\n---\nEOF\n</code></pre> <p>In the above snippet, replace <code>&lt;IAM SERVICE ACCOUNT&gt;</code> with the corresponding service account, e.g. <code>test-account@test-project-123456.google.com.iam.gserviceaccount.com</code>.</p> <p>For more details, refer to the Google documentation.</p>","title":"Assign service account role to IAM user"},{"location":"compute-envs/gke/#compute-environment","tags":["gke","google","compute environment"],"text":"<ol> <li> <p>In a Tower workspace, select Compute environments and then New environment.</p> </li> <li> <p>Enter a descriptive name for this environment, e.g., \"Google GKE (europe-west1)\".</p> </li> <li> <p>From the Provider drop-down, select Google GKE.</p> </li> <li> <p>From the Credentials drop-down, select existing GKE credentials, or add new credentials by selecting the + button. If you select to use existing credentials, skip to step 7.</p> </li> <li> <p>Enter a name for the credentials, e.g., \"GKE Credentials\".</p> </li> <li> <p>Enter the Service account key for your Google Service account.</p>  <p>Multiple credentials</p> <p>You can create multiple credentials in your Tower environment.</p>  </li> <li> <p>Select the Location of your GKE cluster.</p>  <p>Regional and zonal clusters</p> <p>GKE clusters can be either regional or zonal. For example, <code>us-west1</code> identifies the United States West-Coast region, which has three zones: <code>us-west1-a</code>, <code>us-west1-b</code>, and <code>us-west1-c</code>. Tower self-completion only shows regions. You should manually edit this field if you are using a zonal GKE cluster. </p>  </li> <li> <p>Select or enter the Cluster name of your GKE cluster.</p> </li> <li> <p>Specify the Namespace created in the cluster preparation instructions. This is <code>tower-nf</code> by default.</p> </li> <li> <p>Specify the Head service account created in the cluster preparation instructions. This is <code>tower-launcher-sa</code> by default.</p> </li> <li> <p>Specify the Storage claim created in the cluster preparation instructions. This serves as a scratch filesystem for Nextflow pipelines. The storage claim is called <code>tower-scratch</code> in each of the provided examples.</p> </li> <li> <p>Apply Resource labels to the cloud resources consumed by this compute environment. Workspace default resource labels are prefilled. </p> </li> <li> <p>Expand Staging options to include optional pre- or post-run Bash scripts that execute before or after the Nextflow pipeline execution in your environment. </p> </li> <li> <p>You can use the Environment variables option to specify custom environment variables for the Head job and/or Compute jobs.</p> </li> <li> <p>Configure any advanced options described below, as needed.</p> </li> <li> <p>Select Create to finalize the compute environment setup.</p> </li> </ol> <p>Jump to the documentation for launching pipelines.</p>","title":"Compute Environment"},{"location":"compute-envs/gke/#advanced-options","tags":["gke","google","compute environment"],"text":"<ul> <li> <p>The Storage mount path is the file system path where the Storage claim is mounted (default: <code>/scratch</code>).</p> </li> <li> <p>The Work directory is the file system path used as a working directory by Nextflow pipelines. It must be the storage mount path (default) or a subdirectory of it.</p> </li> <li> <p>The Compute service account is the service account used by Nextflow to submit tasks (default: the <code>default</code> account in the given namespace).</p> </li> <li> <p>The Pod cleanup policy determines when to delete terminated pods.</p> </li> <li> <p>Use Custom head pod specs to provide custom options for the Nextflow workflow pod (<code>nodeSelector</code>, <code>affinity</code>, etc). For example:</p> </li> </ul> <pre>1\n2\n3</pre><pre><code>spec:\n  nodeSelector:\n    disktype: ssd\n</code></pre> <ul> <li> <p>Use Custom service pod specs to provide custom options for the compute environment pod. See above for an example.</p> </li> <li> <p>Use Head Job CPUs and Head Job Memory to specify the hardware resources allocated for the Nextflow workflow pod.</p> </li> </ul>","title":"Advanced options"},{"location":"compute-envs/google-cloud-batch/","tags":["google","batch","gcp","compute environment"],"text":"<p>Warning</p> <p>Tower Google Cloud Batch support is in Beta \u2014 more features will be added as Nextflow GCB support is enhanced over time.</p>  <p>This guide assumes you have an existing Google Cloud account. Sign-up for a free account here.</p> <p>Tower provides integration to Google Cloud via the Batch API.</p> <p>The guide is split into two parts:</p> <ol> <li> <p>How to configure your Google Cloud account to use the Batch API.</p> </li> <li> <p>How to create a Google Cloud Batch compute environment in Tower.</p> </li> </ol>","title":"Google Cloud Batch"},{"location":"compute-envs/google-cloud-batch/#configure-google-cloud","tags":["google","batch","gcp","compute environment"],"text":"","title":"Configure Google Cloud"},{"location":"compute-envs/google-cloud-batch/#create-a-project","tags":["google","batch","gcp","compute environment"],"text":"<p>Navigate to the Google Project Selector page and select an existing project or select Create project.</p> <p>Enter a name for your new project, e.g., \"tower-nf\".</p> <p>If you are part of an organization, the location will default to your organization.</p>","title":"Create a project"},{"location":"compute-envs/google-cloud-batch/#enable-billing","tags":["google","batch","gcp","compute environment"],"text":"<p>See these instructions to enable billing in your Google Cloud account.</p>","title":"Enable billing"},{"location":"compute-envs/google-cloud-batch/#enable-apis","tags":["google","batch","gcp","compute environment"],"text":"<p>See here to enable the following APIs for your project:</p> <ul> <li>Batch API</li> <li>Compute Engine API</li> <li>Cloud Storage API</li> </ul> <p>Select your project from the dropdown menu and select Enable.</p> <p>Alternatively, you can enable each API manually by selecting your project in the navigation bar and visiting each API page:</p> <ul> <li> <p>Batch API</p> </li> <li> <p>Compute Engine API</p> </li> <li> <p>Cloud Storage API</p> </li> </ul>","title":"Enable APIs"},{"location":"compute-envs/google-cloud-batch/#create-a-service-account-key","tags":["google","batch","gcp","compute environment"],"text":"<ol> <li> <p>In the navigation menu, select IAM &amp; Admin, then Service Accounts.</p> </li> <li> <p>Select the email address of the Compute Engine default service account.</p> </li> <li> <p>Select Keys, then Add key, then Create new key.</p> </li> <li> <p>Select JSON as the key type.</p> </li> <li> <p>Select Create.</p> </li> </ol> <p>A JSON file will be downloaded to your computer. This file contains the credential needed to configure the compute environment in Tower.</p> <p>You can manage your key from the Service Accounts page.</p>","title":"Create a service account key"},{"location":"compute-envs/google-cloud-batch/#create-a-cloud-storage-bucket","tags":["google","batch","gcp","compute environment"],"text":"<ol> <li> <p>In the navigation menu (\u2261), select Cloud Storage, then Create bucket.</p> </li> <li> <p>Enter a name for your bucket. You will reference this name when creating the compute environment in Tower.</p>  <p>Warning</p> <p>Do not use underscores (<code>_</code>) in your bucket name. Use hyphens (<code>-</code>) instead.</p>  </li> <li> <p>Select Region for the Location type and select the Location for your bucket. You will reference this location when creating the compute environment in Tower.</p> </li> <li> <p>Select Standard for the default storage class.</p> </li> <li> <p>Select Uniform for the Access control.</p>  <p>Note</p> <p>The Batch API is available in a limited number of locations. These locations are only used to store metadata about the pipeline operations. The storage bucket and compute resources can be in any region.</p>  </li> <li> <p>Select Create.</p> </li> <li> <p>Once the bucket is created, you will be redirected to the Bucket details page.</p> </li> <li> <p>Select Permissions, then + Add.</p> </li> <li> <p>Copy the email address of the Compute Engine default service account into New principals.</p> </li> <li> <p>Select the following roles:</p> </li> <li> <p>Storage Admin</p> </li> <li>Storage Legacy Bucket Owner</li> <li>Storage Legacy Object Owner</li> <li>Storage Object Creator</li> </ol>  <p>Google Cloud configured</p> <p>You have created a project, enabled the necessary Google APIs, created a bucket, and created a JSON file with the required credentials. You are now ready to set up a new compute environment in Tower.</p>","title":"Create a Cloud Storage bucket"},{"location":"compute-envs/google-cloud-batch/#compute-environment","tags":["google","batch","gcp","compute environment"],"text":"<p>Requirements</p> <p>The following guide to configure Tower assumes you have (1) a service account key for a Google Cloud account and (2) the name and location of a Cloud Storage bucket.</p>  <p>To create a new compute environment for Google Cloud in Tower:</p> <ol> <li> <p>In a workspace, select Compute Environments and then New Environment.</p> </li> <li> <p>Enter a descriptive name for this environment, e.g., \"Google Cloud Batch (europe-north1)\".</p> </li> <li> <p>Select Google Cloud Batch as the target platform.</p> </li> <li> <p>From the Credentials drop-down, select existing Google credentials, or select + to add new credentials. If you have existing credentials, skip to step 7.</p> </li> <li> <p>Enter a name for the credentials, e.g. \"Google Cloud Credentials\".</p> </li> <li> <p>Enter the Service account key created previously.</p> </li> <li> <p>Select the Location where you wish to execute pipelines.</p> </li> <li> <p>In the Pipeline work directory field, enter your storage bucket URL, e.g., <code>gs://my-bucket</code>. This bucket should be accessible in the location selected in the previous step.</p> </li> <li> <p>Select Enable Wave containers to facilitate access to private container repositories and provision containers in your pipelines using the Wave containers service. See Wave containers for more information.</p> </li> <li> <p>Select Enable Fusion v2 to allow access to your S3-hosted data via the Fusion v2 virtual distributed file system. This speeds up most data operations. The Fusion v2 file system requires Wave containers to be enabled (see above). </p> </li> <li> <p>Enable Spot to use spot instances, which have significantly reduced cost compared to on-demand instances.</p> </li> <li> <p>Apply Resource labels to the cloud resources consumed by this compute environment. Workspace default resource labels are prefilled. </p> </li> <li> <p>Expand Staging options to include optional pre- or post-run Bash scripts that execute before or after the Nextflow pipeline execution in your environment. </p> </li> <li> <p>You can use the Environment variables option to specify custom environment variables for the Head job and/or Compute jobs.</p> </li> <li> <p>Configure any advanced options described below, as needed.</p> </li> <li> <p>Select Create to finalize the compute environment setup.</p> </li> </ol> <p>Jump to the documentation for launching pipelines.</p>","title":"Compute environment"},{"location":"compute-envs/google-cloud-batch/#advanced-options","tags":["google","batch","gcp","compute environment"],"text":"<ul> <li> <p>Enable Use Private Address to ensure that your Google Cloud VMs aren't accessible to the public internet.</p> </li> <li> <p>Use Boot disk size to control the boot disk size of VMs.</p> </li> <li> <p>Use Head Job CPUs and Head Job Memory to specify the CPUs and memory allocated for head jobs.</p> </li> </ul>","title":"Advanced options"},{"location":"compute-envs/google-cloud-lifesciences/","tags":["google","gcp","life sciences","compute environment"],"text":"<p>This guide assumes you have an existing Google Cloud account. Sign-up for a free account here.</p> <p>Tower provides integration to Google Cloud via the Cloud Life Sciences API.</p> <p>This guide is split into two parts:</p> <ol> <li> <p>How to configure your Google Cloud account to use the Cloud Life Sciences API.</p> </li> <li> <p>How to create a Google Life Sciences compute environment in Tower.</p> </li> </ol>","title":"Google Life Sciences"},{"location":"compute-envs/google-cloud-lifesciences/#configure-google-cloud","tags":["google","gcp","life sciences","compute environment"],"text":"","title":"Configure Google Cloud"},{"location":"compute-envs/google-cloud-lifesciences/#create-a-project","tags":["google","gcp","life sciences","compute environment"],"text":"<p>Navigate to the Google Project Selector page and either select an existing project or select Create project.</p> <p>Enter a name for your new project, e.g., \"tower-nf\".</p> <p>If you are part of an organization, the location will default to your organization.</p>","title":"Create a project"},{"location":"compute-envs/google-cloud-lifesciences/#enable-billing","tags":["google","gcp","life sciences","compute environment"],"text":"<p>See here to enable billing in your Google Cloud account.</p>","title":"Enable billing"},{"location":"compute-envs/google-cloud-lifesciences/#enable-apis","tags":["google","gcp","life sciences","compute environment"],"text":"<p>See here to enable the following APIs for your project:</p> <ul> <li>Cloud Life Sciences API</li> <li>Compute Engine API</li> <li>Cloud Storage API</li> </ul> <p>Select your project from the dropdown menu and select Enable.</p> <p>Alternatively, select your project in the navigation bar and enable each API manually from these pages:</p> <ul> <li> <p>Cloud Life Sciences API</p> </li> <li> <p>Compute Engine API</p> </li> <li> <p>Cloud Storage API</p> </li> </ul>","title":"Enable APIs"},{"location":"compute-envs/google-cloud-lifesciences/#create-a-service-account-key","tags":["google","gcp","life sciences","compute environment"],"text":"<ol> <li> <p>In the navigation menu, select IAM &amp; Admin, then Service Accounts.</p> </li> <li> <p>Select the email address of the Compute Engine default service account.</p> </li> <li> <p>Select Keys, then Add key, then Create new key.</p> </li> <li> <p>Select JSON as the key type.</p> </li> <li> <p>Select Create.</p> </li> </ol> <p>A JSON file will be downloaded to your computer. This file contains the credential needed to configure the compute environment in Tower.</p> <p>You can manage your key from the Service Accounts page.</p>","title":"Create a service account key"},{"location":"compute-envs/google-cloud-lifesciences/#create-a-cloud-storage-bucket","tags":["google","gcp","life sciences","compute environment"],"text":"<ol> <li> <p>In the navigation menu (\u2261), select Cloud Storage, then Create bucket.</p> </li> <li> <p>Enter a name for your bucket. You will reference this name when creating the compute environment in Tower.</p>  <p>Warning</p> <p>Do not use underscores (<code>_</code>) in your bucket name. Use hyphens (<code>-</code>) instead.</p>  </li> <li> <p>Select Region for the Location type and select the Location for your bucket. You will reference this location when creating the compute environment in Tower.</p> </li> <li> <p>Select Standard for the default storage class.</p> </li> <li> <p>Select Uniform for the Access control.</p>  <p>Note</p> <p>The Cloud Life Sciences API is available in a limited number of locations. These locations are only used to store metadata about the pipeline operations. The storage bucket and compute resources can be in any region.</p>  </li> <li> <p>Select Create.</p> </li> <li> <p>Once the bucket is created, you will be redirected to the Bucket details page.</p> </li> <li> <p>Select Permissions, then + Add.</p> </li> <li> <p>Copy the email address of the Compute Engine default service account into New principals.</p> </li> <li> <p>Select the following roles:</p> </li> <li> <p>Storage Admin</p> </li> <li>Storage Legacy Bucket Owner</li> <li>Storage Legacy Object Owner</li> <li>Storage Object Creator</li> </ol>","title":"Create a Cloud Storage bucket"},{"location":"compute-envs/google-cloud-lifesciences/#compute-environment","tags":["google","gcp","life sciences","compute environment"],"text":"<p>Requirements</p> <p>The following guide to configure Tower assumes you have (1) a service account key for a Google Cloud account and (2) the name and location of a Cloud Storage bucket.</p>  <p>To create a new compute environment for Google Cloud in Tower:</p> <ol> <li> <p>In a workspace, select Compute Environments and then New Environment.</p> </li> <li> <p>Enter a descriptive name for this environment, e.g., \"Google Life Sciences (europe-west2)\".</p> </li> <li> <p>Select Google Life Sciences as the target platform.</p> </li> <li> <p>From the Credentials drop-down, select existing Google Cloud credentials, or add new credentials by selecting the + button. If you select to use existing credentials, skip to step 7.</p> </li> <li> <p>Enter a name for the credentials, e.g. \"Google Cloud Credentials\".</p> </li> <li> <p>Enter the Service account key created previously.</p>  <p>Multiple credentials</p> <p>You can create multiple credentials in your Tower workspace.</p>  </li> <li> <p>Select the Region and Zones where you wish to execute pipelines. Leave the Location empty for the Cloud Life Sciences API to use the closest available location.</p> </li> <li> <p>In the Pipeline work directory field, enter your storage bucket URL, e.g., <code>gs://my-bucket</code>. This bucket should be accessible in the region selected in the previous step.</p> </li> <li> <p>You can enable Preemptible to use preemptible instances, which have significantly reduced cost compared to on-demand instances.</p> </li> <li> <p>You can use a Filestore file system to automatically mount a Google Filestore volume in your pipelines.</p> </li> <li> <p>Apply Resource labels to the cloud resources consumed by this compute environment. Workspace default resource labels are prefilled. </p> </li> <li> <p>Expand Staging options to include optional pre- or post-run Bash scripts that execute before or after the Nextflow pipeline execution in your environment. </p> </li> <li> <p>Use the Environment variables option to specify custom environment variables for the Head job and/or Compute jobs.</p> </li> <li> <p>Configure any advanced options described below, as needed.</p> </li> <li> <p>Select Create to finalize the compute environment setup.</p> </li> </ol> <p>Jump to the documentation for launching pipelines.</p>","title":"Compute environment"},{"location":"compute-envs/google-cloud-lifesciences/#advanced-options","tags":["google","gcp","life sciences","compute environment"],"text":"<ul> <li> <p>Enable Use Private Address to ensure that your Google Cloud VMs aren't accessible to the public internet.</p> </li> <li> <p>Use Boot disk size to control the boot disk size of VMs.</p> </li> <li> <p>Use Head Job CPUs and Head Job Memory to specify the CPUs and memory allocated for head jobs.</p> </li> </ul>","title":"Advanced options"},{"location":"compute-envs/hpc/","tags":["slurm","lsf","pbs","grid","altair","ibm","moab","slurm","compute environment"],"text":"<p>Tower streamlines the deployment of Nextflow pipelines into both cloud-based and on-prem HPC clusters and supports compute environment creation for the following management and scheduling solutions:</p> <ul> <li>Altair PBS Pro</li> <li>Altair Grid Engine</li> <li>IBM Spectrum LSF (Load Sharing Facility)</li> <li>Moab</li> <li>Slurm</li> </ul>","title":"HPC clusters"},{"location":"compute-envs/hpc/#requirements","tags":["slurm","lsf","pbs","grid","altair","ibm","moab","slurm","compute environment"],"text":"<p>To launch pipelines into an HPC cluster from Tower, the following requirements must be satisfied:</p> <ul> <li>The cluster should allow outbound connections to the Tower web service.</li> <li>The cluster queue used to run the Nextflow head job must be able to submit cluster jobs.</li> <li>The Nextflow runtime version 21.02.0-edge (or later) must be installed on the cluster.</li> </ul>","title":"Requirements"},{"location":"compute-envs/hpc/#compute-environment","tags":["slurm","lsf","pbs","grid","altair","ibm","moab","slurm","compute environment"],"text":"<p>To create a new HPC compute environment in Tower:</p> <ol> <li> <p>In a workspace, select Compute environments and then New environment.</p> </li> <li> <p>Enter a descriptive name for this environment.</p> </li> <li> <p>Select your HPC environment from the Platform dropdown menu.</p> </li> <li> <p>Select your credentials, or select + and SSH or Tower Agent to add new credentials.</p> </li> <li> <p>Enter a name for the credentials.</p> </li> <li> <p>Enter the absolute path of the Work directory to be used on the cluster.</p> </li> <li> <p>Enter the absolute path of the Launch directory to be used on the cluster. If omitted, it will be the same as the work directory.</p> </li> <li> <p>Enter the Login hostname. This is usually the hostname or public IP address of the cluster's login node.</p> </li> <li> <p>Enter the Head queue name. This is the default cluster queue to which the Nextflow job will be submitted.</p> </li> <li> <p>Enter the Compute queue name. This is the default cluster queue to which the Nextflow job will submit tasks.</p> </li> <li> <p>Expand Staging options to include optional pre- or post-run Bash scripts that execute before or after the Nextflow pipeline execution in your environment. </p> </li> <li> <p>Use the Environment variables option to specify custom environment variables for the Head job and/or Compute jobs.</p> </li> <li> <p>Configure any advanced options described below, as needed.</p> </li> <li> <p>Select Create to finalize the creation of the compute environment.</p> </li> </ol> <p>Jump to the documentation for launching pipelines.</p>","title":"Compute environment"},{"location":"compute-envs/hpc/#advanced-options","tags":["slurm","lsf","pbs","grid","altair","ibm","moab","slurm","compute environment"],"text":"<ul> <li> <p>Use the Nextflow queue size to limit the number of jobs that Nextflow can submit to the scheduler at the same time.</p> </li> <li> <p>Use the Head job submit options to specify platform-specific submit options for the head job. You can optionally apply these options to compute jobs as well:</p> </li> </ul> <p></p>  <p>Note</p> <p>Once set during compute environment creation, these options cannot be overridden at pipeline launch time. </p>","title":"Advanced options"},{"location":"compute-envs/hpc/#ibm-lsf-additional-advanced-options","tags":["slurm","lsf","pbs","grid","altair","ibm","moab","slurm","compute environment"],"text":"<ul> <li>Use Unit for memory limits, Per job memory limits, and Per task reserve to control how memory is requested for Nextflow jobs.</li> </ul>","title":"IBM LSF additional advanced options"},{"location":"compute-envs/k8s/","tags":["k8s","kubernetes","compute environment"],"text":"<p>Kubernetes is the leading technology for the deployment and orchestration of containerized workloads in cloud-native environments.</p> <p>Tower streamlines the deployment of Nextflow pipelines into Kubernetes, both for cloud-based and on-prem clusters.</p> <p>The following instructions create a Tower compute environment for a generic Kubernetes distribution. See Amazon EKS or Google GKE for EKS and GKE compute environment instructions.</p>","title":"Kubernetes"},{"location":"compute-envs/k8s/#cluster-preparation","tags":["k8s","kubernetes","compute environment"],"text":"<p>To prepare your Kubernetes cluster for the deployment of Nextflow pipelines using Tower, this guide assumes that you have already created the cluster and that you have administrative privileges.</p> <ol> <li>Verify the connection to your Kubernetes cluster:</li> </ol> <pre>1</pre><pre><code>kubectl cluster-info\n</code></pre> <ol> <li>Create the Tower launcher:</li> </ol> <pre>1</pre><pre><code>kubectl apply -f https://help.tower.nf/latest/_templates/k8s/tower-launcher.yml\n</code></pre> <p>This command creates a service account called <code>tower-launcher-sa</code> and the associated role bindings, all contained in a namespace called <code>tower-nf</code>. Tower uses the service account to launch Nextflow pipelines. Use this service account name when setting up the compute environment for this Kubernetes cluster in Tower.</p> <ol> <li>Create persistent storage. Tower requires a <code>ReadWriteMany</code> persistent volume claim (PVC) mounted to all nodes where workflow pods will be dispatched.</li> </ol> <p>You can use any storage solution that supports the <code>ReadWriteMany</code> access mode. The setup of this storage is beyond the scope of these instructions \u2014 the right solution for you will depend on what is available for your infrastructure or cloud vendor (NFS, GlusterFS, CephFS, Amazon FSx). Ask your cluster administrator for more information.</p> <ul> <li> <p>Example PVC backed by local storage: tower-scratch-local.yml</p> </li> <li> <p>Example PVC backed by NFS server: tower-scratch-nfs.yml</p> </li> </ul> <p>Apply the appropriate PVC configuration to your cluster:</p> <pre>1</pre><pre><code>kubectl apply -f &lt;PVC_YAML_FILE&gt;.\n</code></pre>","title":"Cluster preparation"},{"location":"compute-envs/k8s/#compute-environment","tags":["k8s","kubernetes","compute environment"],"text":"<ol> <li> <p>In a workspace, select Compute environments and then New environment.</p> </li> <li> <p>Enter a descriptive name for this environment, e.g., \"K8s cluster\".</p> </li> <li> <p>Select Kubernetes as the target platform.</p> </li> <li> <p>From the Credentials drop-down, select existing Kubernetes credentials, or select + to add new credentials. If you have existing credentials, skip to step 7.</p> </li> <li> <p>Enter a name, e.g., \"K8s Credentials\".</p> </li> <li> <p>Enter the Service account token.</p> </li> </ol> <p>Obtain the token with the following command:</p> <pre>1\n2</pre><pre><code>SECRET=$(kubectl get secrets | grep &lt;SERVICE-ACCOUNT-NAME&gt; | cut -f1 -d ' ')\nkubectl describe secret $SECRET | grep -E '^token' | cut -f2 -d':' | tr -d '\\t'\n</code></pre> <p>Replace <code>&lt;SERVICE-ACCOUNT-NAME&gt;</code> with the name of the service account created in the cluster preparation instructions (default: <code>tower-launcher-sa</code>).</p> <ol> <li>Enter the Master server URL.</li> </ol> <p>Obtain the master server URL with the following command:</p> <pre>1</pre><pre><code>kubectl cluster-info\n</code></pre> <p>It can also be found in your <code>~/.kube/config</code> file under the <code>server</code> field corresponding to your cluster.</p> <ol> <li>Specify the SSL certificate to authenticate your connection.</li> </ol> <p>Find the certificate data in your <code>~/.kube/config</code> file. It is the <code>certificate-authority-data</code> field corresponding to your cluster.</p> <ol> <li> <p>Specify the Namespace created in the cluster preparation instructions, which is <code>tower-nf</code> by default.</p> </li> <li> <p>Specify the Head service account created in the cluster preparation instructions, which is <code>tower-launcher-sa</code> by default.</p> </li> <li> <p>Specify the Storage claim created in the cluster preparation instructions, which serves as a scratch filesystem for Nextflow pipelines. The storage claim is called <code>tower-scratch</code> in each of the provided examples.</p> </li> <li> <p>Apply Resource labels to the cloud resources consumed by this compute environment. Workspace default resource labels are prefilled. </p> </li> <li> <p>Expand Staging options to include optional pre- or post-run Bash scripts that execute before or after the Nextflow pipeline execution in your environment. </p> </li> <li> <p>You can use the Environment variables option to specify custom environment variables for the Head job and/or Compute jobs.</p> </li> <li> <p>Configure any advanced options described below, as needed.</p> </li> <li> <p>Select Create to finalize the compute environment setup.</p> </li> </ol> <p>Jump to the documentation for launching pipelines.</p>","title":"Compute environment"},{"location":"compute-envs/k8s/#advanced-options","tags":["k8s","kubernetes","compute environment"],"text":"<ul> <li> <p>The Storage mount path is the file system path where the Storage claim is mounted (default: <code>/scratch</code>).</p> </li> <li> <p>The Work directory is the file system path used as a working directory by Nextflow pipelines. It must be the storage mount path (default) or a subdirectory of it.</p> </li> <li> <p>The Compute service account is the service account used by Nextflow to submit tasks (default: the <code>default</code> account in the given namespace).</p> </li> <li> <p>The Pod cleanup policy determines when to delete terminated pods.</p> </li> <li> <p>You can use Custom head pod specs to provide custom options for the Nextflow workflow pod (<code>nodeSelector</code>, <code>affinity</code>, etc). For example:</p> </li> </ul> <pre>1\n2\n3</pre><pre><code>spec:\n  nodeSelector:\n    disktype: ssd\n</code></pre> <ul> <li> <p>You can use Custom service pod specs to provide custom options for the compute environment pod. See above for an example.</p> </li> <li> <p>You can use Head Job CPUs and Head Job memory to specify the hardware resources allocated to the Nextflow workflow pod.</p> </li> </ul>","title":"Advanced options"},{"location":"compute-envs/lsf/","tags":["ibm","lsf","compute environment"],"text":"<p>IBM Spectrum LSF (Load Sharing Facility)is an IBM workload management solution for HPC. LSF aims to enhance user and administrator experience, reliability and performance at scale.</p> <p>Tower streamlines the deployment of Nextflow pipelines into both cloud-based and on-prem LSF clusters.</p>","title":"IBM LSF"},{"location":"compute-envs/lsf/#requirements","tags":["ibm","lsf","compute environment"],"text":"<p>To launch pipelines into an LSF cluster from Tower, the following requirements must be satisfied:</p> <ul> <li>The cluster should allow outbound connections to the Tower web service.</li> <li>The cluster queue used to run the Nextflow head job must be able to submit cluster jobs.</li> <li>The Nextflow runtime version 21.02.0-edge (or later) must be installed on the cluster.</li> </ul>","title":"Requirements"},{"location":"compute-envs/lsf/#compute-environment","tags":["ibm","lsf","compute environment"],"text":"<p>To create a new compute environment for LSF in Tower:</p> <ol> <li> <p>In a workspace, select Compute environments and then New environment.</p> </li> <li> <p>Enter a descriptive name for this environment, e.g., \"LSF\".</p> </li> <li> <p>Select IBM LSF as the target platform.</p> </li> <li> <p>Select your credentials, or select + and SSH or Tower Agent to add new credentials.</p> </li> <li> <p>Enter a name for the credentials.</p> </li> <li> <p>Enter the absolute path of the Work directory to be used on the cluster.</p> </li> <li> <p>Enter the absolute path of the Launch directory to be used on the cluster. If omitted, it will be the same as the work directory.</p> </li> <li> <p>Enter the Login hostname. This is usually the hostname or public IP address of the cluster's login node.</p> </li> <li> <p>Enter the Head queue name. This is the cluster queue to which the Nextflow job will be submitted.</p> </li> <li> <p>Enter the Compute queue name. This is the cluster queue to which the Nextflow job will submit tasks.</p>  <p>Tip</p> <p>The compute queue can be overridden by the Nextflow pipeline configuration. See the Nextflow documentation for more details.</p>  </li> <li> <p>Expand Staging options to include optional pre- or post-run Bash scripts that execute before or after the Nextflow pipeline execution in your environment. </p> </li> <li> <p>You can use the Environment variables option to specify custom environment variables for the Head job and/or Compute jobs.</p> </li> <li> <p>Configure any advanced options described below, as needed.</p> </li> <li> <p>Select Create to finalize the creation of the compute environment.</p> </li> </ol> <p>Jump to the documentation for launching pipelines.</p>","title":"Compute environment"},{"location":"compute-envs/lsf/#advanced-options","tags":["ibm","lsf","compute environment"],"text":"<ul> <li> <p>Use the Nextflow queue size to limit the number of jobs that Nextflow can submit to the scheduler at the same time.</p> </li> <li> <p>Use the Head job submit options to specify LSF options for the head job. You can optionally apply these options to compute jobs as well:</p> </li> </ul> <p></p> <ul> <li>Use Unit for memory limits, Per job memory limits, and Per task reserve to control how memory is requested for Nextflow jobs.</li> </ul>","title":"Advanced options"},{"location":"compute-envs/moab/","tags":["moab","compute environment"],"text":"<p>Moab is a scheduling and management system designed for clusters, grids, and on-demand/utility computing systems. At a high level, Moab applies site policies and extensive optimizations to orchestrate jobs, services, and other workloads across the ideal combination of network, compute, and storage resources.</p> <p>Tower streamlines the deployment of Nextflow pipelines into both cloud-based and on-prem Moab clusters.</p>","title":"Moab"},{"location":"compute-envs/moab/#requirements","tags":["moab","compute environment"],"text":"<p>To launch pipelines into a Moab cluster from Tower, the following requirements must be satisfied:</p> <ul> <li>The cluster should allow outbound connections to the Tower web service.</li> <li>The cluster queue used to run the Nextflow head job must be able to submit cluster jobs.</li> <li>The Nextflow runtime version 21.02.0-edge (or later) must be installed on the cluster.</li> </ul>","title":"Requirements"},{"location":"compute-envs/moab/#compute-environment","tags":["moab","compute environment"],"text":"<p>To create a new compute environment for Moab in Tower:</p> <ol> <li> <p>In a workspace, select Compute environments and then New environment.</p> </li> <li> <p>Enter a descriptive name for this environment, e.g., \"Moab cluster\".</p> </li> <li> <p>Select Moab Workload Manager as the target platform.</p> </li> <li> <p>Select your credentials, or select + and SSH or Tower Agent to add new credentials.</p> </li> <li> <p>Enter a name for the credentials.</p> </li> <li> <p>Enter the absolute path of the Work directory to be used on the cluster.</p> </li> <li> <p>Enter the absolute path of the Launch directory to be used on the cluster. If omitted, it will be the same as the work directory.</p> </li> <li> <p>If using SSH credentials, enter the Login hostname. This is usually the hostname or public IP address of the cluster's login node.</p> </li> <li> <p>Enter the Head queue name. This is the cluster queue to which the Nextflow job will be submitted.</p> </li> <li> <p>Enter the Compute queue name. This is the cluster queue to which the Nextflow job will submit tasks.</p>  <p>Tip</p> <p>The compute queue can be overridden by the Nextflow pipeline configuration. See the Nextflow documentation for more details.</p>  </li> <li> <p>Expand Staging options to include optional pre- or post-run Bash scripts that execute before or after the Nextflow pipeline execution in your environment. </p> </li> <li> <p>Use the Environment variables option to specify custom environment variables for the Head job and/or Compute jobs.</p> </li> <li> <p>Configure any advanced options described below, as needed.</p> </li> <li> <p>Select Create to finalize the creation of the compute environment.</p> </li> </ol> <p>Jump to the documentation for launching pipelines.</p>","title":"Compute environment"},{"location":"compute-envs/moab/#advanced-options","tags":["moab","compute environment"],"text":"<ul> <li> <p>Use the Nextflow queue size to limit the number of jobs that Nextflow can submit to the scheduler at the same time.</p> </li> <li> <p>Use the Head job submit options to specify Moab options for the head job. You can optionally apply these options to compute jobs as well:</p> </li> </ul> <p></p>","title":"Advanced options"},{"location":"compute-envs/overview/","tags":["compute environment"],"text":"","title":"Overview"},{"location":"compute-envs/overview/#overview","tags":["compute environment"],"text":"<p>Tower uses the concept of compute environments to define the execution platform where a pipeline will run. Compute environments enable Tower users to launch pipelines on a growing number of cloud and on-premise infrastructures.</p> <p>Each compute environment must be configured to enable Tower to submit tasks. See the individual compute environment pages below for platform-specific configuration steps.</p>","title":"Overview"},{"location":"compute-envs/overview/#platforms","tags":["compute environment"],"text":"<ul> <li>AWS Batch</li> <li>Azure Batch</li> <li>Google Cloud Batch</li> <li>Google Life Sciences</li> <li>Altair Grid Engine</li> <li>Altair PBS Pro</li> <li>IBM LSF</li> <li>Moab</li> <li>Slurm</li> <li>Kubernetes</li> <li>Amazon EKS</li> <li>Google GKE</li> </ul>","title":"Platforms"},{"location":"compute-envs/overview/#select-a-default-compute-environment","tags":["compute environment"],"text":"<p>If you have more than one compute environment, you can select which one will be used by default when launching a pipeline.</p> <ol> <li> <p>In a workspace, select Compute Environments.</p> </li> <li> <p>Select Make primary for a particular compute environment to make it your default.</p> </li> </ol>","title":"Select a default compute environment"},{"location":"compute-envs/overview/#gpu-usage","tags":["compute environment"],"text":"<p>The process for provisioning GPU instances in your compute environment differs for each cloud provider.</p>","title":"GPU usage"},{"location":"compute-envs/overview/#aws-batch","tags":["compute environment"],"text":"<p>The AWS Batch compute environment creation form in Tower includes an Enable GPUs option. This option makes it possible to run GPU-dependent workflows in the compute environment. Note that:</p> <ul> <li> <p>The Enable GPUs setting alone does not cause GPU instances to deploy in your compute environment. You must still specify GPU-enabled instance types in the Advanced options &gt; Instance types field.</p> </li> <li> <p>The Enable GPUs setting causes Tower Forge to specify the most current AWS-recommended GPU-optimized ECS AMI as the EC2 fleet AMI when creating the compute environment.</p> </li> <li> <p>This setting can be overridden by AMI ID in the advanced options.</p> </li> <li> <p>The NVIDIA Container Runtime uses environment variables in container images to specify a GPU accelerated container. These variables should be included in the <code>containerOptions</code> directive for each GPU-dependent process in your Nextflow script. For example:</p> </li> </ul> <pre>1\n2\n3</pre><pre><code>process UseGPU {\n    containerOptions '-e NVIDIA_DRIVER_CAPABILITIES=compute,utility -e NVIDIA_VISIBLE_DEVICES=all'\n}\n</code></pre>","title":"AWS Batch"},{"location":"compute-envs/slurm/","tags":["slurm","compute environment"],"text":"<p>Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters.</p> <p>Tower streamlines the deployment of Nextflow pipelines into both cloud-based and on-prem Slurm clusters.</p>","title":"Slurm"},{"location":"compute-envs/slurm/#requirements","tags":["slurm","compute environment"],"text":"<p>To launch pipelines into a Slurm cluster from Tower, the following requirements must be satisfied:</p> <ul> <li>The cluster should allow outbound connections to the Tower web service.</li> <li>The cluster queue used to run the Nextflow head job must be able to submit cluster jobs.</li> <li>The Nextflow runtime version 21.02.0-edge (or later) must be installed on the cluster.</li> </ul>","title":"Requirements"},{"location":"compute-envs/slurm/#compute-environment","tags":["slurm","compute environment"],"text":"<p>To create a new compute environment for Slurm in Tower:</p> <ol> <li> <p>In a workspace, select Compute environments and then New environment.</p> </li> <li> <p>Enter a descriptive name for this environment, e.g., \"Slurm cluster\".</p> </li> <li> <p>Select Slurm Workload Manager as the target platform.</p> </li> <li> <p>Select your credentials, or select + and SSH or Tower Agent to add new credentials.</p> </li> <li> <p>Enter a name for the credentials.</p> </li> <li> <p>Enter the absolute path of the Work directory to be used on the cluster.</p> </li> <li> <p>Enter the absolute path of the Launch directory to be used on the cluster. If omitted, it will be the same as the work directory.</p> </li> <li> <p>Enter the Login hostname. This is usually the hostname or public IP address of the cluster's login node.</p> </li> <li> <p>Enter the Head queue name. This is the cluster queue to which the Nextflow job will be submitted.</p> </li> <li> <p>Enter the Compute queue name. This is the cluster queue to which the Nextflow job will submit tasks.</p>  <p>Tip</p> <p>The compute queue can be overridden by the Nextflow pipeline configuration. See the Nextflow documentation for more details.</p>  </li> <li> <p>Expand Staging options to include optional pre- or post-run Bash scripts that execute before or after the Nextflow pipeline execution in your environment. </p> </li> <li> <p>Use the Environment variables option to specify custom environment variables for the Head job and/or Compute jobs.</p> </li> <li> <p>Configure any advanced options described below, as needed.</p> </li> <li> <p>Select Create to finalize the creation of the compute environment.</p> </li> </ol> <p>Jump to the documentation for launching pipelines.</p>","title":"Compute environment"},{"location":"compute-envs/slurm/#advanced-options","tags":["slurm","compute environment"],"text":"<ul> <li> <p>Use the Nextflow queue size to limit the number of jobs that Nextflow can submit to the scheduler at the same time.</p> </li> <li> <p>Use the Head job submit options to specify Slurm options for the head job. You can optionally apply these options to compute jobs as well:</p> </li> </ul> <p></p>","title":"Advanced options"},{"location":"core-concepts/definitions/","tags":[],"text":"","title":"Core concepts"},{"location":"core-concepts/definitions/#pipeline","tags":[],"text":"<p>A pipeline is a pre-configured workflow that can be used by all users in a workspace. It is composed of a workflow repository, launch parameters, and a compute environment.</p>","title":"Pipeline"},{"location":"core-concepts/definitions/#launchpad","tags":[],"text":"<p>The Launchpad contains the collection of available pipelines that can be run in a workspace. From here, you can view and select pre-configured pipelines for launch.</p>","title":"Launchpad"},{"location":"core-concepts/definitions/#run","tags":[],"text":"<p>A run is a workflow execution. The Runs view is used to monitor and inspect the details of workflow executions in a workspace.</p>","title":"Run"},{"location":"core-concepts/definitions/#compute-environments","tags":[],"text":"<p>A compute environment is the platform where workflows are executed. It is composed of the credentials, configuration settings, and storage options configured for that platform.</p>","title":"Compute environments"},{"location":"core-concepts/definitions/#credentials","tags":[],"text":"<p>Credentials are access keys stored by Tower in an encrypted format, using AES-256 encryption. They allow the safe storage of authentication keys for compute environments, private code repositories, and external services.</p>","title":"Credentials"},{"location":"core-concepts/definitions/#datasets","tags":[],"text":"<p>Datasets are collections of versioned, structured data, usually in TSV (tab-separated values) and CSV (comma-separated values) formats. They are used to manage sample sheets and metadata, to be validated and used as inputs for workflow executions.</p>","title":"Datasets"},{"location":"core-concepts/definitions/#actions","tags":[],"text":"<p>Actions are used to automate the execution of pre-configured workflows (pipelines), based on event triggers such as code commits and webhooks.</p>","title":"Actions"},{"location":"core-concepts/definitions/#pipeline-secrets","tags":[],"text":"<p>Pipeline secrets are keys used by workflow tasks to interact with external systems, such as a password to connect to an external database or an API token. They are stored in Tower using AES-256 encryption.</p> <p>There are two types of pipeline secrets:</p> <ul> <li> <p>Pipeline secrets defined in a workspace are available to the workflows launched within that workspace.</p> </li> <li> <p>Pipeline secrets defined by a user are available to the workflows launched by that user in any workspace.</p> </li> </ul>","title":"Pipeline secrets"},{"location":"core-concepts/definitions/#workspaces","tags":[],"text":"<p>A workspace provides the context in which a user operates, including what resources are available and who can access them. It is composed of pipelines, compute environments, credentials, runs, actions, and datasets. Access permissions are controlled through participants, collaborators, and teams.</p>","title":"Workspaces"},{"location":"core-concepts/definitions/#organizations","tags":[],"text":"<p>An organization is the top-level entity where businesses, institutions, and groups can collaborate. It can contain multiple workspaces.</p>","title":"Organizations"},{"location":"core-concepts/definitions/#members","tags":[],"text":"<p>A member is a user who is internal to the organization. Members have an organization role and can operate in one or more organization workspaces. In each workspace, members can have a participant role that defines the permissions granted to them within that workspace.</p>","title":"Members"},{"location":"core-concepts/definitions/#team","tags":[],"text":"<p>A team is a group of members in the same organization. Teams can operate in one or more organization workspaces with a specific workspace role (one role per workspace).</p>","title":"Team"},{"location":"core-concepts/definitions/#participant","tags":[],"text":"<p>A user operating with an assigned role within a workspace.</p>","title":"Participant"},{"location":"core-concepts/definitions/#participant-role","tags":[],"text":"<p>The participant role defines the permissions granted to a user to perform actions or tasks within a workspace.</p>","title":"Participant role"},{"location":"credentials/agent_credentials/","tags":["agent","credentials"],"text":"<p>Tower Agent enables Tower to launch pipelines on HPC clusters that do not allow direct access through an SSH client. Tower Agent authenticates a secure connection with Tower using a Tower Agent credential.</p>","title":"Tower Agent credentials"},{"location":"credentials/agent_credentials/#tower-agent-sharing","tags":["agent","credentials"],"text":"<p>You can share a single Tower Agent instance with all members of a workspace. Create a Tower Agent credential, with Shared agent enabled, in the relevant workspace. All workspace members can then use this Tower Agent credential (Connection ID + Tower access token) to use the same Tower Agent instance.</p>","title":"Tower Agent sharing"},{"location":"credentials/agent_credentials/#create-a-tower-agent-credential","tags":["agent","credentials"],"text":"<ul> <li> <p>From an organization workspace: navigate to the Credentials tab and select Add Credentials.</p> </li> <li> <p>From your personal workspace: select Your credentials from the user top-right menu, then select Add credentials.</p> </li> </ul> <p></p>    Property Description Example     Name A unique name for the credentials using alphanumeric characters, dashes, or underscores. <code>my-agent-creds</code>   Provider Credential type Tower Agent   Agent connection ID The connection ID used to run your Tower Agent instance. Must match the connection ID used when running the Agent (see Usage below) <code>5429d66d-7712-xxxx-xxxx-xxxxxxxxxxxx</code>   Shared agent Enables Tower Agent sharing for all workspace members.    Usage Populates a code snippet for Tower Agent download with your connection ID. Replace <code>&lt;YOUR TOKEN&gt;</code> with your Tower access token.     <p>Once the form is complete, select Add. The new credential is now listed under the Credentials tab.</p>","title":"Create a Tower Agent credential"},{"location":"credentials/aws_registry_credentials/","tags":["aws","ecr","credentials"],"text":"","title":"AWS Elastic Container Registry"},{"location":"credentials/aws_registry_credentials/#container-registry-credentials","tags":["aws","ecr","credentials"],"text":"<p>From version 22.3, Tower supports the configuration of credentials for the Nextflow Wave container service to authenticate to private and public container registries. For more information on Wave containers, see here.</p>  <p>Note</p> <p>Container registry credentials are only leveraged by the Wave containers service. In order for your pipeline execution to leverage Wave containers, add <code>wave { enabled=true }</code> either to the Nextflow config field on the launch page, or to your nextflow.config file.</p>","title":"Container registry credentials"},{"location":"credentials/aws_registry_credentials/#aws-ecr-access","tags":["aws","ecr","credentials"],"text":"<p>Wave requires programmatic access to your private registry via long-term access keys. Create a user with registry read permissions (e.g. a subset of the AWS-managed <code>AmazonEC2ContainerRegistryReadOnly</code> policy) for this purpose.</p> <p>An IAM administrator can create and manage access keys from the AWS management console:</p> <ol> <li>Open the IAM console.</li> <li>Select Users from the navigation pane.</li> <li>Select the name of the user whose keys you want to manage, then select the Security credentials tab. We recommend creating an IAM user specifically for Wave authentication instead of using existing credentials with broader permissions.</li> <li>In the Access keys section, select Create access key. Each IAM user can have only two access keys at a time, so if the Create option is deactivated, delete an existing access key first.</li> <li>On the Access key best practices &amp; alternatives page, select Other and then Next.</li> <li>On the Retrieve access key page, you can either Show the user's secret access key details, or store them by selecting Download .csv file.</li> <li>The newly created access key pair is active by default and can be stored as a container registry credential in Tower.</li> </ol>  <p>Note</p> <p>Your credential must be stored in Tower as a container registry credential, even if the same access keys already exist in Tower as a workspace credential.</p>","title":"AWS ECR access"},{"location":"credentials/aws_registry_credentials/#add-credentials-to-tower","tags":["aws","ecr","credentials"],"text":"<ul> <li> <p>From an organization workspace: navigate to the Credentials tab and select Add Credentials.</p> </li> <li> <p>From your personal workspace: select Your credentials from the user top-right menu, then select Add credentials.</p> </li> </ul> <p></p>    Property Description Example     Name A unique name for the credentials using alphanumeric characters, dashes, or underscores <code>my-registry-creds</code>   Provider Credential type Container registry   User name IAM user access key ID <code>AKIAIOSFODNN7EXAMPLE</code>   Password IAM user secret access key <code>wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</code>   Registry server The container registry server name <code>https://&lt;aws_account_id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com</code>    <p>Once the form is complete, select Add. The new credential is now listed under the Credentials tab.</p>","title":"Add credentials to Tower"},{"location":"credentials/azure_registry_credentials/","tags":["azure","registry","credentials"],"text":"","title":"Azure Container Registry"},{"location":"credentials/azure_registry_credentials/#container-registry-credentials","tags":["azure","registry","credentials"],"text":"<p>From version 22.3, Tower supports the configuration of credentials for the Nextflow Wave container service to authenticate to private and public container registries. For more information on Wave containers, see here.</p>  <p>Note</p> <p>Container registry credentials are only leveraged by the Wave containers service. In order for your pipeline execution to leverage Wave containers, add <code>wave { enabled=true }</code> either to the Nextflow config field on the launch page, or to your nextflow.config file.</p>","title":"Container registry credentials"},{"location":"credentials/azure_registry_credentials/#azure-container-registry-access","tags":["azure","registry","credentials"],"text":"<p>Azure container registry makes use of Azure RBAC (Role-Based Access Control) to grant users access \u2014 for further details, see Azure container registry roles and permissions.</p> <p>You must use Azure credentials with long-term registry read (content/read) access to authenticate Tower to your registry. We recommend a token with repository-scoped permissions that is used only by Tower.</p> <ol> <li>In the Azure portal, navigate to your container registry.</li> <li>Under Repository permissions, select Tokens -&gt; +Add.</li> <li>Enter a token name.</li> <li>Under Scope map, select Create new.</li> <li>In the Create scope map section, enter a name and description for the new scope map.</li> <li>Select your Repository from the drop-down menu.</li> <li>Select content/read from the Permissions drop-down menu, then select Add to create the scope map.</li> <li>In the Create token section, ensure the Status is Enabled (default), then select Create.</li> <li>Return to Repository permissions -&gt; Tokens for your registry, then select the token you just created.</li> <li>On the token details page, select password1 or password2.</li> <li>In the password details section, uncheck the Set expiration date? checkbox, then select Generate.</li> <li>Copy and save the password after it is generated. The password will be displayed only once.</li> </ol>","title":"Azure Container Registry access"},{"location":"credentials/azure_registry_credentials/#add-credentials-to-tower","tags":["azure","registry","credentials"],"text":"<ul> <li> <p>From an organization workspace: navigate to the Credentials tab and select Add Credentials.</p> </li> <li> <p>From your personal workspace: select Your credentials from the user top-right menu, then select Add credentials.</p> </li> </ul> <p></p>    Property Description Example     Name A unique name for the credentials using alphanumeric characters, dashes, or underscores <code>my-registry-creds</code>   Provider Credential type Container registry   User name Registry token name <code>my-registry-token</code>   Password Registry token password <code>OuSrehzUX...ACRDO+2TX</code>   Registry server The container registry server name (Settings -&gt; Access keys -&gt; Login server in Azure portal) <code>myregistry.azurecr.io</code>    <p>Once the form is complete, select Add. The new credential is now listed under the Credentials tab.</p>","title":"Add credentials to Tower"},{"location":"credentials/docker_hub_registry_credentials/","tags":["docker","registry","credentials"],"text":"","title":"Docker Hub"},{"location":"credentials/docker_hub_registry_credentials/#container-registry-credentials","tags":["docker","registry","credentials"],"text":"<p>From version 22.3, Tower supports the configuration of credentials for the Nextflow Wave container service to authenticate to private and public container registries. For more information on Wave containers, see here.</p>  <p>Note</p> <p>Container registry credentials are only leveraged by the Wave containers service. In order for your pipeline execution to leverage Wave containers, add <code>wave { enabled=true }</code> either to the Nextflow config field on the launch page, or to your nextflow.config file.</p>","title":"Container registry credentials"},{"location":"credentials/docker_hub_registry_credentials/#docker-hub-registry-access","tags":["docker","registry","credentials"],"text":"<p>You must use Docker Hub credentials with Read-only access to authenticate Tower to your registry. Docker Hub makes use of Personal Access Tokens (PATs) for authentication. Note that we do not currently support Docker Hub authentication using 2FA (two-factor authentication).</p> <p>To create your access token in Docker Hub:</p> <ol> <li>Log in to Docker Hub.</li> <li>Select your username in the top right corner and select Account Settings.</li> <li>Select Security -&gt; New Access Token.</li> <li>Enter a token description and select Read-only from the Access permissions drop-down menu, then select Generate.</li> <li>Copy and save the generated access token (this is only displayed once).</li> </ol>","title":"Docker Hub registry access"},{"location":"credentials/docker_hub_registry_credentials/#add-credentials-to-tower","tags":["docker","registry","credentials"],"text":"<ul> <li> <p>From an organization workspace: navigate to the Credentials tab and select Add Credentials.</p> </li> <li> <p>From your personal workspace: select Your credentials from the user top-right menu, then select Add credentials.</p> </li> </ul> <p></p>    Property Description Example     Name A unique name for the credentials using alphanumeric characters, dashes, or underscores <code>my-registry-creds</code>   Provider Credential type Container registry   User name Your Docker username <code>user1</code>   Password Your Personal Access Token <code>1fcd02dc-...215bc3f3</code>   Registry server The container registry hostname (excluding protocol) <code>docker.io</code>    <p>Once the form is complete, select Add. The new credential is now listed under the Credentials tab.</p>","title":"Add credentials to Tower"},{"location":"credentials/google_registry_credentials/","tags":["google","container","registry","artifact","credentials"],"text":"","title":"Google Cloud"},{"location":"credentials/google_registry_credentials/#container-registry-credentials","tags":["google","container","registry","artifact","credentials"],"text":"<p>From version 22.3, Tower supports the configuration of credentials for the Nextflow Wave container service to authenticate to private and public container registries. For more information on Wave containers, see here.</p>  <p>Note</p> <p>Container registry credentials are only leveraged by the Wave containers service. In order for your pipeline execution to leverage Wave containers, add <code>wave { enabled=true }</code> either to the Nextflow config field on the launch page, or to your nextflow.config file.</p>","title":"Container registry credentials"},{"location":"credentials/google_registry_credentials/#google-cloud-registry-access","tags":["google","container","registry","artifact","credentials"],"text":"<p>Although Container Registry is still available and supported as a Google Enterprise API, new features will only be available in Artifact Registry. Container Registry will only receive critical security fixes. Google recommends using Artifact Registry for all new registries moving forward.</p> <p>Google Cloud Artifact Registry and Container Registry are fully integrated with Google Cloud services and support various authentication methods. Tower requires programmatic access to your private registry using long-lived service account keys in JSON format.</p> <p>Create dedicated service account keys that are only used to interact with your repositories. Tower requires the Artifact Registry Reader or Storage Object Viewer role.</p>","title":"Google Cloud registry access"},{"location":"credentials/google_registry_credentials/#create-a-service-account","tags":["google","container","registry","artifact","credentials"],"text":"Google Cloud Artifact RegistryGoogle Cloud Container Registry   <p>Administrators can create a service account from the Google Cloud console:</p> <ol> <li>Navigate to the Create service account page.</li> <li>Select a Cloud project.</li> <li>Enter a service account name and (optional) description.</li> <li>Select Create and continue.</li> <li>From the Role drop-down menu under step 2, select Artifact Registry -&gt; Artifact Registry Reader, then select Continue.</li> <li>(Optional) Grant other users and admins access to this service account under step 3.</li> <li>Select Done.</li> <li>From the project service accounts page, select the three-dot menu button under Actions for the service account you just created, then select Manage keys.</li> <li>On the Keys page, select Add key.</li> <li>On the Create private key popup, select JSON and then Create. This triggers a download of a JSON file containing the service account private key and service account details.</li> <li>Base-64 encode the contents of the JSON key file:</li> </ol> <pre>1\n2\n3\n4\n5\n6\n7\n8</pre><pre><code>#Linux\nbase64 KEY-FILE-NAME &gt; NEW-KEY-FILE-NAME\n\n#macOS\nbase64 -i KEY-FILE-NAME -o NEW-KEY-FILE-NAME\n\n#Windows\nBase64.exe -e KEY-FILE-NAME &gt; NEW-KEY-FILE-NAME\n</code></pre>   <p>Administrators can create a service account from the Google Cloud console:</p> <ol> <li>Navigate to the Create service account page.</li> <li>Select a Cloud project.</li> <li>Enter a service account name and (optional) description.</li> <li>Select Create and continue.</li> <li>From the Role drop-down menu under step 2, search for and select Storage Object Viewer, then select Continue.</li> <li>(Optional) Grant other users and admins access to this service account under step 3.</li> <li>Select Done.</li> <li>From the project service accounts page, select the three-dot menu button under Actions for the service account you just created, then select Manage keys.</li> <li>On the Keys page, select Add key.</li> <li>On the Create private key popup, select JSON and then Create. This triggers a download of a JSON file containing the service account private key and service account details.</li> </ol>","title":"Create a service account"},{"location":"credentials/google_registry_credentials/#add-credentials-to-tower","tags":["google","container","registry","artifact","credentials"],"text":"<ul> <li> <p>From an organization workspace: navigate to the Credentials tab and select Add Credentials.</p> </li> <li> <p>From your personal workspace: select Your credentials from the user top-right menu, then select Add credentials.</p> </li> </ul> <p></p>    Property Description Example     Name A unique name for the credentials using alphanumeric characters, dashes, or underscores <code>my-registry-creds</code>   Provider Credential type Container registry   User name Service account key type (Container Registry: <code>_json_key</code>, Artifact Registry: <code>_json_key_base64</code>)   Password JSON key file content (base64-encoded for Artifact Registry \u2014 remove any line breaks or trailing spaces) <code>wewogICJ02...9tIgp9Cg==</code>   Registry server The container registry hostname (excluding protocol) <code>&lt;location&gt;-docker.pkg.dev</code>    <p>Once the form is complete, select Add. The new credential is now listed under the Credentials tab.</p>","title":"Add credentials to Tower"},{"location":"credentials/overview/","tags":["credentials"],"text":"","title":"Overview"},{"location":"credentials/overview/#credentials","tags":["credentials"],"text":"<p>In Tower, you can configure workspace credentials to store the access keys and tokens for your compute environments and Git hosting services.</p> <p>From Tower 22.3, you can configure container registry credentials to be used by the Wave containers service to authenticate to private and public container registries such as Docker Hub, Google Artifact Registry, Quay, etc.</p> <p>See the Container registry credentials section for registry-specific instructions.</p> <p></p>  <p>Note</p> <p>All credentials are (AES-256) encrypted before secure storage and not exposed in an unencrypted way by any Tower API.</p>","title":"Credentials"},{"location":"credentials/quay_registry_credentials/","tags":["quay","credentials"],"text":"","title":"Quay"},{"location":"credentials/quay_registry_credentials/#container-registry-credentials","tags":["quay","credentials"],"text":"<p>From version 22.3, Tower supports the configuration of credentials for the Nextflow Wave container service to authenticate to private and public container registries. For more information on Wave containers, see here.</p>  <p>Note</p> <p>Container registry credentials are only leveraged by the Wave containers service. In order for your pipeline execution to leverage Wave containers, add <code>wave { enabled=true }</code> either to the Nextflow config field on the launch page, or to your nextflow.config file.</p>","title":"Container registry credentials"},{"location":"credentials/quay_registry_credentials/#quay-repository-access","tags":["quay","credentials"],"text":"<p>For Quay repositories, we recommend using robot accounts with Read access permissions for authentication:</p> <ol> <li>Sign in to quay.io.</li> <li>From the user or organization view, select the Robot Accounts tab.</li> <li>Select Create Robot Account.</li> <li>Enter a robot account name. The username for robot accounts have the format <code>namespace+accountname</code>, where <code>namespace</code> is the user or organization name and <code>accountname</code> is your chosen robot account name.</li> <li>Grant the robot account repository Read permissions from Settings -&gt; User and Robot Permissions in the repository view.</li> <li>Select the robot account in your admin panel to retrieve the token value.</li> </ol>","title":"Quay repository access"},{"location":"credentials/quay_registry_credentials/#add-credentials-to-tower","tags":["quay","credentials"],"text":"<ul> <li> <p>From an organization workspace: navigate to the Credentials tab and select Add Credentials.</p> </li> <li> <p>From your personal workspace: select Your credentials from the user top-right menu, then select Add credentials.</p> </li> </ul> <p></p>    Property Description Example     Name A unique name for the credentials using alphanumeric characters, dashes, or underscores <code>my-registry-creds</code>   Provider Credential type Container registry   User name Robot account username (<code>namespace+accountname</code>) <code>mycompany+myrobotaccount</code>   Password Robot account access token <code>PasswordFromQuayAdminPanel</code>   Registry server The container registry hostname <code>quay.io</code>    <p>Once the form is complete, select Add. The new credential is now listed under the Credentials tab.</p>","title":"Add credentials to Tower"},{"location":"credentials/ssh_credentials/","tags":["ssh","credentials"],"text":"<p>SSH public key authentication relies on asymmetric cryptography to generate a public and private key pair. The public key remains on the target (remote) machine, while the private key (and passphrase) is stored in Tower as a credential. The key pair is used to authenticate a Tower connection with your SSH-enabled environment.</p>  <p>Note</p> <p>All credentials are (AES-256) encrypted before secure storage and not exposed in an unencrypted way by any Tower API.</p>","title":"SSH credentials"},{"location":"credentials/ssh_credentials/#create-an-ssh-key-pair","tags":["ssh","credentials"],"text":"<p>To use SSH public key authentication:</p> <ul> <li>The remote system must have a version of SSH installed. This guide assumes the remote system uses OpenSSH. If you are using a different version of SSH, the key generation steps may differ.</li> <li>The SSH public key must be present on the remote system (usually in <code>~/.ssh/authorized_keys</code>).</li> </ul> <p>To generate an SSH key pair:</p> <ol> <li>From the target machine, open a terminal and run <code>ssh-keygen</code>.</li> <li>Follow the prompts to:</li> <li>specify a file path and name (or keep the default)</li> <li>specify a passphrase (recommended)</li> <li>Navigate to the target folder (default <code>/home/user/.ssh/id_rsa</code>) and open the private key file with a plain text editor.</li> <li>Copy the private key file contents before navigating to Tower.</li> </ol>","title":"Create an SSH key pair"},{"location":"credentials/ssh_credentials/#create-an-ssh-credential-in-tower","tags":["ssh","credentials"],"text":"<ul> <li> <p>From an organization workspace: navigate to the Credentials tab and select Add Credentials.</p> </li> <li> <p>From your personal workspace: select Your credentials from the user top-right menu, then select Add credentials.</p> </li> </ul> <p></p>    Property Description Example     Name A unique name for the credentials using alphanumeric characters, dashes, or underscores. <code>my-ssh-creds</code>   Provider Credential type SSH   SSH private key The SSH private key file contents. <code>-----BEGIN OPENSSH PRIVATE KEY-----b3BlbnNza....</code>   Passphrase SSH private key passphrase (recommended). If your key pair was created without a passphrase, leave this blank.     <p>Once the form is complete, select Add. The new credential is now listed under the Credentials tab.</p>","title":"Create an SSH credential in Tower"},{"location":"dashboard/overview/","tags":["dashboard","runs","monitoring"],"text":"","title":"Dashboard"},{"location":"dashboard/overview/#overview","tags":["dashboard","runs","monitoring"],"text":"<p>Note</p> <p>This feature is available from Tower v.22.3.</p>  <p>The Tower Dashboard provides an overview of runs in your organizations and personal workspace at a glance. Access it from the user top-right menu, under Dashboard. </p>","title":"Overview"},{"location":"dashboard/overview/#filters-and-summary","tags":["dashboard","runs","monitoring"],"text":"<p></p> <p>The Dashboard view defaults to all organizations and workspaces you can access. Select the drop-down next to View: to filter by specific organizations and workspaces, or to view statistics for your personal workspace only.</p> <p>The filter button provides options for filtering by time, including a custom date range of up to 12 months. The button icon and color changes to indicate when a filter has been applied to your dashboard view. </p> <p>Below the filters, a summary of total runs is shown by status.</p>","title":"Filters and summary"},{"location":"dashboard/overview/#export-data","tags":["dashboard","runs","monitoring"],"text":"<p>Select Export data in the filter panel near the top of the page to download a CSV file of your dashboard data. The export contains data based on the filters you have applied. </p>","title":"Export data"},{"location":"dashboard/overview/#runs-per-organization","tags":["dashboard","runs","monitoring"],"text":"<p>Below the cards displaying total runs by status, run totals for your selected filters above are displayed.</p> <p>Depending on the filter selected, each card details a separate workspace or organization. Total runs for each organization are arranged by workspace and status.</p> <p></p> <p>Select a run value in the table to navigate to a run list filtered by the status and time range selected.</p> <p>Select a workspace name in the table to navigate to a run list filtered by the workspace selected.</p>","title":"Runs per organization"},{"location":"data-privacy/overview/","tags":["data","privacy"],"text":"","title":"Data privacy"},{"location":"data-privacy/overview/#your-data","tags":["data","privacy"],"text":"<p>Your data stays strictly within your infrastructure itself. When you launch a workflow through Tower, you need to connect your infrastructure (HPC/VMs/K8s) by creating the appropriate credentials and compute environment in a workspace.</p> <p>Tower then uses this configuration to trigger a Nextflow workflow within your infrastructure similar to what is done via the Nextflow CLI, therefore Tower does not manipulate any data itself and no data is transferred to the infrastructure where Tower is running.</p> <p>It may be possible to access some data within your storage from the Nextflow Tower interface - for example, viewing logs and reports generated in a pipeline run - however, this data is never stored within the Tower infrastructure.</p>","title":"Your data"},{"location":"data-privacy/overview/#metadata-stored-by-nextflow-tower","tags":["data","privacy"],"text":"<p>Workflow execution metadata is sent by the Nextflow runtime to Nextflow Tower when:</p> <ul> <li>Launching workflow with Tower</li> <li>Using the <code>-with-tower</code> option at the command line</li> <li>When a Nextflow Tower is specified in the Nextflow config</li> </ul> <p>The following sections describe the data structure and metadata fields collected by Tower.</p>","title":"Metadata stored by Nextflow Tower"},{"location":"data-privacy/overview/#workflow-metadata","tags":["data","privacy"],"text":"<p>The following metadata fields are collected and stored by the Tower backend during a workflow execution:</p>    Name Description     <code>command_line</code> The command line used to launch the workflow execution   <code>commit_id</code> The workflow project commit Id at the time of the execution   <code>complete</code> The workflow execution completion timestamp   <code>config_files</code> The nextflow config file paths(s) involved in the workflow execution   <code>config_text</code> The nextflow config content used for the workflow execution. Note: secrets, such as, AWS keys are stripped and not included in this field.   <code>container</code> The container image name(s) used for the pipeline execution   <code>container_engine</code> The container engine name used for the pipeline execution   <code>duration</code> The workflow execution overall duration (wall time)   <code>error_message</code> The error message reported in the case of nextflow execution failure   <code>error_report</code> The extended error message reported in case of workflow execution error.   <code>exit_status</code> The workflow execution (POSIX) exit code   <code>home_dir</code> The launching user home directory path   <code>launch_dir</code> The workflow launching directory path   <code>manifest_author</code> The workflow project author as defined in the nextflow config manifest file   <code>manifest_default_branch</code> The workflow project default Git branch as defined in the nextflow config manifest file   <code>manifest_description</code> The workflow project description as defined in the nextflow config manifest file   <code>manifest_gitmodules</code> The workflow project Git submodule flag in the nextflow config manifest file   <code>manifest_home_page</code> The workflow project Git home page as defined in the nextflow config manifest file   <code>manifest_main_script</code> The workflow project main script file name as defined in the nextflow config manifest file   <code>manifest_name</code> The workflow project name as defined in the nextflow config manifest file   <code>manifest_nextflow_version</code> The workflow project required Nextflow version defined in the nextflow config manifest file   <code>manifest_version</code> The workflow project version string as defined in the nextflow config manifest file   <code>nextflow_build</code> The build number of the Nextflow runtime used to launch the workflow execution   <code>nextflow_timestamp</code> The build timestamp of the Nextflow runtime used to launch the workflow execution   <code>nextflow_version</code> The version string of the Nextflow runtime used to launch the workflow execution   <code>params</code> The workflow params used to launch the pipeline execution   <code>profile</code> The workflow config profile string used for the pipeline execution   <code>project_dir</code> The directory path where the workflow scripts are stored   <code>project_name</code> The workflow project name   <code>repository</code> The workflow project repository   <code>resume</code> The flag set when a resume execution was submitted   <code>revision</code> The workflow project revision number   <code>run_name</code> The workflow run name as given by the Nextflow runtime   <code>script_file</code> The workflow script file path   <code>script_id</code> The workflow script checksum number   <code>script_name</code> The workflow script filename   <code>session_id</code> The workflow execution unique UUID as assigned by the Nextflow runtime   <code>start</code> The workflow execution start timestamp   <code>stats_cached_count</code> The number of cached tasks upon completion   <code>stats_cached_duration</code> The aggregate time of cached tasks upon completion   <code>stats_cached_pct</code> The percentage of cached tasks upon completion   <code>stats_compute_time_fmt</code> The overall compute time as a formatted string   <code>stats_failed_count</code> The number of failed tasks upon completion   <code>stats_failed_count_fmt</code> The number of failed tasks upon completion as a formatted string   <code>stats_failed_duration</code> The aggregate time of failed tasks upon completion   <code>stats_failed_pct</code> The percentage of failed tasks upon completion   <code>stats_ignored_count</code> The number of ignored tasks upon completion   <code>stats_ignored_count_fmt</code> The number of ignored tasks upon completion as a formatted string   <code>stats_ignored_pct</code> The percentage of ignored tasks upon completion   <code>stats_succeed_count</code> The number of succeeded tasks upon completion   <code>stats_succeed_count_fmt</code> The number of succeeded tasks upon completion as a formatted string   <code>stats_succeed_duration</code> The aggregate time of succeeded tasks upon completion   <code>stats_succeed_pct</code> The percentage of succeeded tasks upon completion   <code>status</code> The workflow execution status   <code>submit</code> The workflow execution submission timestamp   <code>success</code> The flag reporting whether the execution completed successfully   <code>user_name</code> The POSIX user name launching that launched the workflow execution   <code>work_dir</code> The workflow execution scratch directory path","title":"Workflow metadata"},{"location":"data-privacy/overview/#task-metadata","tags":["data","privacy"],"text":"Name Description     <code>attempt</code> Number of execution attempt of the task   <code>cloud_zone</code> Cloud zone where the task execution was allocated   <code>complete</code> Task execution completion timestamp   <code>container</code> Container image name used to execute the task   <code>cost</code> Estimated task compute cost   <code>cpus</code> Number of CPUs requested   <code>disk</code> Amount of disk storage requested   <code>duration</code> Amount of time for the task completion   <code>env</code> Task execution environment variables   <code>error_action</code> Action applied on task failure   <code>executor</code> Executor requested for the task execution   <code>exit_status</code> Task POSIX exit code on completion   <code>hash</code> Task unique hash code   <code>inv_ctxt</code> Number of involuntary context switches   <code>machine_type</code> Cloud virtual machine type   <code>memory</code> Amount of memory requested   <code>module</code> Environment Module requested   <code>name</code> Task unique name   <code>native_id</code> Task unique ID as assigned by the underlying execution platform   <code>pcpu</code> Percentage of CPU used to compute the task   <code>peak_rss</code> Peak of real memory during the task execution   <code>peak_vmem</code> Peak of virtual memory during the task execution   <code>pmem</code> Percentage of memory used to compute the task   <code>price_model</code> The cloud price model applied for the task   <code>process</code> The nextflow process name   <code>queue</code> The compute queue name requested   <code>rchar</code> Number of bytes the process read, using any read-like system call from files, pipes, tty, etc.   <code>read_bytes</code> Number of bytes the process directly read from disk   <code>realtime</code> The time required to compute the task   <code>rss</code> Real memory (resident set) size of the process   <code>scratch</code> Flag reporting the task was executed in a local scratch path   <code>script</code> The task command script   <code>start</code> Task execution start timestamp   <code>status</code> The task execution status   <code>submit</code> Task submission timestamp   <code>syscr</code> Number of read-like system call invocations that the process performed   <code>syscw</code> Number of write-like system call invocations that the process performed   <code>tag</code> Nextflow tag associated to the task execution   <code>task_id</code> Nextflow task ID   <code>time</code> Task execution timeout requested   <code>vmem</code> Virtual memory size used by the task execution   <code>vol_ctxt</code> Number of voluntary context switches   <code>wchar</code> Number of bytes the process wrote, using any write-like system call   <code>workdir</code> Task execution work directory   <code>write_bytes</code> Number of bytes the process written to disk","title":"Task Metadata"},{"location":"datasets/overview/","tags":["datasets"],"text":"","title":"Datasets"},{"location":"datasets/overview/#overview","tags":["datasets"],"text":"<p>Note</p> <p>This feature is only available in organization workspaces.</p>  <p>Datasets in Nextflow Tower are CSV (comma-separated values) and TSV (tab-separated values) formatted files stored in a workspace. They are designed to be used as inputs to pipelines to simplify data management, minimize user data-input errors, and facilitate reproducible workflows.</p> <p>The combination of datasets, pipeline secrets, and pipeline actions in Tower allow you to automate workflows to curate your data and maintain and launch pipelines based on specific events. See here for an example of pipeline workflow automation using Tower.</p> <ul> <li> <p>Using datasets reduces errors that occur due to manual data entry when launching pipelines.</p> </li> <li> <p>Datasets can be generated automatically in response to events (such as S3 storage new file notifications).</p> </li> <li> <p>Datasets can streamline differential data analysis when using the same pipeline to launch a run for each dataset as it becomes available.</p> </li> </ul> <p>For your pipeline to use your dataset as input during runtime, information about the dataset and file format must be included in the relevant parameters of your pipeline schema.</p>","title":"Overview"},{"location":"datasets/overview/#dataset-validation-and-file-content-requirements","tags":["datasets"],"text":"<p>Tower does not validate your dataset file contents. While datasets can contain static file links, you are responsible for maintaining the access to that data.</p> <p>Datasets can point to files stored in various locations, such as Amazon S3 or GitHub. To stage the file paths defined in the dataset, Nextflow requires access to the infrastructure where the files reside, whether on Cloud or HPC systems. Add the access keys for data sources that require authentication to your pipeline secrets.</p>","title":"Dataset validation and file content requirements"},{"location":"datasets/overview/#dataset-permissions","tags":["datasets"],"text":"<p>All Tower users have access to the datasets feature in organization workspaces.</p>","title":"Dataset permissions"},{"location":"datasets/overview/#creating-a-new-dataset","tags":["datasets"],"text":"<p>To create a new dataset, follow these steps:</p> <ol> <li>Open the Datasets tab in your organization workspace.</li> <li>Select New dataset.</li> <li>Complete the Name and Description fields using information relevant to your dataset.</li> <li>Add the dataset file to your workspace with drag-and-drop or the system file explorer dialog.</li> <li>For dataset files that use the first row for column names, customize the dataset view with the First row as header option.</li> </ol>  <p>Warning</p> <p>The size of the dataset file cannot exceed 10MB.</p>","title":"Creating a new dataset"},{"location":"datasets/overview/#dataset-versions","tags":["datasets"],"text":"<p>Datasets in Tower can accommodate multiple versions of a dataset. To add a new version for an existing dataset, follow these steps:</p> <ol> <li>Select Edit next to the dataset you wish to update.</li> <li>In the Edit dialog, select Add a new version.</li> <li>Upload the newer version of the dataset and select Update.</li> </ol>  <p>Warning</p> <p>All subsequent versions of a dataset must be in the same format (.csv or .tsv) as the initial version.</p>","title":"Dataset versions"},{"location":"datasets/overview/#using-a-dataset","tags":["datasets"],"text":"<p>To use a dataset with the saved pipelines in your workspace, follow these steps:</p> <ol> <li>Open any pipeline that contains a pipeline-schema from the Launchpad.</li> <li>Select the input field for the pipeline, removing any default value.</li> <li>Pick the dataset to use as input to your pipeline.</li> </ol>  <p>Note</p> <p>The datasets shown in the drop-down menu depend on the chosen format in your <code>pipeline-schema.json</code>. If the schema specifies <code>\"mimetype\": \"text/csv\"</code>, no TSV datasets will be available, and vice versa.</p>","title":"Using a dataset"},{"location":"enterprise/","tags":["installation","deployment"],"text":"<p>Tip</p> <p>It is highly recommended to first Sign up and try the hosted version of Tower for free, or request a demo for deployment to your own cloud or on-prem infrastructure.</p>  <p>Tower is a web application with a microservice-oriented architecture that is designed to maximize portability, scalability and security.</p> <p>Tower is composed of several modules that can be configured and deployed according to your organization's requirements. These modules are provided as Docker container images which are securely hosted on several major cloud platforms.</p>","title":"Introduction"},{"location":"enterprise/#architecture","tags":["installation","deployment"],"text":"<p>Note</p> <p>To deploy Nextflow Tower on your infrastructure, you will need a license key. Contact us to obtain a license.</p>  <p>Tower is composed of several Seqera-provided services and standard services that can be provisioned by Tower or by the client.</p> <p></p>","title":"Architecture"},{"location":"enterprise/#tower-backend","tags":["installation","deployment"],"text":"<p>The Tower backend is a JVM-based web application based on the Micronaut framework, which provides a modern and secure backbone for the application.</p> <p>The backend implements the main application logic, which is exposed via a REST API and defined via an OpenAPI schema. The backend uses JPA/Hibernate/JDBC API industry standards to interact with the underlying relational database.</p> <p>The backend can be run standalone or as multiple replicas for scalability when deployed in high-availability mode. It should run on port <code>8080</code>.</p>","title":"Tower backend"},{"location":"enterprise/#tower-cron","tags":["installation","deployment"],"text":"<p>Tower cron is an auxiliary backend service that executes regularly-occurring activities, such as sending email notifications and cleaning up stale data. The cron service also performs database migrations at startup.</p>","title":"Tower cron"},{"location":"enterprise/#tower-frontend","tags":["installation","deployment"],"text":"<p>The Tower frontend is an NGINX web server that serves the Angular application and reverse-proxies HTTP traffic to the Tower backend.</p> <p>The frontend should run on port <code>80</code> within the container, and it should be the only service that accepts incoming HTTP traffic. The frontend can also be exposed via HTTPS or a load balancer.</p>","title":"Tower frontend"},{"location":"enterprise/#redis-database","tags":["installation","deployment"],"text":"<p>Tower requires a Redis database for caching purposes.</p>","title":"Redis database"},{"location":"enterprise/#sql-database","tags":["installation","deployment"],"text":"<p>Tower requires a SQL database to persist user activities and state. Tower has been tested against MySQL 5.7. Please contact Seqera support if you would like to use a different JDBC-compliant SQL database.</p>","title":"SQL database"},{"location":"enterprise/#smtp-service","tags":["installation","deployment"],"text":"<p>Tower requires a SMTP relay to send email messages and user notifications.</p>","title":"SMTP service"},{"location":"enterprise/#authentication-service-optional","tags":["installation","deployment"],"text":"<p>Tower supports enterprise authentication mechanisms such as OAuth and OpenID. Third-party identity providers and custom single sign-on flows can be developed according to specific customer requirements.</p>","title":"Authentication service (optional)"},{"location":"enterprise/#deployment-options","tags":["installation","deployment"],"text":"<p>Tower can be deployed to a single node (via Docker Compose or natively), or to a Kubernetes cluster. This documentation includes instructions for both options across multiple platforms, including AWS, Azure, Google Cloud, and on-prem infrastructure.</p>","title":"Deployment options"},{"location":"enterprise/#single-node","tags":["installation","deployment"],"text":"<p>The minimal Tower deployment requires only the frontend, backend, and database services. These services can be deployed as Docker containers or as native services.</p>","title":"Single-node"},{"location":"enterprise/#kubernetes","tags":["installation","deployment"],"text":"<p>Kubernetes is emerging as the technology of choice for deploying applications that require high-availability, scalability, and security. Tower Enterprise includes configuration manifests for Kubernetes deployment.</p> <p> Reference architecture diagram of Tower on AWS using Elastic Kubernetes Service (EKS)</p>","title":"Kubernetes"},{"location":"enterprise/#tower-container-images","tags":["installation","deployment"],"text":"<p>Nextflow Tower is distributed as a collection of Docker containers available through the Seqera Labs container registry <code>cr.seqera.io</code>. Contact support to get your container access credentials. Once you have received your credentials, log in to the registry using these steps:</p> <ol> <li> <p>Retrieve the name and secret values from the JSON file you received from Seqera Labs support.</p> </li> <li> <p>Authenticate to the registry (using the <code>name</code> and <code>secret</code> values copied in step 1):</p> <pre>1</pre><pre><code>docker login -u '&lt;NAME&gt;' -p '&lt;SECRET&gt;' cr.seqera.io\n</code></pre> </li> <li> <p>Pull the Nextflow Tower container images:</p> <pre>1\n2\n3</pre><pre><code>docker pull cr.seqera.io/private/nf-tower-enterprise/backend:v23.1.0\n\ndocker pull cr.seqera.io/private/nf-tower-enterprise/frontend:v23.1.0\n</code></pre> </li> </ol>   <p>Warning</p> <p>The Seqera Labs container registry <code>cr.seqera.io</code> is the default Tower container image registry from version 22.4. Use of the AWS, Azure, and Google Cloud Tower image registries in existing installations is still supported, but will be deprecated for new installations starting June 2023.</p>","title":"Tower container images"},{"location":"enterprise/#support","tags":["installation","deployment"],"text":"<p>For further information, contact Seqera support via email or the support channel provided as part of the Tower Enterprise License agreement.</p>","title":"Support"},{"location":"enterprise/aws_troubleshooting/","tags":["aws","troubleshooting","deployment"],"text":"","title":"AWS troubleshooting"},{"location":"enterprise/aws_troubleshooting/#unable-to-mount-the-fsx-filesystem","tags":["aws","troubleshooting","deployment"],"text":"<p>While trying to mount the newly created <code>FSx</code> filesystem in the compute environment, the following error is observed in <code>/var/log/tower-forge.log</code>:</p>  Click to expand error log! <pre>1\n2\n3</pre><pre><code>mount.lustre: Can't parse NID 'fs-xxxxxxxxxxxx.fsx.us-east-1.amazonaws.com@tcp:/xxxxxxx\n\nThis mount helper should only be invoked via the mount (8) command, e.g. mount -t lustre dev dir\n</code></pre>  <p>SOLUTION </p> <p>Please enable DNS hostnames on your VPC. </p>","title":"Unable to mount the FSx filesystem"},{"location":"enterprise/aws_troubleshooting/#workflow-execution-fails-with-cannotstartcontainererror","tags":["aws","troubleshooting","deployment"],"text":"<p>While trying to run workflows on AWS Batch environment with custom VPC and subnets environment, it is possible to encounter the <code>CannotStartContainerError</code> issue.</p>  Click to expand error log! <pre>1\n2\n3\n4</pre><pre><code>Workflow execution completed unsuccessfully\nThe exit status of the task that caused the workflow execution to fail was: -\nCannotStartContainerError: Error response from daemon: failed to initialize logging driver: failed to create Cloudwatch log stream: RequestError: send request failed\ncaused by: Post https://logs.us-east-2.amazonaws.com/: dial tcp 10.20.10.16:443: i/o time\n</code></pre>  <p>SOLUTION</p> <p>This error is encountered when the custom VPC configuration are not specified in the <code>Advanced</code> settings in Tower for AWS Batch environment.</p>","title":"Workflow execution fails with <code>CannotStartContainerError</code>"},{"location":"enterprise/aws_troubleshooting/#workflow-execution-fails-with-javanetunknownhostexception","tags":["aws","troubleshooting","deployment"],"text":"<p>Workflow executions fails upon launching and the following error message is observed in <code>.nextflow.log</code> file.</p>  Click to expand error log! <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28</pre><pre><code>java.net.UnknownHostException: &lt;YOUR_TOWER_HOSTNAME&gt;\n    at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:220)\n    at java.base/java.net.Socket.connect(Socket.java:609)\n    at java.base/java.net.Socket.connect(Socket.java:558)\n    at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:182)\n    at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)\n    at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)\n    at java.base/sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:242)\n    at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:341)\n    at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:362)\n    at java.base/sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1253)\n    at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1187)\n    at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1081)\n    at java.base/sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:1015)\n    at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1592)\n    at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1520)\n    at nextflow.file.http.XFileSystemProvider.newInputStream(XFileSystemProvider.groovy:291)\n    at java.base/java.nio.file.Files.newInputStream(Files.java:156)\n    at java.base/java.nio.file.Files.newBufferedReader(Files.java:2839)\n    at org.apache.groovy.nio.extensions.NioExtensions.newReader(NioExtensions.java:1404)\n    at org.apache.groovy.nio.extensions.NioExtensions.getText(NioExtensions.java:397)\n    at nextflow.scm.ProviderConfig.getFromFile(ProviderConfig.groovy:270)\n    at nextflow.scm.ProviderConfig.getDefault(ProviderConfig.groovy:287)\n    at nextflow.scm.AssetManager.&lt;init&gt;(AssetManager.groovy:107)\n    at nextflow.cli.CmdRun.getScriptFile(CmdRun.groovy:360)\n    at nextflow.cli.CmdRun.run(CmdRun.groovy:265)\n    at nextflow.cli.Launcher.run(Launcher.groovy:475)\n    at nextflow.cli.Launcher.main(Launcher.groovy:657)\n</code></pre>  <p>SOLUTION</p> <p>This error indicates that Nextflow running in AWS Batch jobs is not able to connect to your Tower instance.</p> <p>The solution is to specify the correct <code>VPC</code> and <code>Security Group</code> during the creation of <code>Compute Environment</code>. For further information, please refer to the note regarding <code>Networking configs</code> after point-17 in the AWS compute environment setup guide.</p>","title":"Workflow execution fails with <code>java.net.UnknownHostException</code>"},{"location":"enterprise/aws_troubleshooting/#long-running-workflow-fails-because-of-failure-to-retrieve-ecs-metadata","tags":["aws","troubleshooting","deployment"],"text":"<p>Long running workflows eventually fail with partially completed processes, with the following error message:</p> <pre>1</pre><pre><code>Error when retrieving credentials from container-role: Error retrieving metadata: Received error when attempting to retrieve ECS metadata: Read timeout on endpoint URL: `http://&lt;YOUR_HOST_IP&gt;/v2/credentials/xxxxxxxxxx-a707-2cea702a1fb9`\n</code></pre> <p>SOLUTION</p> <p>The solution is to increase the throttling rate in the user data script, in the launch template</p> <pre>1</pre><pre><code>echo \"ECS_TASK_METADATA_RPS_LIMIT=120,180\" &gt;&gt; /etc/ecs/ecs.config\n</code></pre>","title":"Long running workflow fails because of failure to retrieve ECS metadata"},{"location":"enterprise/docker-compose/","tags":["docker","compose","deployment"],"text":"<p>This guide assumes that all prerequisites have been met. Please visit the corresponding Prerequisites page for your infrastructure provider.</p> <p>We recommend configuring your database or Redis details in either the <code>tower.yml</code> or the <code>docker-compose.yml</code>, but not both.</p>   <p>Note</p> <p>In order for your DB or Redis volume to persist after a <code>docker restart</code>, uncomment the <code>volumes</code> key in the <code>db</code> or <code>redis</code> section of your docker-compose.yml file. Use this section to specify a local path to the DB or Redis instance.</p>","title":"Docker Compose"},{"location":"enterprise/docker-compose/#deploy-tower","tags":["docker","compose","deployment"],"text":"Environment variables <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22</pre><pre><code> TOWER_SERVER_URL=http://localhost:8000\n TOWER_CONTACT_EMAIL=admin@your-org.com\n TOWER_JWT_SECRET=&lt;Replace This With A Secret String containing at least 35 characters&gt;\n TOWER_CRYPTO_SECRETKEY=&lt;Replace This With Another Secret String&gt;\n TOWER_LICENSE=&lt;YOUR TOWER LICENSE KEY&gt;\n\n # Compute environment settings\n TOWER_ENABLE_PLATFORMS=awsbatch-platform,azbatch-platform,gls-platform,k8s-platform,slurm-platform\n\n # DB settings\n TOWER_DB_URL=jdbc:mysql://db:3306/tower\n TOWER_DB_DRIVER=org.mariadb.jdbc.Driver\n TOWER_DB_DIALECT=io.seqera.util.MySQL55DialectCollateBin\n TOWER_DB_USER=tower\n TOWER_DB_PASSWORD=tower\n FLYWAY_LOCATIONS=classpath:db-schema/mysql\n\n # SMTP settings \n TOWER_SMTP_HOST=mail\n TOWER_SMTP_PORT=587\n TOWER_SMTP_USER=foo\n TOWER_SMTP_PASSWORD=foo\n</code></pre>   tower.yml <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48</pre><pre><code> # Replace these settings with a SMTP server provided by your cloud vendor\n # The mail scope is used for providing config to the underlying Micronaut framework\n mail:\n   from: \"${TOWER_CONTACT_EMAIL}\"\n   smtp:\n     host: ${TOWER_SMTP_HOST}\n     port: ${TOWER_SMTP_PORT}\n     user: ${TOWER_SMTP_USER}\n     password: ${TOWER_SMTP_PASSWORD}\n     # `starttls` should be enabled with a production SMTP host\n     auth: true\n     starttls:\n       enable: false\n       required: false\n\n # Duration of Tower sign-in email link validity\n auth:\n   mail:\n     duration: 30m\n\n # The tower scope is used for providing config for your Tower Enterprise installation\n tower:\n   trustedEmails:\n       - '*@org.xyz'\n       - 'named_user@org.xyz'\n\n   # Tower instance-wide configuration for authentication. For further information, see https://install.tower.nf/latest/configuration/authentication/\n   auth:\n     google:\n       allow-list:\n         - \"*@org.xyz\"\n     oidc:\n       allow-list:\n         - \"*@org.xyz\"\n\n   # Tower instance-wide configuration for SCM providers. For further information, see https://install.tower.nf/latest/configuration/git_integration/\n   scm:\n     providers:\n       github:\n         user: &lt;YOUR GITHUB USER NAME&gt;\n         password: &lt;YOUR GITHUB ACCESS TOKEN OR PASSWORD&gt;\n       gitlab:\n         user: &lt;YOUR GITLAB USER NAME&gt;\n         password: &lt;YOUR GITLAB PASSWORD&gt;\n         token: &lt;YOUR GITLAB TOKEN&gt;\n       bitbucket:\n         user: &lt;YOUR BITBUCKET USER NAME&gt;\n         password: &lt;YOUR BITBUCKET TOKEN OR PASSWORD&gt;\n</code></pre>   docker-compose.yml <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80</pre><pre><code> version: \"3\"\n services:\n   db:\n     image: mysql:5.6\n     networks:\n       - backend\n     expose:\n       - 3306\n     environment:\n       MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n       MYSQL_USER: tower\n       MYSQL_PASSWORD: tower\n       MYSQL_DATABASE: tower\n     restart: always\n   # enable this snippet to store the Mysql data in the host volume\n   #    volumes:\n   #      - $HOME/.tower/db/mysql:/var/lib/mysql\n\n   redis:\n     image: cr.seqera.io/public/redis:5.0.8\n     networks:\n       - backend\n     expose:\n       - 6379\n     command: --appendonly yes\n     restart: always\n   # enable this snippet to store the Redis data in the host volume\n   #    volumes:\n   #      - $HOME/.tower/db/redis:/data\n\n   cron:\n     image: cr.seqera.io/private/nf-tower-enterprise/backend:v23.1.0\n     command: -c '/wait-for-it.sh db:3306 -t 60; /migrate-db.sh; /tower.sh'\n     networks:\n       - frontend\n       - backend\n     volumes:\n       - $PWD/tower.yml:/tower.yml\n     env_file:\n       - tower.env\n     environment:\n       - MICRONAUT_ENVIRONMENTS=prod,redis,cron\n     restart: always\n     depends_on:\n       - db\n       - redis\n\n   backend:\n     image: cr.seqera.io/private/nf-tower-enterprise/backend:v23.1.0\n     command: -c '/wait-for-it.sh db:3306 -t 60; /tower.sh'\n     networks:\n       - frontend\n       - backend\n     expose:\n       - 8080\n     volumes:\n       - $PWD/tower.yml:/tower.yml\n     env_file:\n       - tower.env\n     environment:\n       - MICRONAUT_ENVIRONMENTS=prod,redis,ha\n     restart: always\n     depends_on:\n       - db\n       - redis\n       - cron\n\n   frontend:\n     image: cr.seqera.io/private/nf-tower-enterprise/frontend:v23.1.0\n     networks:\n       - frontend\n     ports:\n       - 8000:80\n     restart: always\n     depends_on:\n       - backend\n\n networks:\n   frontend: {}\n   backend: {}\n</code></pre>  <ol> <li> <p>Download and configure tower.env.</p> </li> <li> <p>Download and configure tower.yml, update values for allowed emails.</p> </li> <li> <p>Download and configure docker-compose.yml.</p> </li> </ol> <p>The <code>db</code> and <code>mail</code> containers should only be used for local testing; you may remove them if you have configured these services elsewhere.</p> <p>Make sure to customize the <code>TOWER_ENABLE_PLATFORMS</code> variable to include the execution platform(s) you will use.</p> <ol> <li>Deploy Tower and wait for it to initialize (takes a few minutes):</li> </ol> <pre>1</pre><pre><code>docker-compose up\n</code></pre>   <p>Note</p> <p>For more information on configuration, see Configuration options.</p>","title":"Deploy Tower"},{"location":"enterprise/docker-compose/#test-the-application","tags":["docker","compose","deployment"],"text":"<p>To make sure that Tower is properly configured, follow these steps:</p> <ol> <li> <p>Login to Tower.</p> </li> <li> <p>Create an organization.</p> </li> <li> <p>Create a workspace within that organization.</p> </li> <li> <p>Create a new Compute Environment. Refer to Compute Environments for detailed instructions.</p> </li> <li> <p>Select Quick Launch from the Launchpad tab in your workspace.</p> </li> <li> <p>Enter the repository URL for the <code>nf-core/rnaseq</code> pipeline (<code>https://github.com/nf-core/rnaseq</code>).</p> </li> <li> <p>In the Config profiles dropdown, select the <code>test</code> profile.</p> </li> <li> <p>In the Pipeline parameters text area, change the output directory to a sensible location based on your Compute Environment:</p> </li> </ol> <pre>1\n2\n3\n4\n5</pre><pre><code># save to S3 bucket\noutdir: s3://&lt;your-bucket&gt;/results\n\n# save to scratch directory (Kubernetes)\noutdir: /scratch/results\n</code></pre> <ol> <li>Select Launch.</li> </ol> <p>You'll be transitioned to the Runs tab for the workflow. After a few minutes, you'll see the progress logs in the Execution log tab for that workflow.</p>   <p>Tip</p> <p>Once you've made sure that Tower is configured correctly and you can launch workflows, you can run <code>docker-compose up -d</code> to deploy Tower as a background process. You can then disconnect from the VM instance.</p>","title":"Test the application"},{"location":"enterprise/general_troubleshooting/","tags":["troubleshooting"],"text":"","title":"Troubleshooting"},{"location":"enterprise/general_troubleshooting/#deployment","tags":["troubleshooting"],"text":"","title":"Deployment"},{"location":"enterprise/general_troubleshooting/#deployments-based-on-httpsgitseqeraio-repositories","tags":["troubleshooting"],"text":"<p>The <code>https://git.seqera.io</code> service has been decommissioned. You can access the configuration file templates needed for your deployment from these pages:</p> <p>Docker compose</p> <p>Kubernetes</p> <p>For further deployment and troubleshooting assistance, contact Seqera Labs support</p>","title":"Deployments based on https://git.seqera.io/ repositories"},{"location":"enterprise/general_troubleshooting/#configuration","tags":["troubleshooting"],"text":"","title":"Configuration"},{"location":"enterprise/general_troubleshooting/#ohejdbcspisqlexceptionhelper-incorrect-string-value-in-cron-log","tags":["troubleshooting"],"text":"<pre>1\n2\n3</pre><pre><code> [scheduled-executor-thread-2] - WARN  o.h.e.jdbc.spi.SqlExceptionHelper - SQL Error: 1366, SQLState: HY000\n [scheduled-executor-thread-2] - ERROR o.h.e.jdbc.spi.SqlExceptionHelper - (conn=34) Incorrect string value: '\\xF0\\x9F\\x94\\x8D |...' for column 'error_report' at row 1\n [scheduled-executor-thread-2] - ERROR i.s.t.service.job.JobSchedulerImpl - Oops .. unable to save status of job id=18165; name=nf-workflow-26uD5XXXXXXXX; opId=nf-workflow-26uD5XXXXXXXX; status=UNKNOWN\n</code></pre>  <p>Runs will fail if your Nextflow script or Nextflow config contain illegal characters (such as emojis or other non-UTF8 characters). Validate your script and config files for any illegal characters before atttempting to run again.</p>","title":"\"o.h.e.jdbc.spi.SqlExceptionHelper - Incorrect string value\" in cron log"},{"location":"enterprise/general_troubleshooting/#networking","tags":["troubleshooting"],"text":"","title":"Networking"},{"location":"enterprise/general_troubleshooting/#runs-fail-with-503-errors","tags":["troubleshooting"],"text":"<p>Error 503 suggests that a service is unreachable. In the Tower context, this means that one or more of the services being contacted as part of workflow execution are unavailable. Troubleshoot this error by ensuring all required services are running and available. Establishing connection to your database is often the culprit for this error. </p>","title":"Runs fail with <code>503</code> errors"},{"location":"enterprise/general_troubleshooting/#sockettimeoutexception-connect-timed-out-error","tags":["troubleshooting"],"text":"<p>This problem is often observed while trying to launch workflows from a self-hosted Git server, for e.g. Bitbucket, Gitlab etc.</p> <p>This error signals that the <code>backend/cron</code> container cannot connect to the Git remote host. This can be caused by a missing proxy configuration.</p>   Expand error log <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90</pre><pre><code>ERROR i.s.t.c.GlobalErrorController - Oops... Unable to process request - Error ID: 6h3HBUkaPe03vgzoDPc5HO\njava.net.SocketTimeoutException: connect timed out\n        at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n        at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n        at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n        at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n        at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n        at java.base/java.net.Socket.connect(Socket.java:609)\n        at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:289)\n        at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)\n        at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)\n        at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)\n        at java.base/sun.net.www.protocol.https.HttpsClient.&lt;init&gt;(HttpsClient.java:265)\n        at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:372)\n        at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)\n        at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1187)\n        at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1081)\n        at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)\n        at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1592)\n        at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1520)\n        at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)\n        at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)\n        at nextflow.scm.RepositoryProvider.checkResponse(RepositoryProvider.groovy:167)\n        at nextflow.scm.RepositoryProvider.invoke(RepositoryProvider.groovy:136)\n        at nextflow.scm.RepositoryProvider.memoizedMethodPriv$invokeAndParseResponseString(RepositoryProvider.groovy:218)\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.base/java.lang.reflect.Method.invoke(Method.java:566)\n        at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)\n        at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)\n        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1259)\n        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1026)\n        at org.codehaus.groovy.runtime.InvokerHelper.invokePogoMethod(InvokerHelper.java:1029)\n        at org.codehaus.groovy.runtime.InvokerHelper.invokeMethod(InvokerHelper.java:1012)\n        at org.codehaus.groovy.runtime.InvokerHelper.invokeMethodSafe(InvokerHelper.java:101)\n        at nextflow.scm.RepositoryProvider$_closure2.doCall(RepositoryProvider.groovy)\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.base/java.lang.reflect.Method.invoke(Method.java:566)\n        at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)\n        at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)\n        at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:263)\n        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1026)\n        at groovy.lang.Closure.call(Closure.java:412)\n        at org.codehaus.groovy.runtime.memoize.Memoize$MemoizeFunction.lambda$call$0(Memoize.java:137)\n        at org.codehaus.groovy.runtime.memoize.ConcurrentCommonCache.getAndPut(ConcurrentCommonCache.java:137)\n        at org.codehaus.groovy.runtime.memoize.ConcurrentCommonCache.getAndPut(ConcurrentCommonCache.java:113)\n        at org.codehaus.groovy.runtime.memoize.Memoize$MemoizeFunction.call(Memoize.java:136)\n        at groovy.lang.Closure.call(Closure.java:428)\n        at nextflow.scm.RepositoryProvider.invokeAndParseResponse(RepositoryProvider.groovy)\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.base/java.lang.reflect.Method.invoke(Method.java:566)\n        at org.codehaus.groovy.runtime.callsite.PlainObjectMetaMethodSite.doInvoke(PlainObjectMetaMethodSite.java:43)\n        at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite$PogoCachedMethodSiteNoUnwrapNoCoerce.invoke(PogoMetaMethodSite.java:193)\n        at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite.callCurrent(PogoMetaMethodSite.java:61)\n        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:185)\n        at nextflow.scm.BitbucketRepositoryProvider.getCloneUrl(BitbucketRepositoryProvider.groovy:114)\n        at nextflow.scm.AssetManager.memoizedMethodPriv$getGitRepositoryUrl(AssetManager.groovy:394)\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.base/java.lang.reflect.Method.invoke(Method.java:566)\n        at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)\n        at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)\n        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1259)\n        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1026)\n        at org.codehaus.groovy.runtime.InvokerHelper.invokePogoMethod(InvokerHelper.java:1029)\n        at org.codehaus.groovy.runtime.InvokerHelper.invokeMethod(InvokerHelper.java:1012)\n        at org.codehaus.groovy.runtime.InvokerHelper.invokeMethodSafe(InvokerHelper.java:101)\n        at nextflow.scm.AssetManager$_closure1.doCall(AssetManager.groovy)\n        at nextflow.scm.AssetManager$_closure1.doCall(AssetManager.groovy)\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.base/java.lang.reflect.Method.invoke(Method.java:566)\n        at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)\n        at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)\n        at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:263)\n        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1026)\n        at groovy.lang.Closure.call(Closure.java:412)\n        at org.codehaus.groovy.runtime.memoize.Memoize$MemoizeFunction.lambda$call$0(Memoize.java:137)\n        at org.codehaus.groovy.runtime.memoize.ConcurrentCommonCache.getAndPut(ConcurrentCommonCache.java:137)\n        at org.codehaus.groovy.runtime.memoize.ConcurrentCommonCache.getAndPut(ConcurrentCommonCache.java:113)\n        at org.codehaus.groovy.runtime.memoize.Memoize$MemoizeFunction.call(Memoize.java:136)\n        at groovy.lang.Closure.call(Closure.java:406)\n        at nextflow.scm.AssetManager.getGitRepositoryUrl(AssetManager.groovy)\n</code></pre>   <p>SOLUTION</p> <p>Update the HTTP proxy configuration in the <code>backend</code> and <code>cron</code> environment. eg</p>  <pre>1\n2</pre><pre><code>export http_proxy=\"http://PROXY_SERVER:PORT\"\nexport https_proxy=\"https://PROXY_SERVER:PORT\"\n</code></pre>","title":"\"SocketTimeoutException: connect timed out\" error"},{"location":"enterprise/general_troubleshooting/#email-server","tags":["troubleshooting"],"text":"","title":"Email server"},{"location":"enterprise/general_troubleshooting/#how-to-configure-smtp-gateway-which-does-not-require-authentication","tags":["troubleshooting"],"text":"<p>SOLUTION</p> <p>Since the SMTP gateway allows sending email without the need to specify a user name and passwords, the <code>user</code> and <code>password</code> should be set to <code>null</code>.</p> <p>Please replace the <code>mail</code> section in your <code>tower.yml</code> with the following.</p>  <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11</pre><pre><code>mail:\n    from: \"${TOWER_CONTACT_EMAIL}\"\n    smtp:\n        host: ${TOWER_SMTP_HOST}\n        port: ${TOWER_SMTP_PORT}\n        user: null\n        password: null\n        auth: false\n        starttls:\n            enable: false\n            required: false\n</code></pre>","title":"How to configure SMTP gateway, which does not require authentication?"},{"location":"enterprise/general_troubleshooting/#unable-to-receive-emails-for-tower_contact_email","tags":["troubleshooting"],"text":"<p>This error occurs due to the organizational security policy on for emails.</p> <p>In case you've setup the SMTP server correctly and the emails are sent correctly via Tower, but they are rejected by your organizational email ID</p> <p>SOLUTION</p> <p>You need to setup/configure the <code>spf</code>, <code>dkim</code> and <code>dmarc</code> for your domain.</p> <p>For further assistance, please contact your IT staff.</p>","title":"Unable to receive emails for <code>TOWER_CONTACT_EMAIL</code>"},{"location":"enterprise/general_troubleshooting/#database","tags":["troubleshooting"],"text":"","title":"Database"},{"location":"enterprise/general_troubleshooting/#sign-in-fails-with-javasqlsqlexception-in-the-backend-log","tags":["troubleshooting"],"text":"<p>While trying to log in, after the authentication, the <code>Oops... Unable to process request</code> error message is observed.</p>   Expand error log <pre>1\n2\n3\n4\n5\n6</pre><pre><code>io.micronaut.transaction.exceptions.CannotCreateTransactionException: Could not open Hibernate Session for transaction\n\u2026\nCaused by: org.hibernate.exception.GenericJDBCException: Unable to acquire JDBC Connection\n\u2026\njava.sql.SQLException: The server time zone value \u2018CEST\u2019 is unrecognized or represents more than one time zone. You must configure either the server or JDBC driver (via the \u2018serverTimezone\u2019 configuration property) to use a more specific time zone value if you want to utilize time zone support.\n\u2026\n</code></pre>   <p>SOLUTION</p> <p>Generally, this means that the webapp is not able to connect to the database and <code>JDBC</code> client needs to specify the time zone value via <code>serverTimezone</code>.</p> <p>To resolve this issue for <code>Europe/Amsterdam</code> time zone, please update the value of <code>TOWER_DB_URL</code> as shown below</p> <pre>1</pre><pre><code>export TOWER_DB_URL\": \"jdbc:mysql://&lt;YOUR_DATABASE_IP&gt;:3306/tower?serverTimezone=Europe/Amsterdam\"\n</code></pre>","title":"Sign in fails with <code>java.sql.SQLException</code> in the backend log"},{"location":"enterprise/general_troubleshooting/#javaioioexception-unsupported-protocol-version-252-is-observed-while-completed-or-terminated-runs-still-display-as-in-progress-on-tower","tags":["troubleshooting"],"text":"<p>When a service is restarted or otherwise interrupted, this may result in invalid entries which corrupt the cache of your installation's Redis instance. If this error and behavior is observed, the key containing the invalid entry must be manually deleted in order to restore corrrect Tower behavior. Run the following commands (replace <code>container-name</code> with your container name):</p>  <pre>1\n2\n3\n4\n5\n6\n7\n8</pre><pre><code>## To check if the key exists\ndocker exec -ti [container-name] redis-cli keys \\* | grep workflow\n\n## This will show the hash contents of the key\ndocker exec -ti [container-name] redis-cli hgetall \"workflow/modified\"\n\n## To delete the key\ndocker exec -ti [container-name] redis-cli del \"workflow/modified\"\n</code></pre>","title":"<code>java.io.IOException: Unsupported protocol version 252</code> is observed while completed or terminated runs still display as in progress on Tower"},{"location":"enterprise/general_troubleshooting/#authentication","tags":["troubleshooting"],"text":"","title":"Authentication"},{"location":"enterprise/general_troubleshooting/#sign-in-fails-with-a-500-error-code-frontend-logs-while-using-openid-connect-provider","tags":["troubleshooting"],"text":"Expand error log <pre>1</pre><pre><code>*8317 upstream sent too big header while reading response header from upstream, client: 10.170.157.186, server: localhost, request: \"GET /oauth/callback\n</code></pre>   <p>SOLUTION</p> <p>This happens when using a OpenID connect, the callback request could send too big HTTP headers causing the Tower web server to report that error message.</p> <p>The solution consists of rebuilding the frontend container adding the following proxy directives in the <code>/etc/nginx/nginx.conf</code> file.</p>  <pre>1\n2\n3</pre><pre><code>        proxy_buffer_size          128k;\n        proxy_buffers              4 256k;\n        proxy_busy_buffers_size    256k;\n</code></pre>","title":"Sign in fails with a 500 error code frontend logs while using OpenID connect provider"},{"location":"enterprise/general_troubleshooting/#using-tower_jwt_secret-to-generate-user-login-tokens-user-sign-in-fails-with-javalangillegalargumentexception-jwt-token-generator-secret-should-be-35-characters-or-longer-in-the-tower-backend-log","tags":["troubleshooting"],"text":"<p>The secret used to generate the login JWT token that is appended to user sign-in email URLs needs to be at least 35 characters long. See here.</p>","title":"Using <code>TOWER_JWT_SECRET</code> to generate user login tokens, user sign in fails, with <code>java.lang.IllegalArgumentException: JWT token generator secret should be 35 characters or longer</code> in the Tower backend log."},{"location":"enterprise/general_troubleshooting/#on-prem-hpc-compute-environment-exhausted-available-authentication-method-error-in-tower","tags":["troubleshooting"],"text":"<p>This error points to an issue with the SSH credentials used to authenticate Tower to your HPC cluster (LSF, Slurm, etc.), such as an invalid SSH key or inappropriate permissions on the user directory. Check the following:</p> <ol> <li> <p>Test the SSH key to ensure it is still valid. If not, create new SSH keys and re-create the compute environment in Tower using the updated credentials.</p> </li> <li> <p>Check the backend logs for a stack trace similar to the following:</p> <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18</pre><pre><code>[io-executor-thread-2] 10.42.0.1 ERROR i.s.t.c.GlobalErrorController - Oops... Unable to process request - Error ID: 5d7rDpS8pByF8YqfUVPvB4\nnet.schmizz.sshj.userauth.UserAuthException: Exhausted available authentication methods\n    at net.schmizz.sshj.SSHClient.auth(SSHClient.java:227)\n    at net.schmizz.sshj.SSHClient.authPublickey(SSHClient.java:342)\n    at net.schmizz.sshj.SSHClient.authPublickey(SSHClient.java:360)\n    at io.seqera.tower.service.platform.ssh.SSHClientFactory.createClient(SSHClientFactory.groovy:110)\n..\n..\nCaused by: net.schmizz.sshj.userauth.UserAuthException: Problem getting public key from PKCS5KeyFile{resource=[PrivateKeyStringResource]}\n    at net.schmizz.sshj.userauth.method.KeyedAuthMethod.putPubKey(KeyedAuthMethod.java:47)\n    at net.schmizz.sshj.userauth.method.AuthPublickey.buildReq(AuthPublickey.java:62)\n    at net.schmizz.sshj.userauth.method.AuthPublickey.buildReq(AuthPublickey.java:81)\n    at net.schmizz.sshj.userauth.method.AbstractAuthMethod.request(AbstractAuthMethod.java:68)\n    at net.schmizz.sshj.userauth.UserAuthImpl.authenticate(UserAuthImpl.java:73)\n    at net.schmizz.sshj.SSHClient.auth(SSHClient.java:221)\n    ... 91 common frames omitted\nCaused by: net.schmizz.sshj.userauth.keyprovider.PKCS5KeyFile$FormatException: Length mismatch: 1152 != 1191\n    at net.schmizz.sshj.userauth.keyprovider.PKCS5KeyFile$ASN1Data.&lt;init&gt;(PKCS5KeyFile.java:248)\n</code></pre> </li> <li> <p>Enable SSH library log tracing with the following environbment variable in your <code>tower.env</code> file for verbose debug logging of the SSH connection:</p> </li> </ol>  <pre>1</pre><pre><code>TOWER_SSH_LOGLEVEL=TRACE\n</code></pre>  <ol> <li>Check the permissions of the <code>/home</code> directory of the user tied to the cluster's SSH credentials. <code>/home/[user]</code> should be <code>chmod 755</code>, whereas <code>/home/[user]/ssh</code> requires <code>chmod 700</code>.</li> </ol>  <pre>1\n2\n3\n4\n5\n6\n7</pre><pre><code>$ pwd ; ls -ld .\n/home/user\ndrwxr-xr-x 41 user user 20480\n\n$ pwd; ls -ld .\n/home/user/.ssh\ndrwx------ 2 user user 4096\n</code></pre>","title":"On-prem HPC compute environment: <code>Exhausted available authentication method</code> error in Tower"},{"location":"enterprise/kubernetes/","tags":["kubernetes","deployment"],"text":"<p>This guide assumes that all prerequisites have been met. Please visit the corresponding Prerequisites page for your infrastructure provider.</p>  <p>Note</p> <p>You may also use this guide for deployments to other cloud platforms (e.g. Oracle Kubernetes Engine), however it is up to you to satisfy any prerequisites for those platforms. Use at your own risk.</p>","title":"Kubernetes"},{"location":"enterprise/kubernetes/#deploy-tower","tags":["kubernetes","deployment"],"text":"","title":"Deploy Tower"},{"location":"enterprise/kubernetes/#create-a-namespace","tags":["kubernetes","deployment"],"text":"<p>Create a namespace to group the Tower resources within your K8s cluster.</p> <ol> <li> <p>Create the namespace (e.g. <code>tower-nf</code>):</p> <pre>1</pre><pre><code>kubectl create namespace tower-nf\n</code></pre> </li> <li> <p>Switch to the namespace:</p> <pre>1</pre><pre><code>kubectl config set-context --current --namespace=tower-nf\n</code></pre> </li> </ol>","title":"Create a namespace"},{"location":"enterprise/kubernetes/#configure-container-registry-credentials","tags":["kubernetes","deployment"],"text":"<p>Nextflow Tower is distributed as a collection of Docker containers available through the Seqera Labs  container registry <code>cr.seqera.io</code>. Contact support to get your container access credentials. Once you have received your credentials, grant your cluster access to the registry using these steps:</p> <ol> <li> <p>Retrieve the <code>name</code> and <code>secret</code> values from the JSON file you received from Seqera Labs support.</p> </li> <li> <p>Create a Kubernetes Secret, using the <code>name</code> and <code>secret</code> retrieved in step 1, with this command:</p> <pre>1\n2\n3\n4</pre><pre><code>kubectl create secret docker-registry cr.seqera.io \\\n  --docker-server=cr.seqera.io \\\n  --docker-username='&lt;YOUR NAME&gt;' \\\n  --docker-password='&lt;YOUR SECRET&gt;'\n</code></pre> <p>Note: The credential <code>name</code> contains a dollar <code>$</code> character. To prevent the Linux shell from interpreting this value as an environment variable, wrap it in single quotes.  </p> </li> <li> <p>The following snippet configures the Tower cron service and the Tower frontend and backend to use the Secret created in step 2 (see tower-cron.yml and tower-svc.yml):</p> </li> </ol> <pre>1\n2</pre><pre><code>imagePullSecrets:\n        - name: \"cr.seqera.io\"\n</code></pre> <p>This parameter is already included in the templates linked above \u2014 if you use a name other than <code>cr.seqera.io</code> for the Kubernetes Secret, update this value accordingly in the configuration files. </p>","title":"Configure container registry credentials"},{"location":"enterprise/kubernetes/#tower-configmap","tags":["kubernetes","deployment"],"text":"configmap.yml <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47</pre><pre><code>   kind: ConfigMap\n   apiVersion: v1\n   metadata:\n     name: tower-backend-cfg\n     labels:\n       app: backend-cfg\n   data:\n     TOWER_SERVER_URL: \"https://&lt;YOUR PUBLIC TOWER HOST NAME&gt;\"\n     TOWER_CONTACT_EMAIL: \"support@tower.nf\"\n     TOWER_JWT_SECRET: \"ReplaceThisWithALongSecretString\"\n     TOWER_DB_URL: \"jdbc:mysql://&lt;YOUR DB HOST NAME AND PORT&gt;/tower\"\n     TOWER_DB_DRIVER: \"org.mariadb.jdbc.Driver\"\n     TOWER_DB_USER: \"tower\"\n     TOWER_DB_PASSWORD: \"&lt;YOUR DB PASSWORD&gt;\"\n     TOWER_DB_DIALECT: \"io.seqera.util.MySQL55DialectCollateBin\"\n     TOWER_DB_MIN_POOL_SIZE: \"2\"\n     TOWER_DB_MAX_POOL_SIZE: \"10\"\n     TOWER_DB_MAX_LIFETIME: \"180000\"\n     TOWER_SMTP_HOST: \"&lt;YOUR SMTP SERVER HOST NAME&gt;\"\n     TOWER_SMTP_USER: \"&lt;YOUR SMTP USER NAME&gt;\"\n     TOWER_SMTP_PASSWORD: \"&lt;YOUR SMTP USER PASSWORD&gt;\"\n     TOWER_CRYPTO_SECRETKEY: \"&lt;YOUR CRYPTO SECRET&gt;\"\n     TOWER_LICENSE: \"&lt;YOUR TOWER LICENSE KEY&gt;\"\n     TOWER_ENABLE_PLATFORMS: \"awsbatch-platform,gls-platform,azbatch-platform,slurm-platform\"\n     FLYWAY_LOCATIONS: \"classpath:db-schema/mysql\"\n     TOWER_REDIS_URL: \"redis://&lt;YOUR REDIS IP&gt;:6379\"\n   ---\n   kind: ConfigMap\n   apiVersion: v1\n   metadata:\n     name: tower-yml\n     labels:\n       app: backend-cfg\n   data:\n     tower.yml: |\n       mail:\n         smtp:\n           auth: true\n           # FIXME `starttls` should be enabled with a production SMTP host\n           starttls:\n             enable: false\n             required: false\n\n       # Uncomment to specify the duration of Tower sign-in email link validity\n       auth:\n         mail:\n           duration: 30m\n</code></pre>  <ol> <li> <p>Download and configure configmap.yml as per the configuration page.</p> </li> <li> <p>Deploy the configmap to your cluster:</p> <pre>1</pre><pre><code>kubectl apply -f configmap.yml\n</code></pre> </li> </ol>  <p>Where is my tower.yml?</p> <p>The <code>configmap.yml</code> manifest includes both <code>tower.env</code> and <code>tower.yml</code>. These files are made available to the other containers through volume mounts.</p>","title":"Tower ConfigMap"},{"location":"enterprise/kubernetes/#redis","tags":["kubernetes","deployment"],"text":"redis.aks.yml <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90</pre><pre><code> kind: StorageClass\n apiVersion: storage.k8s.io/v1\n metadata:\n   name: standard\n   labels:\n     app: redis\n   annotations:\n     storageclass.beta.kubernetes.io/is-default-class: \"true\"\n provisioner: kubernetes.io/disk.csi.azure.com\n parameters:\n   kind: Managed\n   storageaccounttype: Premium_LRS\n allowVolumeExpansion: true\n reclaimPolicy: Retain\n ---\n apiVersion: v1\n kind: PersistentVolumeClaim\n metadata:\n   name: redis-data\n   labels:\n     app: redis\n spec:\n   accessModes:\n     - ReadWriteOnce\n   resources:\n     requests:\n       storage: 10Gi\n   storageClassName: standard\n ---\n apiVersion: apps/v1\n kind: StatefulSet\n metadata:\n   name: redis\n   labels:\n     app: redis\n spec:\n   selector:\n     matchLabels:\n       app: redis\n   serviceName: redis\n   template:\n     metadata:\n       labels:\n         app: redis\n     spec:\n       initContainers:\n         - name: init-sysctl\n           image: busybox\n           command:\n             - /bin/sh\n             - -c\n             - |\n               sysctl -w net.core.somaxconn=1024;\n               echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled\n           securityContext:\n             privileged: true\n           volumeMounts:\n             - name: host-sys\n               mountPath: /sys\n       containers:\n         - image: cr.seqera.io/public/redis:5.0.8\n           name: redis\n           args:\n             - --appendonly yes\n           ports:\n             - containerPort: 6379\n           volumeMounts:\n             - mountPath: \"/data\"\n               name: \"vol-data\"\n       volumes:\n         - name: vol-data\n           persistentVolumeClaim:\n             claimName: redis-data\n         - name: host-sys\n           hostPath:\n             path: /sys\n       restartPolicy: Always\n ---\n apiVersion: v1\n kind: Service\n metadata:\n   name: redis\n   labels:\n     app: redis\n spec:\n   ports:\n     - port: 6379\n       targetPort: 6379\n   selector:\n     app: redis\n</code></pre>   redis.eks.yml <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90</pre><pre><code> kind: StorageClass\n apiVersion: storage.k8s.io/v1\n metadata:\n   name: standard\n   labels:\n     app: redis\n   annotations:\n     storageclass.beta.kubernetes.io/is-default-class: \"true\"\n provisioner: kubernetes.io/aws-ebs\n parameters:\n   type: gp2\n   fsType: ext4\n allowVolumeExpansion: true\n reclaimPolicy: Retain\n ---\n apiVersion: v1\n kind: PersistentVolumeClaim\n metadata:\n   name: redis-data\n   labels:\n     app: redis\n spec:\n   accessModes:\n     - ReadWriteOnce\n   resources:\n     requests:\n       storage: 10Gi\n   storageClassName: standard\n ---\n apiVersion: apps/v1\n kind: StatefulSet\n metadata:\n   name: redis\n   labels:\n     app: redis\n spec:\n   selector:\n     matchLabels:\n       app: redis\n   serviceName: redis\n   template:\n     metadata:\n       labels:\n         app: redis\n     spec:\n       initContainers:\n         - name: init-sysctl\n           image: busybox\n           command:\n             - /bin/sh\n             - -c\n             - |\n               sysctl -w net.core.somaxconn=1024;\n               echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled\n           securityContext:\n             privileged: true\n           volumeMounts:\n             - name: host-sys\n               mountPath: /sys\n       containers:\n         - image: cr.seqera.io/public/redis:5.0.8\n           name: redis\n           args:\n             - --appendonly yes\n           ports:\n             - containerPort: 6379\n           volumeMounts:\n             - mountPath: \"/data\"\n               name: \"vol-data\"\n       volumes:\n         - name: vol-data\n           persistentVolumeClaim:\n             claimName: redis-data\n         - name: host-sys\n           hostPath:\n             path: /sys\n       restartPolicy: Always\n ---\n apiVersion: v1\n kind: Service\n metadata:\n   name: redis\n   labels:\n     app: redis\n spec:\n   ports:\n     - port: 6379\n       targetPort: 6379\n   selector:\n     app: redis\n</code></pre>   redis.gke.yml <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75</pre><pre><code> apiVersion: v1\n kind: PersistentVolumeClaim\n metadata:\n   name: redis-data\n   labels:\n     app: redis\n spec:\n   accessModes:\n     - ReadWriteOnce\n   resources:\n     requests:\n       storage: 10Gi\n   storageClassName: standard\n ---\n apiVersion: apps/v1\n kind: StatefulSet\n metadata:\n   name: redis\n   labels:\n     app: redis\n spec:\n   selector:\n     matchLabels:\n       app: redis\n   serviceName: redis\n   template:\n     metadata:\n       labels:\n         app: redis\n     spec:\n       initContainers:\n         - name: init-sysctl\n           image: busybox\n           command:\n             - /bin/sh\n             - -c\n             - |\n               sysctl -w net.core.somaxconn=1024;\n               echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled\n           securityContext:\n             privileged: true\n           volumeMounts:\n             - name: host-sys\n               mountPath: /sys\n       containers:\n         - image: cr.seqera.io/public/redis:5.0.8\n           name: redis\n           args:\n             - --appendonly yes\n           ports:\n             - containerPort: 6379\n           volumeMounts:\n             - mountPath: \"/data\"\n               name: \"vol-data\"\n       volumes:\n         - name: vol-data\n           persistentVolumeClaim:\n             claimName: redis-data\n         - name: host-sys\n           hostPath:\n             path: /sys\n       restartPolicy: Always\n ---\n apiVersion: v1\n kind: Service\n metadata:\n   name: redis\n   labels:\n     app: redis\n spec:\n   ports:\n     - port: 6379\n       targetPort: 6379\n   selector:\n     app: redis\n</code></pre>  <p>Download the appropriate manifest for your infrastructure:</p> <ul> <li> <p>redis.aks.yml</p> </li> <li> <p>redis.eks.yml</p> </li> <li> <p>redis.gke.yml</p> </li> </ul> <p>Deploy to your cluster:</p> <pre>1</pre><pre><code>kubectl apply -f redis.*.yml\n</code></pre>  <p>Note</p> <p>You may also be able to use a managed Redis service such as Amazon Elasticache or Google Memorystore, however we do not explicitly support these services, and Tower is not guaranteed to work with them. Use at your own risk.</p> <p>If you do use an externally managed Redis service, make sure to update <code>configmap.yml</code> accordingly:</p> <pre>1</pre><pre><code>TOWER_REDIS_URL: redis://&lt;redis private IP&gt;:6379\n</code></pre>","title":"Redis"},{"location":"enterprise/kubernetes/#tower-cron-service","tags":["kubernetes","deployment"],"text":"tower-cron.yml <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60</pre><pre><code> apiVersion: apps/v1\n kind: Deployment\n metadata:\n   name: cron\n   labels:\n     app: cron\n spec:\n   selector:\n     matchLabels:\n       app: cron\n   template:\n     metadata:\n       labels:\n         app: cron\n     spec:\n       imagePullSecrets:\n         - name: \"cr.seqera.io\"\n       volumes:\n         - name: config-volume\n           configMap:\n             name: tower-yml\n       initContainers:\n         - name: migrate-db\n           image: cr.seqera.io/private/nf-tower-enterprise/backend:v23.1.0\n           command: [\"sh\", \"-c\", \"/migrate-db.sh\"]\n           envFrom:\n             - configMapRef:\n                 name: tower-backend-cfg\n           volumeMounts:\n             - name: config-volume\n               mountPath: /tower.yml\n               subPath: tower.yml\n       containers:\n         - name: backend\n           image: cr.seqera.io/private/nf-tower-enterprise/backend:v23.1.0\n           envFrom:\n             - configMapRef:\n                 name: tower-backend-cfg\n           volumeMounts:\n             - name: config-volume\n               mountPath: /tower.yml\n               subPath: tower.yml\n           env:\n             - name: MICRONAUT_ENVIRONMENTS\n               value: \"prod,redis,cron\"\n           ports:\n             - containerPort: 8080\n           readinessProbe:\n             httpGet:\n               path: /health\n               port: 8080\n             initialDelaySeconds: 5\n             timeoutSeconds: 3\n           livenessProbe:\n             httpGet:\n               path: /health\n               port: 8080\n             initialDelaySeconds: 5\n             timeoutSeconds: 3\n             failureThreshold: 10\n</code></pre>  <ol> <li> <p>Download the manifest:</p> </li> <li> <p>tower-cron.yml</p> </li> <li> <p>Deploy to your cluster:</p> </li> </ol> <pre>1</pre><pre><code>kubectl apply -f tower-cron.yml\n</code></pre>  <p>Wait for completion</p> <p>This container will create the required database schema the first time it is instantiated. This process can take a few minutes to complete and must be finished before you instantiate the Tower backend. Make sure this container is in the <code>READY</code> state before proceeding to the next step.</p>","title":"Tower cron service"},{"location":"enterprise/kubernetes/#tower-frontend-and-backend","tags":["kubernetes","deployment"],"text":"tower-svc.yml <pre>  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124</pre><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend\n  labels:\n    app: backend\nspec:\n  selector:\n    matchLabels:\n      app: backend\n  strategy:\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  template:\n    metadata:\n      labels:\n        app: backend\n    spec:\n      imagePullSecrets:\n        - name: \"cr.seqera.io\"\n      volumes:\n        - name: config-volume\n          configMap:\n            name: tower-yml\n      containers:\n        - name: backend\n          image: cr.seqera.io/private/nf-tower-enterprise/backend:v23.1.0\n          envFrom:\n            - configMapRef:\n                name: tower-backend-cfg\n          env:\n            - name: MICRONAUT_ENVIRONMENTS\n              value: \"prod,redis,ha\"\n          ports:\n            - containerPort: 8080\n          volumeMounts:\n            - name: config-volume\n              mountPath: /tower.yml\n              subPath: tower.yml\n          resources:\n            requests:\n              cpu: \"1\"\n              memory: \"1200Mi\"\n            limits:\n              memory: \"4200Mi\"\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 5\n            timeoutSeconds: 3\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 5\n            timeoutSeconds: 3\n            failureThreshold: 10\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  labels:\n    app: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      imagePullSecrets:\n        - name: \"cr.seqera.io\"\n      containers:\n        - name: frontend\n          image: cr.seqera.io/private/nf-tower-enterprise/frontend:v23.1.0\n          ports:\n            - containerPort: 80\n      restartPolicy: Always\n---\n# Services\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend\n  labels:\n    app: backend\nspec:\n  ports:\n    - name: http\n      port: 8080\n      targetPort: 8080\n  selector:\n    app: backend\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend-api\nspec:\n  ports:\n    - port: 8080\n      targetPort: 8080\n      protocol: TCP\n  type: NodePort\n  selector:\n    app: backend\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\nspec:\n  type: LoadBalancer\n  ports:\n    - port: 80\n  selector:\n    app: \"frontend\"\n---\n</code></pre>  <p>Download the manifest:</p> <ul> <li>tower-svc.yml</li> </ul> <p>Deploy to your cluster: <pre>1</pre><pre><code>kubectl apply -f tower-svc.yml\n</code></pre></p>","title":"Tower frontend and backend"},{"location":"enterprise/kubernetes/#tower-ingress","tags":["kubernetes","deployment"],"text":"<p>An ingress is used to make Tower publicly accessible, load balance traffic, terminate SSL/TLS, and offer name-based virtual hosting. The included ingress will create an external IP address and forward HTTP traffic to the Tower frontend.</p>  ingress.aks.yml <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15</pre><pre><code> apiVersion: networking.k8s.io/v1\n kind: Ingress\n metadata:\n   name: front-ingress\n   annotations:\n     kubernetes.io/ingress.class: azure/application-gateway\n spec:\n   rules:\n     - host: YOUR-TOWER-HOST-NAME\n       http:\n         paths:\n           - path: /*\n             backend:\n               serviceName: frontend\n               servicePort: 80\n</code></pre>   ingress.eks.yml <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30</pre><pre><code> apiVersion: networking.k8s.io/v1\n kind: Ingress\n metadata:\n   name: front-ingress\n   annotations:\n     kubernetes.io/ingress.class: alb\n     alb.ingress.kubernetes.io/scheme: internet-facing\n     alb.ingress.kubernetes.io/certificate-arn: YOUR-CERTIFICATE-ARN\n     alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\":443}]'\n     alb.ingress.kubernetes.io/actions.ssl-redirect: '{\"Type\": \"redirect\", \"RedirectConfig\": { \"Protocol\": \"HTTPS\", \"Port\": \"443\", \"StatusCode\": \"HTTP_301\"}}'\n     alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS-1-2-Ext-2018-06\n     alb.ingress.kubernetes.io/load-balancer-attributes: &gt;\n       idle_timeout.timeout_seconds=301,\n       routing.http2.enabled=false,\n       access_logs.s3.enabled=true,\n       access_logs.s3.bucket=YOUR-LOGS-S3-BUCKET,\n       access_logs.s3.prefix=YOUR-LOGS-PREFIX\n spec:\n   rules:\n     - host: YOUR-TOWER-HOST-NAME\n       http:\n         paths:\n           - path: /*\n             backend:\n               serviceName: ssl-redirect\n               servicePort: use-annotation\n           - path: /*\n             backend:\n               serviceName: frontend\n               servicePort: 80\n</code></pre>   ingress.gke.yml <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18</pre><pre><code> apiVersion: networking.k8s.io/v1\n kind: Ingress\n metadata:\n   name: front-ingress\n   annotations:\n     kubernetes.io/ingress.class: \"gce\"\n spec:\n   rules:\n     - host: YOUR-TOWER-HOST-NAME\n       http:\n         paths:\n           - path: /*\n             pathType: ImplementationSpecific\n             backend:\n               service:\n                 name: frontend\n                 port:\n                   number: 80\n</code></pre>  <p>Download the appropriate manifest and configure it according to your infrastructure:</p> <ul> <li> <p>ingress.aks.yml</p> </li> <li> <p>ingress.eks.yml</p> </li> <li> <p>ingress.gke.yml</p> </li> </ul> <p>Deploy to your cluster:</p> <pre>1</pre><pre><code>kubectl apply -f ingress.*.yml\n</code></pre> <p>See the Kubernetes documentation on Ingress for more information. If you don't need to make Tower externally accessible, you can also use a NodePort or a LoadBalancer service to make it accessible within your intranet.</p> <p>Additionally, see the relevant documentation for configuring an Ingress on each cloud provider:</p> <ul> <li>Amazon</li> <li>Azure</li> <li>Google Cloud</li> </ul>","title":"Tower ingress"},{"location":"enterprise/kubernetes/#check-status","tags":["kubernetes","deployment"],"text":"<p>Finally, make sure that all services are up and running:</p> <pre>1</pre><pre><code>kubectl get pods\n</code></pre>","title":"Check status"},{"location":"enterprise/kubernetes/#test-the-application","tags":["kubernetes","deployment"],"text":"<p>To make sure that Tower is properly configured, follow these steps:</p> <ol> <li> <p>Log in to Tower.</p> </li> <li> <p>Create an organization.</p> </li> <li> <p>Create a workspace within that organization.</p> </li> <li> <p>Create a new Compute Environment. Refer to Compute Environments for detailed instructions.</p> </li> <li> <p>Select Quick Launch from the Launchpad tab in your workspace.</p> </li> <li> <p>Enter the repository URL for the <code>nf-core/rnaseq</code> pipeline (<code>https://github.com/nf-core/rnaseq</code>).</p> </li> <li> <p>In the Config profiles dropdown, select the <code>test</code> profile.</p> </li> <li> <p>In the Pipeline parameters textarea, change the output directory to a sensible location based on your Compute Environment:</p> <pre>1\n2\n3\n4\n5</pre><pre><code># save to S3 bucket\noutdir: s3://&lt;your-bucket&gt;/results\n\n# save to scratch directory (Kubernetes)\noutdir: /scratch/results\n</code></pre> </li> <li> <p>Select Launch.</p> <p>You'll be transitioned to the Runs tab for the workflow. After a few minutes, you'll see the progress logs in the Execution log tab for that workflow.</p> </li> </ol>","title":"Test the application"},{"location":"enterprise/kubernetes/#optional-addons","tags":["kubernetes","deployment"],"text":"","title":"Optional addons"},{"location":"enterprise/kubernetes/#database-console","tags":["kubernetes","deployment"],"text":"dbconsole.yml <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34</pre><pre><code> apiVersion: apps/v1\n kind: Deployment\n metadata:\n   name: dbconsole\n   labels:\n     app: dbconsole\n spec:\n   selector:\n     matchLabels:\n       app: dbconsole\n   template:\n     metadata:\n       labels:\n         app: dbconsole\n     spec:\n       containers:\n         - image: adminer:4.7.7\n           name: dbconsole\n           ports:\n             - containerPort: 8080\n       restartPolicy: Always\n ---\n apiVersion: v1\n kind: Service\n metadata:\n   name: dbconsole\n spec:\n   ports:\n     - port: 8080\n       targetPort: 8080\n       protocol: TCP\n   type: NodePort\n   selector:\n     app: dbconsole\n</code></pre>  <p>The included dbconsole.yml can be used to deploy a simple web frontend to the Tower database. It is not required but it can be useful for administrative purposes.</p> <ol> <li> <p>Deploy the database console:</p> <pre>1</pre><pre><code>kubectl apply -f dbconsole.yml\n</code></pre> </li> <li> <p>Port-forward the database console to your local machine:</p> <pre>1</pre><pre><code>kubectl port-forward deployment/dbconsole 8080:8080\n</code></pre> <p>The database console will be available in your browser at <code>http://localhost:8080</code>.</p> </li> </ol>","title":"Database console"},{"location":"enterprise/kubernetes/#high-availability","tags":["kubernetes","deployment"],"text":"<p>When configuring Tower for high availability, it should be noted that:</p> <ul> <li> <p>The <code>cron</code> service may only have a single instance</p> </li> <li> <p>The <code>backend</code> service can be run in multiple replicas</p> </li> <li> <p>The <code>frontend</code> service is replicable, however in most scenarios it is not necessary</p> </li> </ul>","title":"High availability"},{"location":"enterprise/_templates/nginx/cert_on_frontend/","text":"<p>This example assumes deployment on an Amazon Linux 2 AMI.</p> <ol> <li> <p>Install nginx and other required packages:</p> <pre>1\n2\n3\n4\n5\n6</pre><pre><code>sudo amazon-linux-extras install nginx1.12\nsudo wget -r --no-parent -A 'epel-release-*.rpm' https://dl.fedoraproject.org/pub/epel/7/x86_64/Packages/e/\nsudo rpm -Uvh dl.fedoraproject.org/pub/epel/7/x86_64/Packages/e/epel-release-*.rpm\nsudo yum-config-manager --enable epel*\nsudo yum repolist all\nsudo amazon-linux-extras install epel -y\n</code></pre> </li> <li> <p>Generate a private certificate and key.</p> </li> <li> <p>Create a <code>ssl.conf</code> file.</p> <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31</pre><pre><code>server {\n    server_name your.server.name; # replace with your server name\n    root        /usr/share/nginx/html;\n\n    location / {\n    proxy_set_header          Host $host;\n    proxy_set_header          X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header          X-Real-IP $remote_addr;\n    proxy_set_header          X-Forwarded-Proto $scheme;\n\n    proxy_set_header          Authorization $http_authorization;\n    proxy_pass_header         Authorization;\n\n    proxy_pass                http://frontend/;\n    proxy_read_timeout        90;\n    proxy_redirect            http://frontend/ https://your.redirect.url/;\n    }\n\n        error_page 404 /404.html;\n            location = /40x.html {\n        }\n\n        error_page 500 502 503 504 /50x.html;\n            location = /50x.html {\n        }\n        listen [::]:443 ssl ipv6only=on;\n        listen 443 ssl;\n\n        ssl_certificate /etc/ssl/testcrt.crt;\n        ssl_certificate_key /etc/ssl/testkey.key;\n}\n</code></pre> </li> <li> <p>Make a local copy of the <code>frontend</code> container's <code>/etc/nginx/nginx.conf</code> file.</p> </li> <li> <p>Add the following to the <code>server</code> block of your local <code>nginx.conf</code> file:</p> <pre>1</pre><pre><code>include /etc/nginx/ssl.conf;\n</code></pre> </li> <li> <p>Modify the <code>frontend</code> container definition in your <code>docker-compose.yml</code> file:</p> <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15</pre><pre><code>frontend:\nimage: cr.seqera.io/frontend:${TAG}\nnetworks:\n    - frontend\nports:\n    - 8000:80\n    - 443:443\nvolumes:\n    - $PWD/nginx.conf:/etc/nginx/nginx.conf\n    - $PWD/ssl.conf:/etc/nginx/ssl.conf\n    - $PWD/cert/testcrt.crt:/etc/ssl/testcrt.crt\n    - $PWD/cert/testkey.key:/etc/ssl/testkey.key\nrestart: always\ndepends_on:\n    - backend\n</code></pre> </li> </ol>","title":"Cert on frontend"},{"location":"enterprise/advanced-topics/cloudformation/","tags":["ecs","cloudformation","deployment"],"text":"<p>Deprecated</p> <p>This deployment option is deprecated, and will be removed in the future. We strongly recommend against using this option unless you are sufficiently experienced with CloudFormation to customize this template for your own infrastructure.</p>   <p>Tower can be deployed via AWS CloudFormation, using the included configuration.</p> <p>This guide assumes that all prerequisites have been met.</p>","title":"Deploy to AWS CloudFormation"},{"location":"enterprise/advanced-topics/cloudformation/#setup-ecs-cluster","tags":["ecs","cloudformation","deployment"],"text":"<ol> <li> <p>Navigate to the ECS console in AWS.</p> </li> <li> <p>Select Create cluster.</p> </li> <li> <p>Select Amazon ECS -&gt; Clusters -&gt; EC2 Linux + Networking.</p> </li> </ol>","title":"Setup ECS cluster"},{"location":"enterprise/advanced-topics/cloudformation/#ecs-cluster-configuration","tags":["ecs","cloudformation","deployment"],"text":"<ul> <li>Name: nf-tower</li> </ul> <p>Instance Configuration</p> <ul> <li>Provisioning Model: On-demand</li> <li>EC2 instance type: c4.2xlarge</li> <li>Number of instances: 1</li> <li>EC2 AMI id: Amazon Linux 2</li> <li>Root EBS Volume Size (GiB): none</li> <li>Key pair: none</li> </ul>","title":"ECS Cluster Configuration"},{"location":"enterprise/advanced-topics/cloudformation/#networking-configuration","tags":["ecs","cloudformation","deployment"],"text":"<ul> <li>Create new VPC</li> </ul> <p>Container instance IAM role</p> <ul> <li>Create new role (if <code>ecsInstance</code> role doesn't exist)</li> </ul> <p>Obtain Instance <code>ServerURL</code></p> <ul> <li>Record the Public IP of the instance in the ECS cluster e.g. <code>3.122.246.202</code></li> </ul>","title":"Networking Configuration"},{"location":"enterprise/advanced-topics/cloudformation/#deploy-tower","tags":["ecs","cloudformation","deployment"],"text":"Click to view aws-ecs-cloudformation.json <pre>1</pre><pre><code>\n</code></pre>   Click to view params.json.template  <pre>1</pre><pre><code>\n</code></pre>    <ol> <li> <p>Download aws-ecs-cloudformation.json and params.json.template.</p> </li> <li> <p>Rename <code>params.template.json</code> to <code>params.json</code> and configure for your environment.</p> <p>For more information on configuration, visit the Configuration section.</p> </li> <li> <p>Deploy the Tower stack to your ECS cluster:</p> <pre>1\n2\n3\n4</pre><pre><code>aws cloudformation create-stack \\\n    --stack-name Tower \\\n    --template-body file://aws-ecs-cloudformation.json \\\n    --parameters file://params.json\n</code></pre> <p>You can delete the stack at any time, to uninstall Tower or update any parameters:</p> <pre>1\n2</pre><pre><code>aws cloudformation delete-stack \\\n    --stack-name Tower\n</code></pre> </li> </ol>","title":"Deploy Tower"},{"location":"enterprise/advanced-topics/custom-launch-container/","tags":["aws","batch","launch","container"],"text":"<p>Note</p> <p>This feature requires Tower 20.10.2 or later.</p>   <p>Tower automatically registers an AWS Batch job definition to launch pipelines with the required Nextflow runtime.</p> <p>If you need to manage this manually, create a job definition in your AWS Batch environment using the following settings:</p> <ul> <li>name: any of your choice</li> <li>image: a custom image based on the Seqera Labs nf-launcher image</li> <li>vcpus: at least 1</li> <li>memory: at least 1000</li> <li>command: <code>true</code></li> </ul> <p>Once the job definition is registered, update your Tower Enterprise configuration with the following (replace <code>YOUR_JOB_DEFINITION_NAME</code> with the name of the job definition):</p>  <pre>1</pre><pre><code>TOWER_LAUNCH_CONTAINER=job-definition://YOUR_JOB_DEFINITION_NAME\n</code></pre>  <p>Note: The repository where your launch container resides must be accessible to the Batch cluster's ECS Agent.</p>","title":"Use custom launch container"},{"location":"enterprise/advanced-topics/db-docker-to-RDS/","tags":["rds","docker","database"],"text":"<p>While Docker Compose is a fast and convenient way to deploy your Tower instance, production deployments should have a robust database solution to minimize the risk of data loss.</p>","title":"Migrate Docker database to RDS"},{"location":"enterprise/advanced-topics/db-docker-to-RDS/#points-to-consider-before-migration","tags":["rds","docker","database"],"text":"<ol> <li> <p>Target Database</p> <p>You have options when choosing your new MySQL-compliant database. While the process is mostly the same, some of the commands will be different (example: MariaDB on RDS).</p> </li> <li> <p>How much data must be moved?</p> <p>The data in your database must be exported from the MySQL container and imported to the new instance. Depending on the amount of data in your database and the amount of remaining EC2 EBS capacity, you may be able to save your data directly onto the instance, or be forced to make use of a service with more capacity (such as S3).</p> </li> <li> <p>Testing</p> <p>What level of testing is required to determine the data has been properly migrated?</p> </li> <li> <p>Maintenance window</p> <p>It is much simpler to initiate a migration once all transactions to the database cease than to do so while jobs are still running. Perform the migration at a time when an outage can occur and notify your users in advance.</p> </li> <li> <p>MySQL container volume retention</p> <p>Seqera highly suggests retaining your original volume until you are 100% satisfied that the migration occurred without error. So long as the volume is kept, you can fall back to the MySQL container and ensure that Kinnate does not lose any of the material generated thus far.</p> </li> </ol>","title":"Points to consider before migration"},{"location":"enterprise/advanced-topics/db-docker-to-RDS/#prerequisite-work","tags":["rds","docker","database"],"text":"<p>Before starting your migration, do the following:</p> <ol> <li> <p>Create an RDS MySQL-compliant instance and populate it with a tower user and tower database.</p> </li> <li> <p>Ensure your EC2 instance and database instance's Security Group(s) have been configured to allow MySQL traffic (default: Port 3306).</p> </li> </ol>","title":"Prerequisite work"},{"location":"enterprise/advanced-topics/db-docker-to-RDS/#steps","tags":["rds","docker","database"],"text":"<p>Note</p> <p>These steps assume the following:</p> <ol> <li>You have sufficient EC2 instance space to store your data without requiring an S3 mounting solution like S3fs.</li> <li>You have implemented a full maintenance outage.</li> </ol>   <ol> <li> <p>Connect to to the EC2 instance using SSH and navigate to Tower's docker-compose folder.</p> </li> <li> <p>Shut down Tower:</p> </li> </ol>  <pre>1\n2\n3</pre><pre><code>```bash\ndocker-compose down\n```\n</code></pre>  <ol> <li>Mount a new folder into the MySQL container for migration activities:<ol> <li>Create a new folder to hold migration artefacts.</li> </ol> </li> </ol>  <pre>1\n2\n3</pre><pre><code>    ```bash\n    mkdir ~/tower_migration\n    ```\n</code></pre>  <pre>1</pre><pre><code>2. Backup the database.\n</code></pre>  <pre>1\n2\n3</pre><pre><code>    ```bash\n    sudo tar -zcvf ~/tower_migration/tower_backup.tar.gz ~/.tower/db/mysql\n    ```\n</code></pre>  <ol> <li>Modify the Tower docker-compose.yml:<ol> <li>Add new volume entry for the db container.</li> </ol> </li> </ol>  <pre>1\n2\n3</pre><pre><code>```\n$HOME/tower_migration:/tower_migration\n```\n</code></pre>  <ol> <li>Restart Tower to ensure your changes were successful.</li> </ol>  <pre>1\n2\n3</pre><pre><code>```bash\ndocker-compose down\n```\n</code></pre>  <ol> <li>Stop all the non-MySQL containers.</li> </ol>  <pre>1\n2\n3</pre><pre><code>```bash\ndocker container stop &lt;CONTAINER ID&gt;\n```\n</code></pre>  <ol> <li>Exec onto the MySQL ontainer.</li> </ol>  <pre>1\n2\n3</pre><pre><code>```bash\ndocker exec -it &lt;CONTAINER ID&gt; /bin/bash\n```\n</code></pre>  <pre>1</pre><pre><code>1. Dump your data to disk.\n</code></pre>  <pre>1\n2\n3</pre><pre><code>    ```bash\n    mysqldump -u tower -p --databases tower --no-tablespaces --set-gtid-purged=OFF &gt; /tower_migration/tower_dumpfile.sql\n    ```\n</code></pre>  <pre>1</pre><pre><code>2. Exit the container.\n</code></pre> <ol> <li>Stop the MySQL container.</li> </ol>  <pre>1\n2\n3</pre><pre><code>```bash\ndocker container stop &lt;CONTAINER ID&gt;\n```\n</code></pre>  <ol> <li>Import the dump file into your new RDS instance.</li> </ol>  <pre>1\n2\n3</pre><pre><code>```bash\nmysql --host &lt;DB-HOST&gt; --port 3306 -u tower -p tower &lt; ~/tower_migration/tower_dumpfile.sql\n```\n</code></pre>  <ol> <li> <p>Log on to the RDS instance and ensure that the tower database is populated with tables prefixed by 'tw_'.</p> </li> <li> <p>Modify the tower.env in the Tower docker folder:</p> <ol> <li>Comment out the existing TOWER_DB-* variables.</li> <li>Add new entries relevant to your database choice.</li> <li>Save and exit.</li> </ol> </li> <li> <p>Restart Tower.</p> </li> </ol>  <pre>1\n2\n3</pre><pre><code>```bash\ndocker-compose up\n```\n</code></pre>  <ol> <li>Confirm that Tower starts and that your data is available when you log in.</li> </ol> <p>Congratulations! Your migration is complete and your testing can begin.</p>","title":"Steps"},{"location":"enterprise/advanced-topics/manual-aws-batch-setup/","tags":["aws","batch","configuration"],"text":"<p>This document describes how to set up AWS roles and Batch queues manually for the deployment of Nextflow workloads using Tower Enterprise.</p>   <p>Tip</p> <p>These steps are only needed if you want to create the AWS Batch resources manually. If you use the Tower Forge option, these steps are not needed, because Tower will automatically create the required Batch queues.</p>","title":"Configure AWS Batch manually"},{"location":"enterprise/advanced-topics/manual-aws-batch-setup/#create-a-user-policy","tags":["aws","batch","configuration"],"text":"<p>Create the policy for the user launching Nextflow jobs:</p> <ul> <li>Go to the IAM Console</li> <li>Go to the Policy page</li> <li>Create a new policy with the following content:</li> </ul>  <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25</pre><pre><code>```json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Stmt1530313170000\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"batch:CancelJob\",\n                \"batch:RegisterJobDefinition\",\n                \"batch:DescribeComputeEnvironments\",\n                \"batch:DescribeJobDefinitions\",\n                \"batch:DescribeJobQueues\",\n                \"batch:DescribeJobs\",\n                \"batch:ListJobs\",\n                \"batch:SubmitJob\",\n                \"batch:TerminateJob\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ]\n}\n```\n</code></pre>  <ul> <li>Save with it the name <code>nf-tower-user</code>.</li> </ul>","title":"Create a user policy"},{"location":"enterprise/advanced-topics/manual-aws-batch-setup/#create-the-instance-role-policy","tags":["aws","batch","configuration"],"text":"<ul> <li>Go to the IAM Console</li> <li>Go to the Policy page</li> <li>Create a new policy with the following content:</li> </ul>  <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37</pre><pre><code>```json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"batch:DescribeJobQueues\",\n                \"batch:CancelJob\",\n                \"batch:SubmitJob\",\n                \"batch:ListJobs\",\n                \"batch:DescribeComputeEnvironments\",\n                \"batch:TerminateJob\",\n                \"batch:DescribeJobs\",\n                \"batch:RegisterJobDefinition\",\n                \"batch:DescribeJobDefinitions\",\n                \"ecs:DescribeTasks\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeInstanceTypes\",\n                \"ec2:DescribeInstanceAttribute\",\n                \"ecs:DescribeContainerInstances\",\n                \"ec2:DescribeInstanceStatus\",\n                \"logs:Describe*\",\n                \"logs:Get*\",\n                \"logs:List*\",\n                \"logs:StartQuery\",\n                \"logs:StopQuery\",\n                \"logs:TestMetricFilter\",\n                \"logs:FilterLogEvents\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n```\n</code></pre>  <ul> <li>Finally, save it with the name <code>nf-tower-batchjob</code>.</li> </ul>","title":"Create the instance role policy"},{"location":"enterprise/advanced-topics/manual-aws-batch-setup/#create-the-batch-service-role","tags":["aws","batch","configuration"],"text":"<p>This is a role used by AWS Batch to launch EC2 instances on your behalf.</p> <ul> <li>Go to the IAM Console</li> <li>Click on \"Roles\"</li> <li>Click on \"Create role\"</li> <li>Select \"AWS service\" as the trusted entity</li> <li>Choose \"Batch\" as the service to use the role</li> <li>Click \"Next: Permissions\"</li> <li>In Attached permissions policies, the <code>AWSBatchServiceRole</code> will already be attached</li> <li>Click \"Next: Tags\" (adding tags is optional)</li> <li>Click \"Next: Review\"</li> <li>Set the Role Name to <code>nf-tower-servicerole</code></li> <li>Finally, Click \"Create role\"</li> </ul>","title":"Create the Batch Service Role"},{"location":"enterprise/advanced-topics/manual-aws-batch-setup/#create-an-ec2-instance-role","tags":["aws","batch","configuration"],"text":"<p>This is a role that controls which AWS Resources EC2 instances launched by AWS Batch have access to. In this case, you will limit S3 access to just the bucket you created earlier.</p> <ul> <li>Go to the IAM Console</li> <li>Click on \"Roles\"</li> <li>Click on \"Create role\"</li> <li>Select \"AWS service\" as the trusted entity</li> <li>Choose EC2 from the larger services list</li> <li>Choose \"EC2 - Allows EC2 instances to call AWS services on your behalf\" as the use case.</li> <li> <p>Click \"Next: Permissions\"</p> <ul> <li>Type \"ContainerService\" in the search field for policies</li> <li>Click the checkbox next to <code>AmazonEC2ContainerServiceforEC2Role</code> to attach the policy</li> <li>Type \"S3\" in the search field for policies</li> <li>Click the checkbox next to <code>AmazonS3FullAccess</code> to attach the policy (you may want to use to use a custom policy to allow the access only on specific S3 buckets)</li> </ul> </li> <li> <p>Search and attach the custom policy <code>nf-tower-batchjob</code></p> </li> <li>Click \"Next: Tags\". (adding tags is optional)</li> <li>Click \"Next: Review\"</li> <li>Set the Role Name to <code>nf-tower-instancerole</code></li> <li>Finally, Click \"Create role\"</li> </ul>","title":"Create an EC2 Instance Role"},{"location":"enterprise/advanced-topics/manual-aws-batch-setup/#create-an-ec2-spotfleet-role","tags":["aws","batch","configuration"],"text":"<p>This is a role that allows creation and launch of Spot fleets - Spot instances with similar compute capabilities (i.e. vCPUs and RAM). This is for using Spot instances when running jobs in AWS Batch.</p> <ul> <li>Go to the IAM Console</li> <li>Click on \"Roles\"</li> <li>Click on \"Create role\"</li> <li>Select \"AWS service\" as the trusted entity</li> <li>Choose EC2 from the larger services list</li> <li>Choose \"EC2 - Spot Fleet Tagging\" as the use case<ul> <li>In Attached permissions policies, the <code>AmazonEC2SpotFleetTaggingRole</code> will already be attached.</li> </ul> </li> <li>Click \"Next: Tags\". (adding tags is optional)</li> <li>Click \"Next: Review\"</li> <li>Set the Role Name to <code>nf-tower-fleetrole</code></li> <li>Click \"Create role\".</li> </ul>","title":"Create an EC2 SpotFleet Role"},{"location":"enterprise/advanced-topics/manual-aws-batch-setup/#create-a-launch-template","tags":["aws","batch","configuration"],"text":"<p>Required to configure the EC2 instance deployed by the Batch jobs.</p> <ul> <li>Go to the EC2 Console</li> <li>Click Launch template</li> <li>Create a new launch template which uses the <code>User Data</code> (in the Advanced details section) shown below:</li> </ul>  <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27</pre><pre><code>```bash\nMIME-Version: 1.0\nContent-Type: multipart/mixed; boundary=\"//\"\n\n--//\nContent-Type: text/x-shellscript; charset=\"us-ascii\"\n\n#!/bin/sh\nsu - root &lt;&lt; 'EOF'\n(\nset -x\n## install awscli\nUSER=/home/ec2-user\nexport PATH=/usr/local/bin:$PATH\nyum install -y jq python27-pip sed wget bzip2\npip install -U boto3\nwget -q https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $USER/miniconda\n$USER/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\nchown -R ec2-user:ec2-user $USER/miniconda\n) &amp;&gt;&gt; ~/boot.log\nEOF\ncp ~/boot.log ~ec2-user/boot.log\n\n--//--\n```\n</code></pre>  <ul> <li>Finally, save it with name <code>nf-tower-launchtemplate</code>.</li> </ul>","title":"Create a launch template"},{"location":"enterprise/advanced-topics/manual-aws-batch-setup/#create-the-batch-compute-environments","tags":["aws","batch","configuration"],"text":"<ul> <li>Go to the Batch Console</li> <li>Create a new compute environment specifying the Instance profile, Service role, Fleet role and     Launch template created previously.</li> <li>Save it with a name of your choice.</li> </ul>","title":"Create the Batch compute environments"},{"location":"enterprise/advanced-topics/manual-aws-batch-setup/#create-the-batch-queue","tags":["aws","batch","configuration"],"text":"<ul> <li>Go to the Batch Console</li> <li>Create a new queue</li> <li>Associate to the compute environment created in the previous step.</li> <li>Finally, save it with a name of your choice.</li> </ul>","title":"Create the Batch Queue"},{"location":"enterprise/advanced-topics/tower-container-images/","tags":["on-prem","prerequisites","configuration"],"text":"<p>Warning</p> <p>The Seqera Labs container registry <code>cr.seqera.io</code> is the default Tower container image registry from version 22.4. Use of the AWS, Azure, and Google Cloud Tower image registries (referenced below) in existing installations is still supported but will be deprecated for new installations starting June 2023.</p>   Legacy Tower image instructions AWSAzureGoogle Cloud   <p>Seqera Labs publishes the Tower Enterprise containers to a private Elastic Container Registry (ECR) on AWS.</p> <ol> <li> <p>Provide Seqera Labs with your AWS Account ID.</p> <p>Supply this value to the Seqera representative managing your onboarding and wait for confirmation that it has been added to the ECR repository policy as an approved Principal.</p> </li> <li> <p>Retrieve a local copy of the container.</p> <p>Clients using the docker-compose deployment method must retrieve container copies for local use.</p> <ol> <li> <p>Install AWS CLI on the target machine.</p> </li> <li> <p>Configure the AWS CLI with an IAM User with at least these privileges:</p> <pre>1\n2\n3</pre><pre><code>ecr:BatchGetImage\necr:GetAuthorizationToken\necr:GetDownloadUrlForLayer  \n</code></pre> </li> <li> <p>Authenticate Docker against the Seqera ECR:</p> <pre>1\n2\n3\n4\n5\n6</pre><pre><code># AWS CLI v2\naws ecr get-login-password --region eu-west-1 | \\\ndocker login --username AWS --password-stdin 195996028523.dkr.ecr.eu-west-1.amazonaws.com\n\n# AWS CLI v1\n$(aws ecr get-login --registry-ids 195996028523 --region eu-west-1 --no-include-email)\n</code></pre> </li> <li> <p>Pull the containers to your machine:</p> <pre>1\n2\n3\n4\n5</pre><pre><code>export REPOSITORY_URL=\"195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise\"\nexport TAG=\"v22.3.1\"\n\ndocker pull ${REPOSITORY_URL}/backend:${TAG}\ndocker pull ${REPOSITORY_URL}/frontend:${TAG}\n</code></pre> </li> </ol> </li> </ol>   <p>Seqera Labs publishes the Tower Enterprise containers to a private Azure Container Registry instance.</p> <ol> <li> <p>Acquire credentials from Seqera Labs.</p> <p>Customers who chose to retrieve their Tower Enterprise containers from Seqera's Azure Container Registry will be supplied with a user id and authentication token during the onboarding process.</p> </li> <li> <p>Retrieve a local copy of the container.</p> <p>Clients using the docker-compose deployment method must retrieve container copies for local use.</p> <p>a. Authenticate Docker against the Seqera Azure Container Registry:</p> <pre>1\n2</pre><pre><code># Replace USER and TOKEN with the credentials supplied by Seqera Labs\ndocker login -u USER -p TOKEN seqera.azurecr.io\n</code></pre> <p>b. Pull the containers to your local instance:</p> <pre>1\n2\n3\n4\n5</pre><pre><code>export REPOSITORY_URL=\"seqera.azurecr.io/nf-tower-enterprise\"\nexport TAG=\"v22.3.1\"\n\ndocker pull ${REPOSITORY_URL}/backend:${TAG}\ndocker pull ${REPOSITORY_URL}/frontend:${TAG}\n</code></pre> </li> </ol>   <p>Seqera Labs publishes the Tower Enterprise containers to a private Artifact Registry (AR) on GCP.</p> <ol> <li> <p>Provide Seqera Labs with your GCP Service Account.</p> <p>Supply your GCP Project's Service Account email address to the Seqera representative managing your onboarding and wait for confirmation that it has been added as an approved Artifact Registry Reader.</p> </li> <li> <p>Retrieve a local copy of the container.</p> <p>Clients using the docker-compose deployment method must retrieve container copies for local use.</p> <p>a. Install gcloud CLI and Docker on the target machine.</p> <p>b. Authenticate the Service Account with the gcloud CLI.</p> <p>c. Configure Docker to interact with the GCP Region where the Seqera AR resides:</p> <p><pre>1</pre><pre><code>gcloud auth configure-docker europe-west2-docker.pkg.dev\n</code></pre> d. Confirm you have access to the repository:</p> <p><pre>1</pre><pre><code>gcloud artifacts docker images list europe-west2-docker.pkg.dev/nf-tower-enterprise/containers/ --include-tags\n</code></pre> e. Pull the containers to your machine:</p> <pre>1\n2\n3\n4\n5</pre><pre><code>export REPOSITORY_URL=\"europe-west2-docker.pkg.dev/nf-tower-enterprise/containers\"\nexport TAG=\"v22.3.1\"\n\ndocker pull ${REPOSITORY_URL}/backend:${TAG}\ndocker pull ${REPOSITORY_URL}/frontend:${TAG}\n</code></pre> </li> </ol>  <p>Warning</p> <p>If you are unable to pull container images due to a <code>denied: Permission \"artifactregistry.repositories.downloadArtifacts\" denied on resouce \"projects/nf-tower-enterprise/locations/eurpoe-west2/repositories/containers\" (or it may not exist)</code> error, try the following:</p> <ol> <li> <p>If your Docker requires sudo, add sudo to the <code>gcloud auth configure-docker europe-west2-docker.pkg.dev</code> command.</p> </li> <li> <p>If you installed Docker onto Ubuntu using <code>snap</code>, ensure your containerd config is properly updated.</p> </li> </ol>","title":"Legacy Tower container images"},{"location":"enterprise/advanced-topics/use-iam-role/","tags":["aws","iam","role"],"text":"<p>Note</p> <p>This feature requires Tower 21.06.0 or later.</p>   <p>AWS-based customers can configure Tower to interact with other AWS Services like Batch using an IAM Role rather than providing IAM User credentials.</p>","title":"Use IAM role instead of user credentials"},{"location":"enterprise/advanced-topics/use-iam-role/#configure-the-tower-iam-policy","tags":["aws","iam","role"],"text":"<p>Assumptions in Provided Policies</p> <p>The provided policies were designed with certain assumptions:</p> <ol> <li>IAM Policy: Tower and Nextflow should have whole access to identified S3 Buckets.</li> <li>Trust Policy: The Role should be assumable by EC2, ECS, EKS, and only specifically-named IAM actors.</li> </ol> <p>You may wish to limit S3 access to specific prefixes, and/or Role assumption to more specific Platforms.</p>   <p>Create a custom IAM Policy (Tower-Role-Policy.json).</p>   Click to view custom Tower-Role-Policy.json <pre>1</pre><pre><code>\n</code></pre>   <ol> <li>Modify <code>BucketPolicy01</code> and <code>BucketPolicy02</code> with the name(s) of the your S3 Buckets.</li> <li>Revise (if necessary) the scope of access to a specific prefix in the S3 bucket(s).</li> </ol>","title":"Configure the Tower IAM policy"},{"location":"enterprise/advanced-topics/use-iam-role/#modify-the-tower-iam-role-trust-policy-optional","tags":["aws","iam","role"],"text":"<p>Review and modify the Role Trust Policy (Tower-Role-Trust-Policy.json).</p>   Click to view Tower-Role-Trust-Policy <pre>1</pre><pre><code>\n</code></pre>   <ol> <li> <p>Replace <code>YOUR-AWS-ACCOUNT</code> with your own AWS Account Id.</p> </li> <li> <p>Specify the Users and/or Roles able to assume the Tower IAM Role.</p> </li> </ol>","title":"Modify the Tower IAM Role Trust Policy (optional)"},{"location":"enterprise/advanced-topics/use-iam-role/#create-the-iam-artefacts","tags":["aws","iam","role"],"text":"<p>Create the IAM Artefacts in your AWS Account.</p> <ol> <li>Navigate to the folder containing your configured IAM documents:</li> </ol>  <pre>1\n2\n3</pre><pre><code>```bash\ncd &lt;FOLDER_WITH_YOUR_CONFIGURED_IAM_DOCUMENTS&gt;\n```\n</code></pre>  <ol> <li>Create the Role:</li> </ol>  <pre>1\n2\n3</pre><pre><code>```bash\naws iam create-role --role-name Tower-Role --assume-role-policy-document file://Tower-Role-Trust-Policy.json\n```\n</code></pre>  <ol> <li>Create an inline policy for the Role:</li> </ol>  <pre>1\n2\n3</pre><pre><code>```bash\naws iam put-role-policy --role-name Tower-Role --policy-name Tower-Role-Policy --policy-document file://Tower-Role-Policy.json\n```\n</code></pre>  <ol> <li>Create an instance profile:</li> </ol>  <pre>1\n2\n3</pre><pre><code>```bash\naws iam create-instance-profile --instance-profile-name Tower-Instance\n```\n</code></pre>  <ol> <li>Bind the Role to the instance profile:</li> </ol>  <pre>1\n2\n3</pre><pre><code>```bash\naws iam add-role-to-instance-profile --instance-profile-name Tower-Instance --role-name Tower-Role\n```\n</code></pre>","title":"Create the IAM artefacts"},{"location":"enterprise/advanced-topics/use-iam-role/#configure-tower","tags":["aws","iam","role"],"text":"<p>With the IAM artefacts complete, update your Tower application configuration:</p> <ol> <li>Add the following entry to your <code>tower.env</code></li> </ol>  <pre>1\n2\n3</pre><pre><code>```env\nTOWER_ALLOW_INSTANCE_CREDENTIALS=true\n```\n</code></pre>  <ol> <li> <p>Restart the Tower application.</p> </li> <li> <p>Verify that the change took effect by querying the Tower instance <code>service-info</code> endpoint:</p> </li> </ol>  <pre>1\n2\n3</pre><pre><code>```bash\ncurl -X GET \"https://YOUR-TOWER-DOMAIN/api/service-info\" -H \"Accept: application/json\" | jq \".serviceInfo.allowInstanceCredentials\"\n```\n</code></pre>  <ol> <li> <p>Log in to Tower and create a new AWS credential. You are now prompted for an AWS <code>arn</code> instead of access keys.</p> <p>Before Change: </p> <p>After Change: </p> </li> </ol>","title":"Configure Tower"},{"location":"enterprise/configuration/authentication/","tags":["authentication","configuration"],"text":"","title":"Authentication"},{"location":"enterprise/configuration/authentication/#openid-connect-related-variables","tags":["authentication","configuration"],"text":"<ul> <li><code>TOWER_OIDC_CLIENT</code>: The client ID provided by your authentication service.</li> <li><code>TOWER_OIDC_SECRET</code>: The client secret provided by your authentication service.</li> <li><code>TOWER_OIDC_ISSUER</code>: The authentication service URL to which Tower connects to authenticate the sign-in request e.g. <code>https://dev-886323.okta.com/oauth2/default</code>. </li> </ul> <p>Some providers require the full authentication service URL (such as the OKTA example above), while others require only the SSO root domain (without the trailing sub-directories).</p> <p>In your OpenID provider settings, specify the following URL as callback address or authorised redirect: </p> <pre>1</pre><pre><code>https://&lt;YOUR HOST OR IP&gt;/oauth/callback/oidc\n</code></pre>","title":"OpenID Connect related variables"},{"location":"enterprise/configuration/authentication/#okta-identity-provider","tags":["authentication","configuration"],"text":"<p>To setup Okta as the OpenID provider:</p> <ul> <li>Sign in to your Okta organization with your administrator account.</li> <li>From the Admin Console side navigation, click Applications &gt; Applications.</li> <li>Select Add Application.</li> <li>Select Create New App.</li> <li>Select the OpenID Connect sign-on method. </li> <li>Select Create.</li> <li>Enter a name for your new app integration e.g. <code>Tower</code>.</li> <li>In the Configure OpenID Connect, add the following redirect URIs. <ul> <li>Sign-in redirect URIs : <code>https://&lt;YOUR HOST OR IP&gt;/oauth/callback/oidc</code></li> <li>Sign-out redirect URIs : <code>https://&lt;YOUR HOST OR IP&gt;/logout</code></li> </ul> </li> <li>Select Save.</li> </ul> <p>Okta app automatically navigates to your new application settings. You can use these details to complete the Tower configuration by specifying the following variables:</p> <ul> <li><code>TOWER_OIDC_CLIENT</code> : Copy from Client ID field in the Client Credentials section within the General tab for the corresponding app client configuration.</li> <li><code>TOWER_OIDC_SECRET</code>: Copy from Client secret field in the Client Credentials section within the General tab for the corresponding app client configuration.</li> <li><code>TOWER_OIDC_ISSUER</code> : Copy the Okta issuer URL, in the OpenID Connect ID Token section in the Sign On tab for the corresponding app client configuration.</li> </ul> <p>Check the OpenID Connect section above for details.</p>","title":"Okta identity provider"},{"location":"enterprise/configuration/authentication/#github-identity-provider","tags":["authentication","configuration"],"text":"<p>To use GitHub as SSO provider for Tower, register your Tower instance as a GitHub OAuth App in your organization settings page, e.g., https://github.com/organizations/{YOUR-ORGANIZATION}/settings/applications.</p> <p>When creating the OAuth App specify the following path as callback URL: <code>https://{your-deployment-domain-name}/oauth/callback/github</code> (replacing the <code>{your-deployment-domain-name}</code> placeholder with the domain name of your deployment).</p> <p>Finally include the following variable in the backend environment configuration: </p> <ul> <li><code>TOWER_GITHUB_CLIENT</code>: The client id provided by GitHub when register the new OAuth App.</li> <li><code>TOWER_GITHUB_SECRET</code>: The client secret provided by GitHub when register the new OAuth App.</li> </ul>","title":"GitHub identity provider"},{"location":"enterprise/configuration/authentication/#google-identity-provider","tags":["authentication","configuration"],"text":"<p>To use Google as SSO provider for Tower: </p> <ul> <li>Visit https://console.developers.google.com and create a new project</li> <li>From the sidebar, click the Credentials tab</li> <li>Select Create credentials and choose OAuth client ID from the drop-down</li> <li>On the next page, select Web Application type</li> <li>Enter the redirect URL: <code>https://{your-deployment-domain-name}/oauth/callback/google</code> (replacing the <code>{your-deployment-domain-name}</code> placeholder with the domain name of your deployment).</li> <li>Confirm the operation. You will then receive a Client ID and secret ID. </li> </ul> <p>Finally, include the Client ID and Secret ID in following variables in the Tower backend environment configuration: </p> <ul> <li><code>TOWER_GOOGLE_CLIENT</code>: The client id provided by Google in the above steps.</li> <li><code>TOWER_GOOGLE_SECRET</code>: The client secret provided by Google in the above steps.</li> </ul>","title":"Google identity provider"},{"location":"enterprise/configuration/authentication/#keycloak-identity-provider","tags":["authentication","configuration"],"text":"<p>To use Keycloak as identity provider for Tower, configure in your Keycloak service a new client following these steps: </p> <ul> <li>In the Realm settings make sure the Endpoints field include \"OpenID Endpoint Configuration\"</li> <li>Open the Client page and click *Create\" to setup a new client for Tower</li> <li>In the Settings tag, make sure to include the following fields<ul> <li>Client Id: <code>tower</code> for the sake of this tutorial or any other Id of your choice</li> <li>Enabled: <code>ON</code></li> <li>Client Protocol: <code>openid-connect</code></li> <li>Access Type: <code>confidential</code></li> <li>Standard Flow Enabled: <code>ON</code></li> <li>Implicit Flow Enabled: <code>OFF</code></li> <li>Direct Access Grants Enabled: <code>ON</code></li> <li>Valid Redirect URIs: https:///oauth/callback/oidc e.g. <code>http://localhost:8000/oauth/callback/oidc</code> <li>Click Save</li>   <li>In the Credentials tab, take note of the Secret field.</li> <li>In the Keys tab, make sure the field Use JWKS URL is <code>OFF</code>.</li>  <p>Complete the setup on Tower side adding the following environment variables to your configuration:</p> <ul> <li><code>TOWER_OIDC_CLIENT</code>: The client Id assigned to the above client setup e.g. <code>tower</code>.</li> <li><code>TOWER_OIDC_SECRET</code>: The content of the Secret field assigned in the above client setup.</li> <li><code>TOWER_OIDC_ISSUER</code>: The Keycloak issuer URL, you can find it in the Realm Settings page and    clicking on the OpenID Configuration in the Endpoints field. It shows a JSON payload, copy &amp; paste the value   associated to the entry <code>issues</code>, e.g. <code>http://localhost:9000/auth/realms/master</code>.</li> </ul>","title":"Keycloak identity provider"},{"location":"enterprise/configuration/authentication/#azure-ad-oidc-integration","tags":["authentication","configuration"],"text":"<p>To make use of Azure AD for the OIDC as identity provider for Tower, configure a new client in your Azure AD service:</p> <ol> <li>Log in to the Azure portal.</li> <li>Navigate to the Azure Active Directory service.</li> <li>Select Manage Tenants.</li> <li>Create a new Tenant (e.g. <code>NextflowTowerOrg</code>).</li> <li>Navigate to the newly-created Tenant.</li> <li>Go to App Registrations.</li> <li> <p>Click New Registration.</p> <ol> <li>Enter a name for the application.</li> <li>Specify the scope of user verification (e.g. single tenant, multi-tenant, personal MSFT accounts, etc).</li> </ol>  <p>Note</p> <p>The Azure AD app must have user consent settings configured to \"Allow user consent for apps\" to ensure that admin approval is not required for each application login. See User consent settings.</p>  </li> <li> <p>Specify the Redirect (callback) URI (NOTE: Microsoft requires that this URI uses <code>HTTPS</code>)</p> </li> <li> <p>Open the newly-created app:</p> <ol> <li>Note the Application (client) ID under the Essentials table</li> <li>Generate Client credentials under the Essentials table</li> <li>Click Endpoints and note the OpenID Connect metadata document URI</li> </ol> </li> <li> <p>Add users to your tenant as required.</p> </li> <li> <p>Complete the setup on Tower side adding the following environment variables to your configuration:</p> <pre>1\n2\n3</pre><pre><code>TOWER_OIDC_CLIENT=&lt;YOUR_APPLICATION_ID&gt;\nTOWER_OIDC_SECRET=&lt;YOUR_CLIENT_CREDENTIALS_SECRET&gt;\nTOWER_OIDC_ISSUER=&lt;YOUR_OIDC_METADATA_URL_UP_TO_\"v2.0\"&gt;   (e.g. https://login.microsoftonline.com/000000-0000-0000-00-0000000000000/v2.0)\n</code></pre> </li> <li> <p>Add  <code>auth-oidc</code> to the end of the <code>MICRONAUT_ENVIRONMENTS</code> value for both the <code>cron</code> and <code>backend</code> services.</p> </li> </ol>","title":"Azure AD OIDC integration"},{"location":"enterprise/configuration/authentication/#configure-user-access-allow-list","tags":["authentication","configuration"],"text":"<p>When using a public authentication provider such as Google or GitHub, you may need  to restrict the access to specific user email addresses or domains. </p> <p>Replace the <code>&lt;PROVIDER&gt;</code> placeholder with <code>github</code>, <code>google</code>, or <code>oidc</code> (<code>oidc</code> is used to specify any other authentication service based on OpenID Connect, e.g. Okta, AzureAD, Keycloak, etc.). You will need to include each provider separately, if specifying more than one.</p> <p>The allow list entries are case-insensitive.</p>  Environment variables <pre>1</pre><pre><code>TOWER_AUTH_&lt;PROVIDER&gt;_ALLOW_LIST=*@foo.com,user1@bar.com\n</code></pre>   tower.yml <pre>1\n2\n3\n4\n5\n6</pre><pre><code>tower:\n  auth:\n    &lt;PROVIDER&gt;:\n      allow-list:\n        - \"*@foo.com\"\n        - \"me@bar.com\"\n</code></pre>","title":"Configure user access allow list"},{"location":"enterprise/configuration/compute_environments/","tags":["compute","environment","configuration"],"text":"","title":"Compute Environments"},{"location":"enterprise/configuration/compute_environments/#tower_enable_platforms","tags":["compute","environment","configuration"],"text":"<p>Note</p> <p>As of Tower 21.10.1, it is recommended to define your compute environments via the <code>TOWER_ENABLE_PLATFORMS</code> variable. Earlier implementations which rely on the <code>MICRONAUT_ENVIRONMENTS</code> variable will continue to work and do not require modification.</p>  <p>Tower allows the deployment of Nextflow pipelines on various execution platforms. </p> <p>Populate the <code>TOWER_ENABLE_PLATFORMS</code> configuration variable with the platforms which your organization makes availabe to its Tower users. </p> <p>Example: <pre>1\n2</pre><pre><code># Include all available platforms, separating each value with a comma.\nTOWER_ENABLE_PLATFORMS=awsbatch-platform,gls-platform,azbatch-platform,slurm-platform\n</code></pre></p> <p>The following options are available: </p> <ul> <li><code>awsbatch-platform</code>: AWS Batch cloud compute service</li> <li><code>gls-platform</code>: Google Life Sciences cloud compute service</li> <li><code>azbatch-platform</code>: Azure Batch cloud compute service</li> <li><code>lsf-platform</code>: IBM LSF batch scheduler </li> <li><code>slurm-platform</code>: Slurm batch scheduler</li> <li><code>altair-platform</code>: Altair PBS pro batch scheduler</li> <li><code>uge-platform</code>: GridEngine batch scheduler</li> <li><code>k8s-platform</code>: Kubernetes compute platform</li> <li><code>eks-platform</code>: AWS EKS compute platform</li> <li><code>gke-platform</code>: Google GKE compute platform</li> <li><code>googlebatch-platform</code>: Google Batch cloud compute service</li> </ul>","title":"<code>TOWER_ENABLE_PLATFORMS</code>"},{"location":"enterprise/configuration/compute_environments/#micronaut_environments","tags":["compute","environment","configuration"],"text":"<p>Note</p> <p>Clients using Tower 21.10.1 or later do not need to use this configuration variable.</p>  <p>The Tower <code>cron</code> and <code>backend</code> containers are both based on container image <code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:vxx.xx.x</code>. The values supplied to this configuration variable control the behavior of the resulting container.</p> <pre>1\n2\n3\n4\n5</pre><pre><code># Settings for &lt;CRON&gt; container.\nMICRONAUT_ENVIRONMENTS=prod,redis,cron\n\n# Only for &lt;BACKEND&gt; container.\nMICRONAUT_ENVIRONMENTS=prod,redis,ha\n</code></pre>","title":"<code>MICRONAUT_ENVIRONMENTS</code>"},{"location":"enterprise/configuration/database_and_redis/","tags":["database","redis","configuration"],"text":"","title":"Tower and Redis Databases"},{"location":"enterprise/configuration/database_and_redis/#database-configuration","tags":["database","redis","configuration"],"text":"","title":"Database configuration"},{"location":"enterprise/configuration/database_and_redis/#sql-database","tags":["database","redis","configuration"],"text":"<p>The Database configuration differs based on your Tower deployment:</p> <ul> <li>If you use the MySQL container provided in the deployment files, it will create a MySQL user and database for you.</li> <li>If you use an external database service, you must create a MySQL user and database manually.</li> </ul>","title":"SQL Database"},{"location":"enterprise/configuration/database_and_redis/#generate-user-and-schema","tags":["database","redis","configuration"],"text":"<p>If you choose to use an external database service, execute the SQL statements below to initialize the Tower database.</p>","title":"Generate User and Schema"},{"location":"enterprise/configuration/database_and_redis/#mysql","tags":["database","redis","configuration"],"text":"<pre>1\n2\n3\n4\n5</pre><pre><code>CREATE DATABASE tower;\nALTER DATABASE tower CHARACTER SET utf8 COLLATE utf8_bin;\n\nCREATE USER 'tower' IDENTIFIED BY &lt;password&gt;;\nGRANT ALL PRIVILEGES ON tower.* TO tower@'%' ;\n</code></pre>","title":"MySQL"},{"location":"enterprise/configuration/database_and_redis/#mariadb","tags":["database","redis","configuration"],"text":"<pre>1</pre><pre><code>GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, REFERENCES, INDEX, ALTER, CREATE TEMPORARY TABLES, LOCK TABLES, EXECUTE, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, EVENT, TRIGGER on tower.* TO tower@'%';\n</code></pre>","title":"MariaDB"},{"location":"enterprise/configuration/database_and_redis/#tower-configuration","tags":["database","redis","configuration"],"text":"<p>Use environment variables (<code>tower.env</code> file) for database configuration in Tower. </p>  <p>Warning</p> <p>As of Tower v22.2.0, new and pre-existing MySQL configurations must use a new driver. Set <code>TOWER_DB_DRIVER=org.mariadb.jdbc.Driver</code> (in <code>tower.env</code>) or <code>driverClassName:org.mariadb.jdbc.Driver</code> (in <code>tower.yml</code>). All other MySQL-related <code>TOWER_DB_*</code> values should still be used.</p>","title":"Tower configuration"},{"location":"enterprise/configuration/database_and_redis/#mysql_1","tags":["database","redis","configuration"],"text":"tower.env <pre>1\n2\n3\n4\n5\n6</pre><pre><code>TOWER_DB_URL=jdbc:mysql://YOUR-DB-HOST:3306/tower\nTOWER_DB_DRIVER=org.mariadb.jdbc.Driver\nTOWER_DB_DIALECT=io.seqera.util.MySQL55DialectCollateBin\nTOWER_DB_USER=tower\nTOWER_DB_PASSWORD=tower\nFLYWAY_LOCATIONS=classpath:db-schema/mysql\n</code></pre>","title":"MySQL"},{"location":"enterprise/configuration/database_and_redis/#mariadb_1","tags":["database","redis","configuration"],"text":"tower.env <pre>1\n2\n3\n4\n5\n6</pre><pre><code>TOWER_DB_URL=jdbc:mariadb://YOUR-DB-HOST:3306/tower\nTOWER_DB_DRIVER=org.mariadb.jdbc.Driver\nTOWER_DB_DIALECT=io.seqera.util.MariaDB10DialectCollateBin\nTOWER_DB_USER=tower\nTOWER_DB_PASSWORD=tower\nFLYWAY_LOCATIONS=classpath:db-schema/mariadb\n</code></pre>","title":"MariaDB"},{"location":"enterprise/configuration/database_and_redis/#redis-database","tags":["database","redis","configuration"],"text":"<p>Note</p> <p>As of Tower version 22.3, we officially support Redis version 6. Follow your cloud provider specifications to upgrade your instance. </p>   tower.env <pre>1\n2</pre><pre><code>TOWER_REDIS_URL=redis://YOUR_REDIS_HOST:6379\nTOWER_REDIS_PASSWORD=your_redis_password\n</code></pre>","title":"Redis Database"},{"location":"enterprise/configuration/git_integration/","tags":["git configuration"],"text":"","title":"Git Integration"},{"location":"enterprise/configuration/git_integration/#private-git-hosting-services","tags":["git configuration"],"text":"<p>Nextflow has built-in support for public and private Git repositories and services, such as Bitbucket, GitHub and GitLab. To access private repositories, provide your security credentials using either the Tower application interface, or the following keys in the <code>tower.yml</code> file: </p>  tower.env <pre>1\n2\n3\n4\n5\n6\n7</pre><pre><code>TOWER_SCM_PROVIDERS_GITHUB_USER=&lt;YOUR GITHUB USER NAME&gt;\nTOWER_SCM_PROVIDERS_GITHUB_PASSWORD=&lt;YOUR GITHUB ACCESS TOKEN OR PASSWORD&gt;\nTOWER_SCM_PROVIDERS_GITLAB_USER=&lt;YOUR GITLAB USER NAME&gt;\nTOWER_SCM_PROVIDERS_GITLAB_PASSWORD=&lt;YOUR GITLAB PASSWORD&gt;\nTOWER_SCM_PROVIDERS_GITLAB_TOKEN=&lt;YOUR GITLAB TOKEN&gt;\nTOWER_SCM_PROVIDERS_BITBUCKET_USER=&lt;YOUR BITBUCKET USER NAME&gt;\nTOWER_SCM_PROVIDERS_BITBUCKET_PASSWORD=&lt;YOUR BITBUCKET TOKEN OR PASSWORD&gt;\n</code></pre>   tower.yml <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13</pre><pre><code>tower:\n  scm:\n    providers:\n      github:\n        user: &lt;YOUR GITHUB USER NAME&gt;\n        password: &lt;YOUR GITHUB ACCESS TOKEN OR PASSWORD&gt;\n      gitlab:\n        user: &lt;YOUR GITLAB USER NAME&gt;\n        password: &lt;YOUR GITLAB PASSWORD&gt;\n        token: &lt;YOUR GITLAB TOKEN&gt;\n      bitbucket:\n        user: &lt;YOUR BITBUCKET USER NAME&gt;\n        password: &lt;YOUR BITBUCKET TOKEN OR PASSWORD&gt;\n</code></pre>  <p>See the documentation for each provider to learn how to create security access tokens: </p> <ul> <li>Github</li> <li>BitBucket Server</li> <li>Gitlab</li> </ul>","title":"Private Git hosting services"},{"location":"enterprise/configuration/mail_server/","tags":["mail","configuration"],"text":"","title":"Mail Server"},{"location":"enterprise/configuration/mail_server/#mail-server-configuration","tags":["mail","configuration"],"text":"<p>You can specify the configuration settings for email using either environment variables, or the <code>tower.yml</code> file.</p> <p>Note: To configure proxy settings, the <code>tower.yml</code> file must be used (see below).</p>  tower.env <pre>1\n2\n3\n4\n5\n6\n7\n8</pre><pre><code>TOWER_CONTACT_EMAIL=hello@foo.com\nTOWER_SMTP_HOST=your.smtphost.com\nTOWER_SMTP_USER=your_smtp_user\nTOWER_SMTP_PASSWORD=your_smtp_password\nTOWER_SMTP_PORT=587\nTOWER_SMTP_AUTH=true\nTOWER_SMTP_STARTTLS_ENABLED=true\nTOWER_SMTP_STARTTLS_REQUIRED=true\n</code></pre>   tower.yml <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13</pre><pre><code>mail:\n  smtp:\n    host: \"your.smtphost.com\"\n    port: \"587\"\n    auth: \"true\"\n    user: \"your_smtp_user\"\n    password: \"your_smtp_password\"\n    starttls:\n      enable: true\n      required: true\n    proxy:\n      host: \"proxy.com\"\n      port: \"5566\"\n</code></pre>","title":"Mail server configuration"},{"location":"enterprise/configuration/networking/","tags":["networking","configuration"],"text":"","title":"Networking"},{"location":"enterprise/configuration/networking/#http-proxy-environment-variables","tags":["networking","configuration"],"text":"<p>Warning</p> <p>Proxies which require passwords are not supported. </p>  <p>If your Tower instance must access the internet via a proxy server, the settings can be configured using: </p> <ul> <li><code>http_proxy</code>: Defines the proxy server to be used for HTTP connections.</li> <li><code>https_proxy</code>: Defines the proxy server to be used for HTTPS connections.</li> <li><code>no_proxy</code>: Defines one or more host names that should not use the proxy server.</li> </ul> <p>Example: <pre>1\n2\n3\n4</pre><pre><code># syntax\nexport http_proxy='alice.example.com:8080'\nexport https_proxy='alice.example.com:8080'\nexport no_proxy=internal.example.com,internal2.example.com\n</code></pre></p>  <p>Tip</p> <p>The above environment variables can be either lowercase or uppercase.</p>","title":"HTTP proxy environment variables"},{"location":"enterprise/configuration/networking/#isolated-environments","tags":["networking","configuration"],"text":"<p>If you are deploying Tower in an environment that has no external internet access, ensure that no pipeline assets or parameters in your configuration contain external links, as this will lead to connection failures. </p>","title":"Isolated environments"},{"location":"enterprise/configuration/overview/","tags":["configuration"],"text":"<p>The configuration of your Tower instance can be controlled using various environment variables specified in the tower.env and tower.yml files. Note that a number of core Tower configuration values must be specified using environment variables in <code>tower.env</code>. </p> <p>In the tower.yml file, configuration options are objects nested within the <code>tower</code> object. This is formatted as follows:</p> <pre>1\n2\n3\n4\n5\n6\n7</pre><pre><code>tower:\n...\n  mail:\n    from: \"hello@foo.com\"\n    smtp:\n      host: \"your.smtphost.com\"\n...\n</code></pre> <p>The following parameters control the Tower configuration and deployment:</p>","title":"Tower configuration"},{"location":"enterprise/configuration/overview/#generic-options","tags":["configuration"],"text":"<p>Specify general Tower configuration values in your environment variables. The boolean value to enable user workspaces can also be specified in your tower.yml file. </p>  Environment variables <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15</pre><pre><code>`TOWER_SERVER_URL`: Server URL e.g. `https://tower.your-company.com` (**required**).\n\n`TOWER_CONTACT_EMAIL`: Sysadmin email contact e.g. `tower@your-company.com` (**required**).\n\n`TOWER_LICENSE`: Your Tower license key. If you don't have a license key, contact [Seqera sales team](mailto:sales@seqera.io)  (**required**).\n\n`TOWER_APP_NAME`: Application name (default: `Tower`).\n\n`TOWER_CONFIG_FILE`: Custom path for the `tower.yml` file.\n\n`TOWER_LANDING_URL`: Customize the landing page for the application (requires Tower 21.10.1 or later).\n\n`TOWER_CRON_SERVER_PORT`: Define the HTTP port usd by the Tower cron service (default: `8080`, requires Tower 21.06.1 or later).\n\n`TOWER_USER_WORKSPACE_ENABLED` : Enable or disable the showing of the user private workspace context. (default: `true`, requires Tower 22.1.0 or later).\n</code></pre>   tower.yml <pre>1\n2\n3</pre><pre><code>tower:\n  admin:\n    user-workspace-enabled: true \n</code></pre>","title":"Generic options"},{"location":"enterprise/configuration/overview/#tower-and-redis-databases","tags":["configuration"],"text":"<p>For further information, see Tower and Redis Databases.</p>  <p>Note</p> <p>As of Tower version 22.3, we officially support Redis version 6. Follow your cloud provider specifications to upgrade your instance. </p>   Environment variables <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19</pre><pre><code>- `TOWER_DB_URL`: Database JDBC connection URL, e.g., `jdbc:mysql://localhost:3307/tower` (**required**).\n\n- `TOWER_DB_USER`: Database user name (**required**).\n\n- `TOWER_DB_PASSWORD`: Database user password (**required**).\n\n- `TOWER_DB_DRIVER`: Database JDBC driver class name (default: `org.mariadb.jdbc.Driver`).\n\n- `TOWER_DB_DIALECT`: Database SQL Hibernate dialect (default: `io.seqera.util.MySQL55DialectCollateBin`).\n\n- `TOWER_DB_MIN_POOL_SIZE`: Database min connections pool size, e.g., 5 (default: 5).\n\n- `TOWER_DB_MAX_POOL_SIZE`: Database max connections pool size, e.g., 20 (default: 10).\n\n- `TOWER_DB_MAX_LIFETIME`: Database max lifespan of connections in milliseconds (default: 1800000)\n\n- `TOWER_REDIS_URL`: Custom Redis instance connection URL (default: `redis://redis:6379`, requires Tower 21.06.1 or later).\n\n- `TOWER_REDIS_PASSWORD`: Custom Redis password to connect to Redis instance above. \n</code></pre>","title":"Tower and Redis Databases"},{"location":"enterprise/configuration/overview/#mail-server","tags":["configuration"],"text":"<p>For further information, see Mail server.</p>  Environment variables <pre>1\n2\n3\n4\n5</pre><pre><code>- `TOWER_SMTP_HOST`: SMTP server host name e.g. `email-smtp.eu-west-1.amazonaws.com` (**required**)\n- `TOWER_SMTP_USER`: SMTP server username (**required**)\n- `TOWER_SMTP_PASSWORD`: SMTP server user password (**required**)\n- `TOWER_SMTP_PORT`: SMTP server port (default: `587`)\n- `TOWER_SMTP_AUTH`: SMTP server authentication (default: `true`)\n</code></pre>   tower.yml <pre>1\n2\n3\n4\n5\n6\n7</pre><pre><code>mail:\n  smtp:\n    host: \"your.smtphost.com\" # SMTP server host name (required)\n    user: \"your_smtp_user\" # SMTP server username\n    password: \"your_smtp_password\" # SMTP server user password\n    port: \"587\" # SMTP server port (default: 587)\n    auth: \"true\" # SMTP server authentication (default: true)\n</code></pre>  <pre>1</pre><pre><code>\n</code></pre>","title":"Mail Server"},{"location":"enterprise/configuration/overview/#cryptographic-options","tags":["configuration"],"text":"<ul> <li><code>TOWER_JWT_SECRET</code>: Secret used to generate the login JWT token. Use a long random string (35 characters or more) (required).</li> <li><code>TOWER_CRYPTO_SECRETKEY</code>: Secret key used to encrypt user credentials. Use a long random string (required).</li> </ul>  <p>Warning</p> <p>The <code>TOWER_CRYPTO_SECRETKEY</code> should not be modified or altered across Tower starts, otherwise the application won't be able to decrypt the corresponding data. Use different keys for independent installations (e.g. test and production). Make sure to store a copy in a safe location.</p>","title":"Cryptographic options"},{"location":"enterprise/configuration/overview/#compute-environments","tags":["configuration"],"text":"<p>For further information,see Compute environments.</p> <ul> <li><code>TOWER_ENABLE_PLATFORMS</code>: Comma separate list of execution backends to be enabled (required).</li> <li><code>MICRONAUT_ENVIRONMENTS</code>: Enable specific configuration profile for the Micronaut backend service (required).</li> <li><code>TOWER_FORGE_PREFIX</code>: Override the default <code>TowerForge</code> prefix appended to AWS resources created by Tower Forge with a custom value. </li> </ul>","title":"Compute environments"},{"location":"enterprise/configuration/overview/#nextflow-launch-container","tags":["configuration"],"text":"<ul> <li><code>TOWER_LAUNCH_CONTAINER</code>: Container image to run Nextflow execution (requires Tower 20.10.2 or later)</li> </ul>","title":"Nextflow launch container"},{"location":"enterprise/configuration/overview/#tower-api","tags":["configuration"],"text":"<p>For further information, please refer Tower API page.</p> <ul> <li><code>TOWER_ENABLE_OPENAPI</code>: Set <code>true</code> to enable OpenAPI documentation endpoint.</li> </ul>","title":"Tower API"},{"location":"enterprise/configuration/overview/#admin-panel","tags":["configuration"],"text":"<p>Note</p> <p>This feature is available in Tower 21.10.3, 21.12.1, and 22.1.0 (or later).</p>  <p>To enable access to the application admin panel for specific users i.e. <code>root users</code>, add the <code>TOWER_ROOT_USERS</code> variable to your <code>tower.env</code> file (or <code>root-users</code> to your <code>tower.yml</code>). You can specify the user email IDs, separated by commas <code>,</code> as value for this variable. For example:</p>  Environment variables <pre>1</pre><pre><code>TOWER_ROOT_USERS=user1@myorg.com,user2@myorg.com\n</code></pre>   tower.yml <pre>1\n2\n3</pre><pre><code>tower:\n  admin:\n    root-users: \"user1@myorg.com,user2@myorg.com\"\n</code></pre>","title":"Admin panel"},{"location":"enterprise/configuration/overview/#custom-navigation-menu","tags":["configuration"],"text":"<p>To modify the Tower top navigation menu and add custom menu items, add a configuration snippet similar the one shown below in the tower.yml configuration file:</p> <pre>1\n2\n3\n4\n5\n6\n7</pre><pre><code>tower:\n  navbar:\n    menus:\n      - label: \"My Community\"\n        url: \"https://host.com/foo\"\n      - label: \"My Pipelines\"\n        url: \"https://other.com/bar\"\n</code></pre>","title":"Custom navigation menu"},{"location":"enterprise/configuration/overview/#logging","tags":["configuration"],"text":"<p>To customize the log detail pattern displayed when using <code>STDOUT</code>, use the <code>TOWER_LOG_PATTERN</code> environment variable to specify a pattern in the Logback pattern layout encoding syntax. See here for a reference of the full Logback pattern syntax.</p> <pre>1</pre><pre><code>TOWER_LOG_PATTERN=%d{MMM-dd HH:mm:ss.SSS} [%t] %X{ip:--} %-5level %logger{36} - %msg%n}  # Default logging pattern shown\n</code></pre> <p>To change the output format of Tower logs, the <code>TOWER_LOG_APPENDER_TYPE</code> variable can be used. The available logging formats are <code>STDOUT</code> (default) and <code>JSON</code>.</p> <pre>1</pre><pre><code>TOWER_LOG_APPENDER_TYPE=JSON\n</code></pre>","title":"Logging"},{"location":"enterprise/configuration/ssl_tls/","tags":["ssl","tls","https","configuration"],"text":"<p>We recommend using public TLS certificates wherever possible. Private certificates are supported, but require additional configuration during Tower installation and Nextflow execution.</p>","title":"SSL/TLS"},{"location":"enterprise/configuration/ssl_tls/#configure-nextflow-tower-to-trust-your-private-certificate","tags":["ssl","tls","https","configuration"],"text":"<p>If you secure related infrastructure (such as private git repositories) with certificates issued by a private Certificate Authority, these certificates must be loaded into the Tower Enterprise containers. You can achieve this in several ways. </p>  Options <ol> <li>This guide assumes you are using the original containers supplied by Seqera.</li> <li>Replace <code>TARGET_HOSTNAME</code>, <code>TARGET_ALIAS</code>, and <code>PRIVATE_CERT.pem</code> with your unique values.</li> <li>Previous instructions advised using <code>openssl</code>. As of April 2023, the native <code>keytool</code> utility is preferred as it simplifies steps and better accommodates private CA certificates.</li> </ol> Use Docker volumeUse K8s ConfigMapDownload on Pod start   <ol> <li> <p>Retrieve the private certificate on your Tower container host.       <pre>1</pre><pre><code>keytool -printcert -rfc -sslserver TARGET_HOSTNAME:443  &gt;  /PRIVATE_CERT.pem\n</code></pre></p> </li> <li> <p>Modify the <code>backend</code> and <code>cron</code> container configuration blocks in docker-compose.yml.</p> <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15</pre><pre><code>CONTAINER_NAME:\n  # -- Other keys here like `image` and `networks`--\n\n  # Add a new mount for the downloaded certificate.\n  volumes:\n    - type: bind\n      source: /PRIVATE_CERT.pem\n      target: /etc/pki/ca-trust/source/anchors/PRIVATE_CERT.pem\n\n  # Add a new keytool import line PRIOR to 'update-ca-trust' for the certificate.\n  command: &gt;\n    sh -c \"keytool -import -trustcacerts -storepass changeit -noprompt -alias TARGET_ALIAS -file /etc/pki/ca-trust/source/anchor/TARGET_HOSTNAME.pem &amp;&amp;\n          update-ca-trust &amp;&amp;\n          /wait-for-it.sh db:3306 -t 60 &amp;&amp;\n          /tower.sh\"\n</code></pre> </li> </ol>   <ol> <li> <p>Retrieve the private certificate on a machine with CLI access to your Kubernetes cluster. </p> <pre>1</pre><pre><code>keytool -printcert -rfc -sslserver TARGET_HOSTNAME:443 &gt; /PRIVATE_CERT.pem \n</code></pre> </li> <li> <p>Load the  certificate as a ConfigMap in the same namespace where your Tower instance will run.</p> <pre>1</pre><pre><code>kubectl create configmap private-cert-pemstore --from-file=/PRIVATE_CERT.pem\n</code></pre> </li> <li> <p>Modify both the <code>backend</code> and <code>cron</code> Deployment objects:</p> <ol> <li> <p>Define a new volume based on the certificate ConfigMap.</p> <pre>1\n2\n3\n4\n5\n6\n7</pre><pre><code>spec:\n  template:\n    spec:\n      volumes:\n        - name: private-cert-pemstore\n          configMap:\n            name: private-cert-pemstore\n</code></pre> </li> <li> <p>Add a volumeMount entry into the container definition.</p> <pre>1\n2\n3\n4\n5\n6\n7\n8\n9</pre><pre><code>spec:\n  template:\n    spec:\n      containers:\n        - name: CONTAINER_NAME\n          volumeMounts:\n            - name: private-cert-pemstore\n              mountPath: /etc/pki/ca-trust/source/anchors/PRIVATE_CERT.pem\n              subPath: PRIVATE_CERT.pem\n</code></pre> </li> <li> <p>Modify the container start command to load the certificate prior to running Tower.</p> <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11</pre><pre><code>spec:\n  template:\n    spec:\n      containers:\n        - name: CONTAINER_NAME\n          command: [\"/bin/sh\"]\n          args:\n            - -c\n            - | \n                keytool -import -trustcacerts -cacerts -storepass changeit -noprompt -alias TARGET_ALIAS -file /PRIVATE_CERT.pem; \n                ./tower.sh\n</code></pre> </li> </ol> </li> </ol>   <ol> <li> <p>Modify both the <code>backend</code> and <code>cron</code> Deployment objects to retrieve and load the certificate prior to running Tower.</p> <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12</pre><pre><code>spec:\n  template:\n    spec:\n      containers:\n        - name: CONTAINER_NAME\n          command: [\"/bin/sh\"]\n          args:\n            - -c\n            - | \n                keytool -printcert -rfc -sslserver TARGET_HOST:443  &gt;  /PRIVATE_CERT.pem; \n                keytool -import -trustcacerts -cacerts -storepass changeit -noprompt -alias TARGET_ALIAS -file /PRIVATE_CERT.pem; \n                ./tower.sh\n</code></pre> </li> </ol>","title":"Configure Nextflow Tower to trust your private certificate"},{"location":"enterprise/configuration/ssl_tls/#configure-the-nextflow-launcher-image-to-trust-your-private-certificate","tags":["ssl","tls","https","configuration"],"text":"<p>If you secure infrastructure such as private git repositories or your Tower Enterprise instance with certificates issued by a private Certificate Authority, these certificates must also be loaded into the Nextflow launcher container.</p>  Options <ol> <li>This guide assumes you are using the default <code>nf-launcher</code> image supplied by Seqera.</li> <li>Remember to replace <code>TARGET_HOSTNAME</code>, <code>TARGET_ALIAS</code>, and <code>PRIVATE_CERT.pem</code> with unique values.</li> <li>Previous instructions advised using <code>openssl</code>. As of April 2023, the native <code>keytool</code> utility is preferred as it simplifies steps and better accommodates private CA certificates.</li> </ol> Import certificate via pre-run script   <ol> <li>Add the following to your compute environment pre-run script:   <pre>1\n2\n3\n4\n5</pre><pre><code>keytool -printcert -rfc -sslserver TARGET_HOSTNAME:443  &gt;  /PRIVATE_CERT.pem\nkeytool -import -trustcacerts -cacerts -storepass changeit -noprompt -alias TARGET_ALIAS -file /PRIVATE_CERT.pem\n\ncp /PRIVATE_CERT.pem /etc/pki/ca-trust/source/anchors/PRIVATE_CERT.pem\nupdate-ca-trust\n</code></pre></li> </ol>","title":"Configure the Nextflow launcher image to trust your private certificate"},{"location":"enterprise/configuration/ssl_tls/#configure-tower-to-present-a-ssltls-certificate","tags":["ssl","tls","https","configuration"],"text":"<p>You can secure your Tower implementation with a TLS certificate in several ways.</p>  Options Load balancer (recommended)Reverse proxy containerModify <code>frontend</code> container   <p>Place a load balancer, configured to present a certificate and act as a TLS termination point, in front of the Tower application.</p> <p>This solution is likely already implemented for cloud-based Kubernetes implementations and can be easily implemented for Docker Compose-based stacks. See this example.</p>   <p>This solution works well for Docker Compose-based stacks to avoid the additional cost and maintenance of a load balancer. See this example.</p>   <p>Due to complications that can be encountered during upgrades, this approach is not recommended.</p> <p> Show me anyway <p>This example assumes deployment on an Amazon Linux 2 AMI.</p> <ol> <li> <p>Install nginx and other required packages:</p> <pre>1\n2\n3\n4\n5\n6</pre><pre><code>sudo amazon-linux-extras install nginx1.12\nsudo wget -r --no-parent -A 'epel-release-*.rpm' https://dl.fedoraproject.org/pub/epel/7/x86_64/Packages/e/\nsudo rpm -Uvh dl.fedoraproject.org/pub/epel/7/x86_64/Packages/e/epel-release-*.rpm\nsudo yum-config-manager --enable epel*\nsudo yum repolist all\nsudo amazon-linux-extras install epel -y\n</code></pre> </li> <li> <p>Generate a private certificate and key.</p> </li> <li> <p>Create a <code>ssl.conf</code> file.</p> <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31</pre><pre><code>server {\n    server_name your.server.name; # replace with your server name\n    root        /usr/share/nginx/html;\n\n    location / {\n    proxy_set_header          Host $host;\n    proxy_set_header          X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header          X-Real-IP $remote_addr;\n    proxy_set_header          X-Forwarded-Proto $scheme;\n\n    proxy_set_header          Authorization $http_authorization;\n    proxy_pass_header         Authorization;\n\n    proxy_pass                http://frontend/;\n    proxy_read_timeout        90;\n    proxy_redirect            http://frontend/ https://your.redirect.url/;\n    }\n\n        error_page 404 /404.html;\n            location = /40x.html {\n        }\n\n        error_page 500 502 503 504 /50x.html;\n            location = /50x.html {\n        }\n        listen [::]:443 ssl ipv6only=on;\n        listen 443 ssl;\n\n        ssl_certificate /etc/ssl/testcrt.crt;\n        ssl_certificate_key /etc/ssl/testkey.key;\n}\n</code></pre> </li> <li> <p>Make a local copy of the <code>frontend</code> container's <code>/etc/nginx/nginx.conf</code> file.</p> </li> <li> <p>Add the following to the <code>server</code> block of your local <code>nginx.conf</code> file:</p> <pre>1</pre><pre><code>include /etc/nginx/ssl.conf;\n</code></pre> </li> <li> <p>Modify the <code>frontend</code> container definition in your <code>docker-compose.yml</code> file:</p> <p><pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15</pre><pre><code>frontend:\nimage: cr.seqera.io/frontend:${TAG}\nnetworks:\n    - frontend\nports:\n    - 8000:80\n    - 443:443\nvolumes:\n    - $PWD/nginx.conf:/etc/nginx/nginx.conf\n    - $PWD/ssl.conf:/etc/nginx/ssl.conf\n    - $PWD/cert/testcrt.crt:/etc/ssl/testcrt.crt\n    - $PWD/cert/testkey.key:/etc/ssl/testkey.key\nrestart: always\ndepends_on:\n    - backend\n</code></pre> </p> </li> </ol>","title":"Configure Tower to present a SSL/TLS certificate"},{"location":"enterprise/configuration/ssl_tls/#tls-version-support","tags":["ssl","tls","https","configuration"],"text":"<p>Tower Enterprise versions 22.3.2 and earlier rely on Java 11 (Amazon Corretto). You may encounter issues when integrating with third-party services that enforce <code>TLS v1.2</code> (e.g. Azure Active Directory OIDC). </p> <p><code>TLS v1.2</code> can be explicitly enabled by default using JDK environment variables:</p> <pre>1</pre><pre><code>_JAVA_OPTIONS=\"-Dmail.smtp.ssl.protocols=TLSv1.2\n</code></pre>","title":"TLS version support"},{"location":"enterprise/configuration/tower_api/","tags":["api","configuration"],"text":"","title":"Tower API"},{"location":"enterprise/configuration/tower_api/#tower-api-configuration","tags":["api","configuration"],"text":"<p>To enable the OpenAPI specification in your deployment, similar to tower.nf/openapi/index.html, use the <code>TOWER_ENABLE_OPENAPI</code> environment variable.</p> <pre>1</pre><pre><code>TOWER_ENABLE_OPENAPI=true\n</code></pre>  <p>Tower CLI</p> <p>If you'd like to use the Tower CLI, then you'll need to enable the API endpoints.</p>","title":"Tower API configuration"},{"location":"enterprise/configuration/wave/","tags":["wave","containers","configuration"],"text":"<p>From version 22.4, Tower supports Seqera Labs' Wave containers service for on-premise installations.</p> <p>Learn more about Wave's features here, and about Wave integration with Nextflow here.</p>","title":"Wave containers"},{"location":"enterprise/configuration/wave/#pairing-tower-with-wave","tags":["wave","containers","configuration"],"text":"<p>Pairing Tower with Wave requires the following:</p> <ul> <li> <p>Credentials to authenticate to your (private or public) container registry must be added to Tower. Follow the container registry credentials instructions for your provider.</p> </li> <li> <p>Your container registry must allow ingress from the Wave service (<code>https://wave.seqera.io</code>).</p> </li> <li> <p>The Wave service (<code>https://wave.seqera.io</code>) must be accessible from the network where Tower is installed (i.e. the domain should be whitelisted in protected Tower installations).</p> </li> <li> <p>The <code>TOWER_ENABLE_WAVE=true</code> environment variable must be added to the Tower configuration environment.</p> </li> <li> <p>The <code>WAVE_SERVER_URL=\"https://wave.seqera.io\"</code> environment variable must be added to the Tower configuration environment.</p> </li> </ul>   <p>Note</p> <p>Wave does not currently support container repositories that have private CA SSL certificates applied.</p>   <p>You can test connectivity with the Wave service by accessing https://wave.seqera.io/service-info, either from the browser or with cURL:</p>  <pre>1</pre><pre><code>$ curl https://wave.seqera.io/service-info\n</code></pre>  <p>When these conditions are met, the Wave feature is available on the Tower compute environment creation page (currently only available for AWS compute environments).</p> <p>Once Wave is enabled, it will be possible to use private container repositories and the Fusion file system in your Nextflow pipelines.</p> <p>Wave can also be enabled in the Nextflow pipeline config file. See here for more information.</p>","title":"Pairing Tower with Wave"},{"location":"enterprise/prerequisites/aws/","tags":["aws","prerequisites","configuration"],"text":"<p>This page describes the infrastructure and other prerequisites for deploying Tower on Amazon Web Services (AWS).</p>","title":"AWS"},{"location":"enterprise/prerequisites/aws/#tower-container-images","tags":["aws","prerequisites","configuration"],"text":"<p>Nextflow Tower is distributed as a collection of Docker containers available through the Seqera Labs container registry (cr.seqera.io). Contact support to get your container access credentials. Once you have received your credentials, log in to the registry using these steps:</p> <ol> <li> <p>Retrieve the username and password you received from Seqera Labs support.</p> </li> <li> <p>Run the following Docker command to authenticate to the registry (using the <code>username</code> and <code>password</code> values copied in step 1):</p> <p>docker login -u '' -p '' cr.seqera.io  <li> <p>Pull the Nextflow Tower container images with the following commands:</p> </li>  <pre>1\n2\n3</pre><pre><code>docker pull cr.seqera.io/private/nf-tower-enterprise/backend:v23.1.0\n\ndocker pull cr.seqera.io/private/nf-tower-enterprise/frontend:v23.1.0\n</code></pre>   <p>Warning</p> <p>The Seqera Labs container registry <code>cr.seqera.io</code> is the default Tower container image registry from version 22.4. Use of the AWS, Azure, and Google Cloud Tower image registries in existing installations is still supported but will be deprecated for new installations starting June 2023. See here for steps to use the Seqera Labs private AWS Elastic Container Registry.</p>","title":"Tower container images"},{"location":"enterprise/prerequisites/aws/#mandatory-prerequisites","tags":["aws","prerequisites","configuration"],"text":"","title":"Mandatory prerequisites"},{"location":"enterprise/prerequisites/aws/#smtp-server","tags":["aws","prerequisites","configuration"],"text":"<p>If you do not have an email server, you can use Amazon Simple Email Service.</p>   <p>Warning</p>  <p>Amazon blocks EC2 traffic over port 25 by default. Ensure your integration uses a port that can successfully reach your SMTP server.</p>","title":"SMTP server"},{"location":"enterprise/prerequisites/aws/#mysql-database","tags":["aws","prerequisites","configuration"],"text":"<p>An external database (i.e. external to your Docker Compose or Kubernetes deployment) is highly recommended for production deployments. If you don't have your own database service, you can use Amazon Relational Database Service.</p> <p>If you decide to use an external database, you must create a MySQL user and database manually. See Configuration for more details.</p>","title":"MySQL database"},{"location":"enterprise/prerequisites/aws/#ec2-instance-docker-compose","tags":["aws","prerequisites","configuration"],"text":"<p>An EC2 instance is required to deploy Tower via Docker Compose. Refer to the detailed instructions to provision an EC2 instance for this purpose.</p>","title":"EC2 instance (Docker Compose)"},{"location":"enterprise/prerequisites/aws/#eks-cluster-kubernetes","tags":["aws","prerequisites","configuration"],"text":"<p>An Elastic Kubernetes Service (EKS) cluster is required to deploy Tower via Kubernetes. See the EKS documentation to provision your own cluster. Your EKS cluster must satisfy the following requirements:</p> <ul> <li> <p>Kubernetes Version: 1.19 or later</p> </li> <li> <p>VPC Subnets</p> <ul> <li>At least 2 subnets, across two different Availability Zones.</li> <li>Subnets must be tagged for AWS Load Balancer Controller auto-discovery.</li> <li>Public subnets must be configured to auto-assign IPs on launch.</li> <li>Public and private subnets must allow egress traffic to the public internet.</li> </ul> </li> <li> <p>RBAC</p> <ul> <li>Cluster must be created by a non-root user.</li> <li><code>aws-auth</code> must be updated to allow access to additional IAM users/roles (if needed).</li> </ul> </li> <li> <p>Addons</p> <ul> <li>Install the cert-manager.</li> <li>Install the AWS Load Balancer Controller.</li> </ul> </li> </ul>   <p>Ingress and optional prerequisites</p> <p>The ingress that we provide for EKS assumes that your cluster supports:</p> <ol> <li>ALB provisioning via the AWS Load Balancer Controller</li> <li>ALB integration with the Amazon Certificate Manager</li> </ol> <p>Additionally, the ingress assumes the presence of SSL certificates, DNS resolution, and ALB logging.</p> <p>If you have chosen not to use some or all of these features, you will need to modify the manifest accordingly before applying it to the cluster.</p>","title":"EKS cluster (Kubernetes)"},{"location":"enterprise/prerequisites/aws/#optional-prerequisites","tags":["aws","prerequisites","configuration"],"text":"","title":"Optional prerequisites"},{"location":"enterprise/prerequisites/aws/#ssl-certificate","tags":["aws","prerequisites","configuration"],"text":"<p>An SSL certificate is required for your Tower instance to handle HTTPS traffic.</p> <p>If you do not have a pre-existing SSL certificate, you can request or import an SSL certificate into the Amazon Certificate Manager (ACM).</p>   <p>Warning</p> <p>From Tower 22.1.1, HTTP-only implementations must set the <code>TOWER_ENABLE_UNSAFE_MODE=true</code> environment variable in the Tower hosting infrastructure to enable user login.</p>","title":"SSL certificate"},{"location":"enterprise/prerequisites/aws/#dns","tags":["aws","prerequisites","configuration"],"text":"<p>DNS is required to support human-readable domain names and load-balanced traffic.</p> <p>If you do not have access to a pre-existing DNS service, you can use Amazon Route 53.</p>","title":"DNS"},{"location":"enterprise/prerequisites/aws/#s3-bucket-for-application-load-balancer-alb-logs","tags":["aws","prerequisites","configuration"],"text":"<p>ALB logs can be stored in an S3 Bucket.</p> <p>If you do not have a pre-configured S3 Bucket for ALB access log storage, you will need to specify and configure a target Bucket.</p>","title":"S3 bucket for Application Load Balancer (ALB) logs"},{"location":"enterprise/prerequisites/aws/#detailed-instructions","tags":["aws","prerequisites","configuration"],"text":"<p>This section provides step-by-step instructions for some commonly used AWS services for Tower deployment. See the AWS documentation for up-to-date instructions, and contact AWS support if you have any issues with provisioning AWS resources.</p>","title":"Detailed instructions"},{"location":"enterprise/prerequisites/aws/#fetch-tower-config-values-from-aws-parameter-store","tags":["aws","prerequisites","configuration"],"text":"<p>From Tower version 23.1, you can retrieve Tower configuration values remotely from the AWS Parameter Store. </p> <ol> <li>Configure AWS authentication to grant AWS Parameter Store access on your local host. </li> <li>Retrieve the Tower container images and install Tower per the instructions at the top of this page. </li> <li>The default value for <code>tower.application.name</code> is <code>tower-app</code>. This can be changed in your <code>tower.yml</code> configuration file. Note that your application name must be specified in the path to your configuration values in AWS Parameter Store (see step 5 below).</li> <li>Set the <code>TOWER_ENABLE_AWS_SSM</code> environment variable to <code>true</code>. Alternatively, add the value <code>aws-ssm</code> to the <code>TOWER_ENABLE_PLATFORMS</code> variable. </li> <li> <p>Add configuration parameters to the AWS Parameter Store individually, using the format <code>/config/&lt;application_name&gt;/&lt;cfg_path&gt; : &lt;cfg_value&gt;</code>. For example:</p> <pre>1</pre><pre><code>/config/tower-app/tower.logger.levels.com.amazonaws : \"WARN\"\n</code></pre> </li> <li> <p>Start or restart your Tower instance to confirm that the configuration value is fetched. The following entries should appear in your backend log:</p> <pre>1\n2</pre><pre><code>[main] - INFO  i.m.context.DefaultBeanContext - Reading bootstrap environment configuration\n[main] - INFO  i.m.d.c.c.DistributedPropertySourceLocator - Resolved 2 configuration sources from client: compositeConfigurationClient(AWS Parameter Store)\n</code></pre> </li> </ol>","title":"Fetch Tower config values from AWS Parameter Store"},{"location":"enterprise/prerequisites/aws/#amazon-ses","tags":["aws","prerequisites","configuration"],"text":"<p>Warning</p> <p>If you're using Simple Email Service in sandbox mode, ensure that both the sender and the receiver email addresses are verified via AWS SES. Note that sandbox is not recommended for production use. See the AWS docs for instructions to move out of the sandbox.</p>   <ol> <li> <p>Navigate to the Amazon Simple Email Service console.</p> </li> <li> <p>In the navigation menu, select SMTP Settings.</p> </li> <li> <p>Select Create my SMTP Credentials</p> </li> <li> <p>Select Create.</p> </li> <li> <p>Select Show User SMTP Credentials to copy your credentials, or select Download Credentials.</p>  <p>Warning</p> <p>The credentials (username and password) will not be shown to you again after this instance.</p>  </li> <li> <p>You will be automatically redirected to the IAM dashboard. Log back in to the Amazon SES Console.</p> </li> <li> <p>Select Email Addresses in the navigation menu. Then, select Verify a new Email Address.</p> </li> <li> <p>A pop-up asking for your email should automatically appear. Once you type in your email address and select Verify This Email Address, you should receive a confirmation email from Amazon SES to confirm email address ownership.</p> </li> <li> <p>Open the verification link in the message.</p>  <p>Warning</p> <p>The verification link is only valid for 24 hours after your original request for verification.</p>  </li> </ol> <p>You can now use Amazon SES to send email from this address.</p>  <p>Stop emails being flagged as spam</p> <p>To avoid emails sent from SES being flagged as spam, see here.</p>  <p>See the AWS documentation for more options, such as setting up an Easy DKIM for a Domain or Authentication Email with SPF.</p>","title":"Amazon SES"},{"location":"enterprise/prerequisites/aws/#amazon-rds","tags":["aws","prerequisites","configuration"],"text":"<ol> <li> <p>Open the Amazon RDS console.</p> </li> <li> <p>Select Create database -&gt; Standard create -&gt; MySQL.</p> </li> <li> <p>Under Edition, select MySQL Community and any version under 5.7.x, or 8.0.x.</p> </li> <li> <p>Enter the DB cluster identifier (e.g., <code>nftower-db</code>).</p> </li> <li> <p>Enter the Master username, or keep the default.</p> </li> <li> <p>Enter the Master password.</p> <ul> <li>To use an automatically generated master password, select Auto generate a password.</li> <li>To use a custom master password, deselect Auto generate a password and enter your password in Master password and Confirm password.</li> </ul> </li> <li> <p>Under Instance configuration, select the DB instance class and instance type.</p> </li> <li> <p>Under Connectivity, select the correct VPC security group. Confirm this with your AWS administrator.</p> </li> <li> <p>Under Additional configuration, enter the Initial database name (e.g., <code>tower</code>).</p> </li> <li> <p>Select Create database.</p> </li> </ol> <p>After your database is created:</p> <ol> <li> <p>Update the inbound rules for the underlying EC2 instance to allow MySQL connections.</p> </li> <li> <p>Update <code>TOWER_DB_URL</code> in your configuration value with the database hostname.</p> </li> </ol>","title":"Amazon RDS"},{"location":"enterprise/prerequisites/aws/#amazon-ec2","tags":["aws","prerequisites","configuration"],"text":"<p>If you have never set up an Amazon EC2 instance for Linux, refer to this guide to get started with Amazon EC2.</p> <ol> <li> <p>Open the AWS Management console.</p> </li> <li> <p>Log in as an IAM user with your credentials.</p> </li> <li> <p>Under AWS services, select All Services.</p> </li> <li> <p>Under Compute, select EC2.</p> </li> <li> <p>Select Instances, then Launch instances.</p> </li> <li> <p>You will be asked to choose an Amazon Machine Image (AMI). Scroll to the middle of the page and select Amazon Linux 2.</p> </li> <li> <p>Once you click Select, you will be redirected to Step 2: Choose an Instance Type.</p> </li> <li> <p>Scroll down and select either c5a.xlarge or c5.large \u2014 these provide 4 CPUs and 8GB of RAM.</p> </li> <li> <p>Select Next: Configure Instance Details.</p> </li> <li> <p>If required, configure the instance details settings. Then, select Next: Add Storage.</p> </li> <li> <p>The root storage should be 20GB. Configure this under Size (GiB).</p> </li> <li> <p>Select Add Tags (if required) to add case-sensitive key-value pairs (e.g., <code>key = Name</code> and <code>value = Webserver</code>).</p> </li> <li> <p>Select Next: Configure Security Group.</p> </li> <li> <p>Enter <code>nftower-sg</code> as the Security Group name.</p> </li> <li> <p>Optionally, you can enter a description for your Security Group's name.</p> </li> <li> <p>Configure the type of protocol settings. Note that the security group port must be configured to 8000.</p> </li> <li> <p>Select Review and Launch.</p> </li> <li> <p>Once you have reviewed your instance, select Launch.</p> </li> <li> <p>Select an existing key pair or create a new one in the pop-up that appears.</p> <p>If you already have an existing key pair, select Choose an existing key pair and choose from the available options in the drop-down menu.</p> <p>If you do not have a key pair yet, select Create a new keypair. Enter a name, then select Download Key Pair.</p> <p>Note: once you download the key pair, store it in a secure and accessible location. You will not be able to download the file again after it is created.</p> </li> <li> <p>Select Launch Instances.</p> </li> <li> <p>Use the key pair to connect to the server using SSH and its public IP address. Terminal-based SSH is easier to use than browser-based SSH for copying and pasting text.</p> </li> <li> <p>Enter the following commands to set up <code>docker</code> and <code>docker-compose</code>.</p> </li> </ol>  <pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10</pre><pre><code># Install and start the docker engine\nsudo yum install docker git -y\nsudo service docker start\nsudo usermod -a -G docker ec2-user\nsudo chkconfig docker on\n\n# Setup docker-compose\nsudo curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\nsudo mv /usr/local/bin/docker-compose /bin/docker-compose\n</code></pre>  <p>Then, configure the AWS CLI and Docker as described in Tower container images. The AWS CLI (v1) is pre-installed in Amazon Linux.</p>","title":"Amazon EC2"},{"location":"enterprise/prerequisites/azure/","tags":["azure","prerequisites","configuration"],"text":"<p>This page describes the infrastructure and other prerequisites for deploying Tower on Microsoft Azure.</p>","title":"Azure"},{"location":"enterprise/prerequisites/azure/#tower-container-images","tags":["azure","prerequisites","configuration"],"text":"<p>Nextflow Tower is distributed as a collection of Docker containers available through the Seqera Labs container registry (cr.seqera.io). Contact support to get your container access credentials. Once you have received your credentials, log in to the registry using these steps:</p> <ol> <li> <p>Retrieve the username and password you received from Seqera Labs support.</p> </li> <li> <p>Run the following Docker command to authenticate to the registry (using the <code>username</code> and <code>password</code> values copied in step 1):</p> <pre>1</pre><pre><code>docker login -u '&lt;USERNAME&gt;' -p '&lt;PASSWORD&gt;' cr.seqera.io\n</code></pre> </li> <li> <p>Pull the Nextflow Tower container images with the following commands:</p> </li> </ol> <pre>1\n2\n3</pre><pre><code>docker pull cr.seqera.io/private/nf-tower-enterprise/backend:v23.1.0\n\ndocker pull cr.seqera.io/private/nf-tower-enterprise/frontend:v23.1.0\n</code></pre>   <p>Warning</p> <p>The Seqera Labs container registry <code>cr.seqera.io</code> is the default Tower container image registry from version 22.4. Use of the AWS, Azure, and Google Cloud Tower image registries in existing installations is still supported but will be deprecated for new installations starting June 2023. See here for steps to use the Seqera Labs private Azure registry. </p>","title":"Tower container images"},{"location":"enterprise/prerequisites/azure/#mandatory-prerequisites","tags":["azure","prerequisites","configuration"],"text":"","title":"Mandatory prerequisites"},{"location":"enterprise/prerequisites/azure/#resource-group-and-storage-account","tags":["azure","prerequisites","configuration"],"text":"<p>A resource group and a storage account are required to use Azure. See the detailed instructions to provision these resources.</p>","title":"Resource group and storage account"},{"location":"enterprise/prerequisites/azure/#smtp-server","tags":["azure","prerequisites","configuration"],"text":"<p>If you do not have an email server, you can use Microsoft 365 or a third party service such as SendGrid (recommended by Microsoft) for sending emails from Azure VMs.</p>","title":"SMTP server"},{"location":"enterprise/prerequisites/azure/#mysql-database","tags":["azure","prerequisites","configuration"],"text":"<p>An external database (i.e. external to your Docker Compose or Kubernetes deployment) is highly recommended for production deployments. If you don't have your own database service, you can use Azure Database for MySQL.</p> <p>If you decide to use an external database, you must create a MySQL user and database manually. See Configuration for more details.</p>   <p>Note</p> <p>When creating a MySQL user, use the <code>USER@HOSTNAME</code> format for the <code>TOWER_DB_USER</code> environment variable.</p>   <p>Note</p> <p>For Azure managed MySQL, it is recommended to pass an explicit <code>serverTimezone</code> to the <code>TOWER_DB_URL</code> environment variable, which (depending on your configuration) may be <code>UTC</code>. The connection string should therefore look like <code>jdbc:mysql://MYSQL_INSTANCE_NAME.mysql.database.azure.com/TOWER_DATABASE?serverTimezone=UTC</code>.</p>","title":"MySQL database"},{"location":"enterprise/prerequisites/azure/#vm-instance-docker-compose","tags":["azure","prerequisites","configuration"],"text":"<p>A Linux VM instance is required to deploy Tower via Docker Compose. See the detailed instructions to provision a VM instance for this purpose.</p>","title":"VM instance (Docker Compose)"},{"location":"enterprise/prerequisites/azure/#aks-cluster-kubernetes","tags":["azure","prerequisites","configuration"],"text":"<p>An Azure Kubernetes Service (AKS) cluster is required to deploy Tower via Kubernetes. See the AKS documentation to provision your own cluster.</p>   <p>HTTPS redirects</p> <p>To customize your cluster's Ingress Controller to support HTTPS redirects and TLS certificates, see these instructions.</p>","title":"AKS cluster (Kubernetes)"},{"location":"enterprise/prerequisites/azure/#optional-prerequisites","tags":["azure","prerequisites","configuration"],"text":"","title":"Optional prerequisites"},{"location":"enterprise/prerequisites/azure/#ssl-certificate","tags":["azure","prerequisites","configuration"],"text":"<p>An SSL certificate is required for your Tower instance to handle HTTPS traffic.</p>   <p>Warning</p> <p>From Tower 22.1.1, HTTP-only implementations must set the <code>TOWER_ENABLE_UNSAFE_MODE=true</code> environment variable in the Tower hosting infrastructure to enable user login.</p>   <p>While there are many ways to implement DNS and TLS-termination, Seqera recommends using the specialized native services offered by your cloud provider. In the case of Azure:</p> <ul> <li>Use Application Gateway for TLS-termination and load-balancing.</li> <li>Use App Service Domains for domain acquisition.</li> <li>Use Azure DNS for domain record management.</li> <li>Use Azure Vault for PKI certificate storage.</li> </ul> <p>These decisions should be made before you continue as they impact how Tower configuration files are updated.</p>","title":"SSL certificate"},{"location":"enterprise/prerequisites/azure/#detailed-instructions","tags":["azure","prerequisites","configuration"],"text":"<p>This section provides step-by-step instructions for some commonly used Azure services for Tower deployment. See the Azure documentation for up-to-date instructions and contact Azure support if you have any issues with provisioning Azure resources.</p>","title":"Detailed instructions"},{"location":"enterprise/prerequisites/azure/#azure-resource-group","tags":["azure","prerequisites","configuration"],"text":"<ol> <li> <p>Sign in to the Azure portal.</p> </li> <li> <p>Select Resource groups.</p> </li> <li> <p>Select Add.</p> </li> <li> <p>Enter the following values:</p> <ul> <li> <p>Subscription: Select your Azure subscription.</p> </li> <li> <p>Resource group: Enter a new resource group name (e.g. <code>nftowerrg</code>).</p> </li> <li> <p>Region: Select the Region where your assets will exist (e.g. <code>East US</code>).</p> </li> </ul> </li> <li> <p>Select Review and Create.</p> </li> <li> <p>Select Create.</p> </li> </ol>","title":"Azure Resource Group"},{"location":"enterprise/prerequisites/azure/#azure-storage-account","tags":["azure","prerequisites","configuration"],"text":"<ol> <li> <p>Sign in to the Azure portal.</p> </li> <li> <p>Select Storage accounts.</p> </li> <li> <p>Select Create.</p> </li> <li> <p>Enter the following values:</p> <ul> <li> <p>Subscription: Select your Azure subscription.</p> </li> <li> <p>Resource group: Enter your resource group name.</p> </li> <li> <p>Storage account name: Enter a new storage account name (e.g. <code>nftowerstorage</code>).</p> </li> <li> <p>Region: Select the Region where your Resource Group exists (e.g. <code>East US</code>).</p> </li> <li> <p>Performance: Select <code>Standard</code>.</p> </li> <li> <p>Redundancy: Select <code>Geo-redundant storage (GRS)</code></p> </li> </ul> </li> <li> <p>Select Review + create. Note that the default values are used in the other tabs. See the Azure documentation for further details on each setting.</p> </li> <li> <p>Select Create.</p> </li> </ol>","title":"Azure Storage Account"},{"location":"enterprise/prerequisites/azure/#azure-linux-vm","tags":["azure","prerequisites","configuration"],"text":"<p>We recommend the following VM settings:</p> <ol> <li>Use default values unless otherwise specified.</li> <li>Provision at least 2 CPUS and 8GB RAM.</li> <li>Use the Ubuntu Server 20.04 LTS - Gen2 image.</li> <li>Ensure your VM is accessible by SSH.</li> <li>Do not implement DNS or Load Balancing directly against the VM (do so via Azure Application Gateway instead).</li> </ol> <p>To create a VM:</p> <ol> <li> <p>Configure the Basics tab:</p> <ul> <li>Ensure your Region is the same as your Resource group.</li> <li>Do not set the VM as an Azure Spot instance.</li> <li>Ensure your Security Group allows ingress on Port 8000.</li> </ul> </li> <li> <p>Configure the Disks tab:</p> <ul> <li>Ensure your OS disk type is Standard SSD.</li> </ul> </li> <li> <p>Configure the Network tab:</p> <ul> <li>Ensure that a Public IP is assigned to the VM.</li> <li>Do not place the VM in the backend pool of an existing load balancing solution.</li> </ul> </li> <li> <p>Select Review + create.</p> </li> <li> <p>Select Create.</p> </li> </ol> <p>To make the VM's IP address static:</p> <ol> <li> <p>Enter Public IP addresses in the search.</p> </li> <li> <p>Under Services, select Public IP addresses.</p> </li> <li> <p>On the Public IP addresses page, select the entry containing your VM name. A page opens with that IP's details.</p> </li> <li> <p>Select Configuration from the left-hand navigation panel.</p> </li> <li> <p>Ensure that your IP address assignment is Static.</p> </li> <li> <p>Do not add a custom DNS name label to the VM.</p> </li> </ol> <p>To allow ingress on port 8000:</p> <ol> <li> <p>Enter Virtual Machines in the search bar.</p> </li> <li> <p>Under Services, select Virtual machines.</p> </li> <li> <p>On the Virtual machines page, select your VM name to navigate to the VM details.</p> </li> <li> <p>Select Networking from the left-hand navigation panel.</p> </li> <li> <p>Add inbound port rule for port 8000.</p> </li> </ol>","title":"Azure Linux VM"},{"location":"enterprise/prerequisites/gcp/","tags":["gcp","prerequisites","configuration"],"text":"<p>This page describes the infrastructure and other prerequisites for deploying Tower on Google Cloud Platform (GCP).</p>","title":"GCP"},{"location":"enterprise/prerequisites/gcp/#tower-container-images","tags":["gcp","prerequisites","configuration"],"text":"<p>Nextflow Tower is distributed as a collection of Docker containers available through the Seqera Labs container registry <code>cr.seqera.io</code>. Contact support to get your container access credentials. Once you have received your credentials, log in to the registry using these steps:</p> <ol> <li> <p>Retrieve the username and password you received from Seqera Labs support.</p> </li> <li> <p>Run the following Docker command to authenticate to the registry (using the <code>username</code> and <code>password</code> values copied in step 1):</p> </li> </ol> <pre>1</pre><pre><code>docker login -u '&lt;USERNAME&gt;' -p '&lt;PASSWORD&gt;' cr.seqera.io\n</code></pre> <ol> <li>Pull the Nextflow Tower container images with the following commands:</li> </ol> <pre>1\n2\n3</pre><pre><code>docker pull cr.seqera.io/private/nf-tower-enterprise/backend:v23.1.0\n\ndocker pull cr.seqera.io/private/nf-tower-enterprise/frontend:v23.1.0\n</code></pre>   <p>Warning</p> <p>The Seqera Labs container registry <code>cr.seqera.io</code> is the default Tower container image registry from version 22.4. Use of the AWS, Azure, and Google Cloud Tower image registries in existing installations is still supported but will be deprecated for new installations starting June 2023. See here for steps to use the Seqera Labs private GCP Artifact Registry. </p>","title":"Tower container images"},{"location":"enterprise/prerequisites/gcp/#mandatory-prerequisites","tags":["gcp","prerequisites","configuration"],"text":"","title":"Mandatory prerequisites"},{"location":"enterprise/prerequisites/gcp/#smtp-server","tags":["gcp","prerequisites","configuration"],"text":"<p>If you do not have an email server, Google Cloud provides several ways to send emails:</p> <ul> <li> <p>Google Workspace:</p> <ul> <li>WorkSpace SMTP Relay</li> </ul> </li> <li> <p>Third-party services from the Google Cloud marketplace, including:</p> <ul> <li>SendGrid</li> <li>Mailgun</li> <li>Mailjet</li> </ul> </li> </ul> <p>Work with your IT team to select the best solution for your organization.</p>","title":"SMTP server"},{"location":"enterprise/prerequisites/gcp/#mysql-database","tags":["gcp","prerequisites","configuration"],"text":"<p>An external database (i.e. external to your Docker Compose or Kubernetes deployment) is highly recommended for production deployments. If you don't have your own database service, you can use Google CloudSQL.</p> <p>If you decide to use an external database, you must create a MySQL user and database manually. See Configuration for more details.</p>","title":"MySQL database"},{"location":"enterprise/prerequisites/gcp/#vm-instance-docker-compose","tags":["gcp","prerequisites","configuration"],"text":"<p>A Google Compute Engine (GCE) instance is required to deploy Tower via Docker Compose. See the detailed instructions to provision a VM instance for this purpose.</p>","title":"VM instance (Docker Compose)"},{"location":"enterprise/prerequisites/gcp/#gke-cluster-kubernetes","tags":["gcp","prerequisites","configuration"],"text":"<p>A Google Kubernetes Engine (GKE) cluster is required to deploy Tower via Kubernetes. See the GKE documentation to provision your own cluster.</p>   <p>Note</p> <p>GKE Autopilot is not currently supported by Tower due to a privilege issue with the Redis deployment. However, you can achieve most of the same behavior with a Standard cluster by enabling autoscaling and node auto-provisioning.</p>","title":"GKE cluster (Kubernetes)"},{"location":"enterprise/prerequisites/gcp/#optional-prerequisites","tags":["gcp","prerequisites","configuration"],"text":"","title":"Optional prerequisites"},{"location":"enterprise/prerequisites/gcp/#ssl-certificate","tags":["gcp","prerequisites","configuration"],"text":"<p>An SSL certificate is required for your Tower instance to handle HTTPS traffic.</p>   <p>Warning</p> <p>From Tower 22.1.1, HTTP-only implementations must set the <code>TOWER_ENABLE_UNSAFE_MODE=true</code> environment variable in the Tower hosting infrastructure to enable user login.</p>","title":"SSL certificate"},{"location":"enterprise/prerequisites/gcp/#public-ip-address","tags":["gcp","prerequisites","configuration"],"text":"<p>A public IP address can be reserved for the Tower ingress to keep the IP address constant across restarts. If you do not reserve an IP address, the ingress will create one for you automatically, but it will be different every time you deploy the ingress. See the detailed instructions to reserve a public IP address.</p> <ol> <li> <p>Browse to VPC network \u2192 External IP addresses and select Reserve Static Address</p> </li> <li> <p>Assign a name (e.g., <code>tower-ip</code>). This name will be used later to configure the ingress.</p> </li> <li> <p>Select the region where your GKE cluster is deployed.</p> </li> <li> <p>Select Reserve.</p> </li> </ol>","title":"Public IP address"},{"location":"enterprise/prerequisites/gcp/#detailed-instructions","tags":["gcp","prerequisites","configuration"],"text":"<p>This section provides step-by-step instructions for some commonly used GCP services for Tower deployment. See the GCP documentation for up-to-date instructions and contact GCP support if you have any issues with provisioning GCP resources.</p>","title":"Detailed instructions"},{"location":"enterprise/prerequisites/gcp/#google-cloudsql","tags":["gcp","prerequisites","configuration"],"text":"<ol> <li> <p>Browse to Cloud SQL and select Create Instance.</p> </li> <li> <p>Select MySQL (you may need to enable the API).</p> </li> <li> <p>Change to Single zone availability, unless there is a need for high availability.</p> </li> <li> <p>Update the Region and Zone to match the location of your Tower deployment.</p> </li> <li> <p>Expand Show configuration options and update the Machine type and Storage settings. The recommended machine type and disk size depends on the number of parallel pipelines you expect to run. In this guide, we use the Standard machine type with 1 vCPU, and 20 GB SSD storage.</p> </li> <li> <p>Expand Connections, disable Public IP, and enable Private IP.</p> </li> <li> <p>Select the Network (usually default). You may need to set up a Private services access connection for this VPC if you have not done so already. Enable the API and select Use an automatically allocated IP range. Select Continue, then Create Connection.</p> </li> <li> <p>Select Create Instance.</p> </li> <li> <p>Once the database has been created, select the instance, then Databases. Create a new database named tower.</p> </li> <li> <p>Note the Private IP address of the instance as it must be supplied to the <code>TOWER_DB_URL</code> environment variable.</p> </li> </ol>","title":"Google CloudSQL"},{"location":"enterprise/prerequisites/gcp/#google-compute-engine","tags":["gcp","prerequisites","configuration"],"text":"<ol> <li> <p>From the Navigation menu of the Google Cloud console, select Compute Engine to create a new VM instance. Select the machine name, region/zone, and machine type. In this example we have used an <code>e2-standard-2</code> instance (2 vCPUs, 8 GB memory). We recommend using the container-optimized OS for the VM.</p> </li> <li> <p>Enable HTTP traffic. By default, the frontend is exposed to port 8000, so you will need to add a firewall rule to the underlying VPC network to allow port 8000 (after VM creation).</p> </li> <li> <p>Connect to the machine using SSH. If you run into issues with SSH, or would like to set up IAP SSH, refer to the documentation for TCP forward to IAP.</p> </li> <li> <p>Install Docker if it is not already installed.</p> </li> <li> <p>Test Docker by running the Docker Compose image. If Docker does not have sufficient permissions, use these steps to run it without root, or use <code>sudo</code>.</p> <pre>1\n2\n3\n4\n5</pre><pre><code># test docker compose\ndocker run docker/compose:1.24.0 version\n\n# check that docker/compose image was pulled\ndocker images\n</code></pre> </li> <li> <p>Create an alias for <code>docker-compose</code>:</p> <pre>1\n2\n3\n4\n5\n6\n7</pre><pre><code>echo alias docker-compose=\"'\"'docker run --rm \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -v \"$PWD:$PWD\" \\\n    -w=\"$PWD\" \\\n    docker/compose:1.24.0'\"'\" &gt;&gt; ~/.bashrc\n\nsource .bashrc\n</code></pre> </li> <li> <p>Configure <code>gcloud</code> and Docker as described in Tower container images.</p> </li> </ol>","title":"Google Compute Engine"},{"location":"enterprise/prerequisites/on-prem/","tags":["on-prem","prerequisites","configuration"],"text":"<p>This page describes the prerequisites for deploying Tower to your on-premises infrastructure.</p>","title":"On-prem"},{"location":"enterprise/prerequisites/on-prem/#tower-container-images","tags":["on-prem","prerequisites","configuration"],"text":"<p>Nextflow Tower is distributed as a collection of Docker containers available through the Seqera Labs container registry <code>cr.seqera.io</code>. Contact support to get your container access credentials. Once you have received your credentials, log in to the registry using these steps:</p> <ol> <li> <p>Retrieve the username and password you received from Seqera Labs support.</p> </li> <li> <p>Run the following Docker command to authenticate to the registry (using the <code>username</code> and <code>password</code> values copied in step 1):</p> <pre>1</pre><pre><code>docker login -u '&lt;USERNAME&gt;' -p '&lt;PASSWORD&gt;' cr.seqera.io\n</code></pre> </li> <li> <p>Pull the Nextflow Tower container images with the following commands:</p> <pre>1\n2\n3</pre><pre><code>docker pull cr.seqera.io/private/nf-tower-enterprise/backend:v23.1.0\n\ndocker pull cr.seqera.io/private/nf-tower-enterprise/frontend:v23.1.0\n</code></pre> </li> </ol>  <p>Warning</p> <p>The Seqera Labs container registry <code>cr.seqera.io</code> is the default Tower container image registry from version 22.4. Use of the AWS, Azure, and Google Cloud Tower image registries in existing installations is still supported but will be deprecated for new installations starting June 2023.</p>  <p>See Docker Compose for instructions to deploy Tower via Docker Compose.</p> <p>See Kubernetes for instructions to deploy Tower via Kubernetes.</p>","title":"Tower container images"},{"location":"enterprise/prerequisites/on-prem/#mandatory-prerequisites","tags":["on-prem","prerequisites","configuration"],"text":"","title":"Mandatory prerequisites"},{"location":"enterprise/prerequisites/on-prem/#smtp-server","tags":["on-prem","prerequisites","configuration"],"text":"<p>An SMTP server is required to send emails from Tower.</p> <p>If you don't have your own mail server, you can use an external service from a cloud provider. Visit the corresponding Prerequisites page for more information and consult your IT team to select the most suitable solution for your organization.</p>","title":"SMTP server"},{"location":"enterprise/prerequisites/on-prem/#mysql-database","tags":["on-prem","prerequisites","configuration"],"text":"<p>An external database (i.e. external to your Docker Compose or Kubernetes deployment) is highly recommended for production deployments. If you don't have your own database service, you can use an external service from a cloud providers. Visit the corresponding Prerequisites page for more information and consult your IT team to select the most suitable solution for your organization.</p> <p>If you decide to use an external database, you must create a MySQL user and database manually. See Configuration for more details.</p>","title":"MySQL database"},{"location":"enterprise/prerequisites/on-prem/#optional-prerequisites","tags":["on-prem","prerequisites","configuration"],"text":"","title":"Optional prerequisites"},{"location":"enterprise/prerequisites/on-prem/#ssl-certificate","tags":["on-prem","prerequisites","configuration"],"text":"<p>An SSL certificate is required for your Tower instance to handle HTTPS traffic.</p>  <p>Warning</p> <p>From Tower 22.1.1, HTTP-only implementations must set the <code>TOWER_ENABLE_UNSAFE_MODE=true</code> environment variable in the Tower hosting infrastructure to enable user login.</p>","title":"SSL certificate"},{"location":"enterprise/release_notes/20.06/","text":"","title":"Release notes for version 20.06"},{"location":"enterprise/release_notes/20.06/#highlights","text":"<p>Tower 20.06.0 introduces the support for:</p> <ul> <li>Pipelines launch</li> <li>Execution cancellation</li> <li>Git private repositories</li> <li>User credentials management </li> <li>UI looks &amp; feel improvements</li> <li>System security improvements</li> <li>OAuth login improvements</li> <li>Improve DB connection pool</li> <li>Upgrade Micronaut runtime to 1.3.3</li> </ul>","title":"Highlights"},{"location":"enterprise/release_notes/20.06/#updating-tower-deployment-from-version-2005x-to-2006x","text":"<p>This Tower version requires a database schema update. Follow this steps  to update your DB instance and the Tower installation. </p>","title":"Updating Tower deployment from version 20.05.x to 20.06.x"},{"location":"enterprise/release_notes/20.06/#kubernetes-based-deployment","text":"<p>1. Update the Tower container images in your Kubernetes manifest yaml files to: </p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v20.06.1\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v20.06.1\n</code></pre> <p>Please refer to the manifests included in the Kubernetes section for details. </p> <p>2. Set a proper value for the <code>TOWER_CRYPTO_SECRETKEY</code> environment variable in the <code>configmap.yml</code>  and deploy it using the command: </p> <pre>1</pre><pre><code>kubectl apply -f configmap.yml\n</code></pre> <p>3. Update the Tower cron service using the following: </p> <pre>1</pre><pre><code>kubectl apply -f tower-cron.yml\n</code></pre>  <p>Note</p> <p>This task will automatically run the Tower database schema update tool. </p>  <p>4. Update the Tower backend and frontend services using the following command: </p> <pre>1</pre><pre><code>kubectl apply -f tower-svc.yml\n</code></pre>","title":"Kubernetes based deployment"},{"location":"enterprise/release_notes/20.06/#custom-deployment-script","text":"<p>1. Pull or update the Tower container images references in your  deployment script(s) to:</p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v20.06.1\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v20.06.1\n</code></pre> <p>2. Make sure to add the following new variable in the backend container environment:</p> <ul> <li><code>TOWER_CRYPTO_SECRETKEY</code>: See the configuration section for details. </li> </ul> <p>3. Update the Tower database schema running the <code>/migrate-db.sh</code> provided in the   backend container. </p>  <p>Note</p> <p>Make sure to include the identical environment as for the normal backend execution. </p>  <p>4. Once the schema update completes, deploy Tower as usual procedure. </p>","title":"Custom deployment script"},{"location":"enterprise/release_notes/20.06/#questionsfeedback","text":"<p>Contact us at support@seqera.io. </p>","title":"Questions/Feedback"},{"location":"enterprise/release_notes/20.08/","text":"","title":"Release notes for version 20.08"},{"location":"enterprise/release_notes/20.08/#highlights","text":"<p>Tower 20.08.0 introduces the support for:</p> <ul> <li>Pipelines Actions</li> <li>Forge for AWS Batch</li> <li>Execution and task logs view &amp; download</li> <li>Advanced config options for AWS Batch</li> <li>GA4GH WES API (beta)</li> <li>UI looks &amp; feel improvements</li> <li>System security improvements</li> <li>Upgrade Micronaut runtime to 1.3.7</li> </ul>","title":"Highlights"},{"location":"enterprise/release_notes/20.08/#updating-tower-deployment-from-version-2006x-to-2008x","text":"<p>This Tower version requires a database schema update. Follow this steps  to update your DB instance and the Tower installation. </p>","title":"Updating Tower deployment from version 20.06.x to 20.08.x"},{"location":"enterprise/release_notes/20.08/#kubernetes-based-deployment","text":"<p>1. Update the Tower container images in your Kubernetes manifest yaml files to: </p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v20.08.0\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v20.08.0\n</code></pre> <p>Please refer the manifests included in the Kubernetes section for details. </p> <p>2. Update the Tower cron service using the following: </p> <pre>1</pre><pre><code>kubectl apply -f tower-cron.yml\n</code></pre>  <p>Note</p> <p>This task will automatically run the Tower database schema update tool. </p>  <p>3. Update the Tower backend and frontend services using the following command: </p> <pre>1</pre><pre><code>kubectl apply -f tower-svc.yml\n</code></pre>","title":"Kubernetes based deployment"},{"location":"enterprise/release_notes/20.08/#custom-deployment-script","text":"<p>1. Pull or update the Tower container images references in your  deployment script(s) to:</p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v20.08.0\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v20.08.0\n</code></pre> <p>2. Update the Tower database schema running the <code>/migrate-db.sh</code> provided in the   backend container. </p>  <p>Note</p> <p>Make sure to include the identical environment as for the normal backend execution. </p>  <p>3. Once the schema update completes, deploy Tower as usual procedure. </p>","title":"Custom deployment script"},{"location":"enterprise/release_notes/20.08/#questionsfeedback","text":"<p>Contact us at support@seqera.io. </p>","title":"Questions/Feedback"},{"location":"enterprise/release_notes/20.10/","text":"","title":"Release notes for version 20.10"},{"location":"enterprise/release_notes/20.10/#highlights","text":"<p>Tower 20.10.x introduces support for:</p> <ul> <li>Add Workflow sharing feature</li> <li>Add support for Slurm batch cluster </li> <li>Add support for IBM LSF batch cluster</li> <li>Add customizable navbar menu </li> <li>Add built-in support for MariaDB</li> <li>Add built-in support for Google SSO </li> <li>Add auth allow-list emails</li> <li>Update Java mail 1.6.2</li> <li>System security improvements</li> </ul>","title":"Highlights"},{"location":"enterprise/release_notes/20.10/#updating-tower-deployment-from-version-2008x-to-2010x","text":"","title":"Updating Tower deployment from version 20.08.x to 20.10.x"},{"location":"enterprise/release_notes/20.10/#compute-environments","text":"<p>It is now required to define the Compute environments that are to be available to users in the Tower configuration.</p> <p>The following ids options are available: </p> <ul> <li><code>awsbatch-platform</code>: AWS Batch cloud compute service</li> <li><code>gls-platform</code>: Google LifeSciences cloud compute service</li> <li><code>lsf-platform</code>: IBM LSF batch scheduler </li> <li><code>slurm-platform</code>: Slurm batch scheduler</li> </ul> <p>Choose one or more of these platform ids and append to your current <code>MICRONAUT_ENVIRONMENTS</code>  variable, separating them via a comma.</p>","title":"Compute environments"},{"location":"enterprise/release_notes/20.10/#database-schema","text":"<p>This Tower version requires a database schema update. Follow these steps  to update your DB instance and the Tower installation. </p>","title":"Database schema"},{"location":"enterprise/release_notes/20.10/#kubernetes-based-deployment","text":"<p>1. Update the Tower container images in the Kubernetes manifest yaml files to: </p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v20.10.2\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v20.10.2\n</code></pre> <p>Refer to the manifests included in the Kubernetes section for details. </p> <p>2. Update the Tower cron service using the following: </p> <pre>1</pre><pre><code>kubectl apply -f tower-cron.yml\n</code></pre>  <p>Note</p> <p>This task will automatically run the Tower database schema update tool. </p>  <p>3. Update the Tower backend and frontend services using the following command: </p> <pre>1</pre><pre><code>kubectl apply -f tower-svc.yml\n</code></pre>","title":"Kubernetes based deployment"},{"location":"enterprise/release_notes/20.10/#custom-deployment-script","text":"<p>1. Pull or update the Tower container images references in your  deployment script(s) to:</p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v20.10.2\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v20.10.2\n</code></pre> <p>2. Update the Tower database schema by running the <code>/migrate-db.sh</code> provided in the   backend container.</p>  <p>Note</p> <p>Make sure to include the identical environment as used in the normal backend execution. </p>  <p>3. Once the schema update completes, deploy Tower according to the usual procedure. </p>","title":"Custom deployment script"},{"location":"enterprise/release_notes/20.10/#questionsfeedback","text":"<p>Contact us at support@seqera.io. </p>","title":"Questions/Feedback"},{"location":"enterprise/release_notes/20.12/","text":"","title":"Release notes for version 20.12"},{"location":"enterprise/release_notes/20.12/#highlights","text":"<p>Tower 20.12.x introduces support for:</p> <ul> <li>Add support for Kubernetes clusters </li> <li>Add support for AWS EKS clusters </li> <li>Add support for Google Cloud GKE clusters </li> <li>Add support for Launch stub-run feature</li> <li>Add AWS Batch Fusion mounts </li> <li>Enhanced security, API uses HTTP bearer auth token </li> <li>System security improvements </li> <li>Upgrade Java runtime to version 11 </li> <li>Upgrade Micronaut runtime to version 2.1</li> <li>Upgrade Nextflow launcher to version 20.12.0-edge</li> </ul>","title":"Highlights"},{"location":"enterprise/release_notes/20.12/#updating-tower-deployment-from-version-2010x-to-2012x","text":"<p>New token format</p> <p>If you are using the Tower API, this version Tower supports HTTP Bearer authentication. This means newly generated user access tokens should use the <code>Authentication: Bearer</code> header. HTTP Basic authentication is still supported for backwards compatibility with existing tokens, even though we suggest upgrading to the new authentication scheme. Find more details in the Tower documentation.</p>","title":"Updating Tower deployment from version 20.10.x to 20.12.x"},{"location":"enterprise/release_notes/20.12/#compute-environments","text":"<p>It is now required to define the Compute environments that are to be available  to users in the Tower configuration.</p> <p>The following ids options are available: </p> <ul> <li><code>awsbatch-platform</code>: AWS Batch cloud compute service</li> <li><code>gls-platform</code>: Google LifeSciences cloud compute service</li> <li><code>lsf-platform</code>: IBM LSF batch scheduler </li> <li><code>slurm-platform</code>: Slurm batch scheduler</li> <li><code>k8s-platform</code>: Kubernetes compute platform</li> <li><code>eks-platform</code>: AWS EKS compute platform</li> <li><code>gke-platform</code>: Google GKE compute platform</li> </ul> <p>Choose one or more of these platform ids and append to your current <code>MICRONAUT_ENVIRONMENTS</code> variable, separating them via a comma.</p>","title":"Compute environments"},{"location":"enterprise/release_notes/20.12/#database-schema","text":"<p>This Tower version requires a database schema update. Follow these steps  to update your DB instance and the Tower installation. </p>","title":"Database schema"},{"location":"enterprise/release_notes/20.12/#kubernetes-based-deployment","text":"<p>1. Update the Tower container images in the Kubernetes manifest yaml files to: </p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v20.12.1\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v20.12.1\n</code></pre> <p>If you are using AWS Batch with a custom launcher job definition you need to update it to use the following container image (please refer the configuration section for details): </p> <pre>1</pre><pre><code>public.ecr.aws/e2x4u7r1/tower/nf-launcher:20.12.0-edge\n</code></pre> <p>Refer to the manifests included in the Kubernetes section for details. </p> <p>2. Update the Tower cron service using the following: </p> <pre>1</pre><pre><code>kubectl apply -f tower-cron.yml\n</code></pre>  <p>Note</p> <p>This task will automatically run the Tower database schema update tool. </p>  <p>3. Update the Tower backend and frontend services using the following command: </p> <pre>1</pre><pre><code>kubectl apply -f tower-svc.yml\n</code></pre>","title":"Kubernetes based deployment"},{"location":"enterprise/release_notes/20.12/#custom-deployment-script","text":"<p>1. Pull or update the Tower container images references in your  deployment script(s) to:</p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v20.12.1\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v20.12.1\n</code></pre> <p>If you are using AWS Batch with a custom launcher job definition you need to update it to use the following container image (please refer the configuration section for details): </p> <pre>1</pre><pre><code>public.ecr.aws/e2x4u7r1/tower/nf-launcher:20.12.0-edge\n</code></pre> <p>2. Update the Tower database schema by running the <code>/migrate-db.sh</code> provided in the   backend container. </p>  <p>Note</p> <p>Make sure to include the identical environment as used in the normal backend execution. </p>  <p>3. Once the schema update completes, deploy Tower following your usual procedure. </p>","title":"Custom deployment script"},{"location":"enterprise/release_notes/20.12/#questionsfeedback","text":"<p>Contact us at support@seqera.io. </p>","title":"Questions/Feedback"},{"location":"enterprise/release_notes/21.02/","text":"","title":"Release notes for version 21.02"},{"location":"enterprise/release_notes/21.02/#highlights","text":"<p>Tower 21.02.x introduces support for:</p> <ul> <li>Add Azure Batch provider </li> <li>Add Altair PBS pro provider </li> <li>Add sessionId to workflows search-box criteria</li> <li>Add support for multiple GLS zones</li> <li>Add Grid provider head job options </li> <li>Add support for AWS Batch cost percentage </li> <li>Add Azure Batch Forge</li> <li>Add support for Grid Engine batch scheduler </li> <li>Add Kubernetes service pod </li> <li>Add support for Tower license </li> <li>Improve detection of NF config profiles #1074</li> <li>Fix issue on work dir path composition with ending slash</li> <li>Fix issue when retrieving non-existing file via SSH/SCP </li> <li>Fix issue resolving non-canonical GitHub/Gitlab project name #353</li> <li>Fix issue with AWS Batch allocation strategy #931</li> <li>Fix job phantom unknown status</li> <li>Fix Prevent requeue mail with invalid addresses</li> <li>Fix issue on creating AWS CE with manual config</li> <li>Update backend base image to corretto:11.0.10</li> <li>Bump NF 21.03.0-edge</li> <li>Upgrade to Angular 11</li> <li>Use Kubernetes Java-client 10.0.1 </li> <li>Upgrade Nextflow runtime to 21.03.0-edge</li> </ul>","title":"Highlights"},{"location":"enterprise/release_notes/21.02/#updating-tower-deployment-from-version-2012x-to-2102x","text":"","title":"Updating Tower deployment from version 20.12.x to 21.02.x"},{"location":"enterprise/release_notes/21.02/#license-key","text":"<p>As of this version, a license key must be provided to enable the Tower  deployment feature. The license key should be specified using the configuration  variable <code>TOWER_LICENSE</code>. </p>  <p>Warning</p> <p>If you don't have a license key, contact sales@seqera.io. </p>","title":"License key"},{"location":"enterprise/release_notes/21.02/#compute-environments","text":"<p>The Tower compute environments that are available  to users must be specified in the Tower configuration.</p> <p>The following ids options are available: </p> <ul> <li><code>awsbatch-platform</code>: AWS Batch cloud compute service</li> <li><code>gls-platform</code>: Google LifeSciences cloud compute service</li> <li><code>azbatch-platform</code>: Azure Batch cloud compute service </li> <li><code>lsf-platform</code>: IBM LSF batch scheduler </li> <li><code>slurm-platform</code>: Slurm batch scheduler</li> <li><code>altair-platform</code>: Altair PBS Pro batch scheduler</li> <li><code>univa-platform</code>: (Univa/Sun) GridEngine </li> <li><code>k8s-platform</code>: Kubernetes compute platform</li> <li><code>eks-platform</code>: AWS EKS compute platform</li> <li><code>gke-platform</code>: Google GKE compute platform</li> </ul> <p>Choose one or more of these platform ids and append to your current <code>MICRONAUT_ENVIRONMENTS</code> variable, separating them via a comma.</p>","title":"Compute environments"},{"location":"enterprise/release_notes/21.02/#database-schema","text":"<p>This Tower version requires a database schema update. Follow these steps  to update your DB instance and the Tower installation. </p>","title":"Database schema"},{"location":"enterprise/release_notes/21.02/#kubernetes-based-deployment","text":"<p>1. Update the Tower container images in the Kubernetes manifest yaml files to: </p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v21.02.5\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v21.02.5\n</code></pre> <p>If you are using AWS Batch with a custom launcher job definition you need to update it to use the following container image (please refer the configuration section for details): </p> <pre>1</pre><pre><code>public.ecr.aws/seqera-labs/tower/nf-launcher:21.04.0-edge\n</code></pre> <p>Refer to the manifests included in the Kubernetes section for details. </p> <p>2. Update the Tower cron service using the following: </p> <pre>1</pre><pre><code>kubectl apply -f tower-cron.yml\n</code></pre>  <p>Note</p> <p>This task will automatically run the Tower database schema update tool. </p>  <p>3. Update the Tower backend and frontend services using the following command: </p> <pre>1</pre><pre><code>kubectl apply -f tower-svc.yml\n</code></pre>","title":"Kubernetes based deployment"},{"location":"enterprise/release_notes/21.02/#custom-deployment-script","text":"<p>1. Pull or update the Tower container images references in your deployment script(s) to:</p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v21.02.5\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v21.02.5\n</code></pre> <p>If you are using AWS Batch with a custom launcher job definition you need to update it to use the following container image (please refer the configuration section for details): </p> <pre>1</pre><pre><code>public.ecr.aws/seqera-labs/tower/nf-launcher:21.04.0-edge\n</code></pre> <p>2. Update the Tower database schema by running the <code>/migrate-db.sh</code> provided in the   backend container. </p>  <p>Note</p> <p>Make sure to include the identical environment as used in the normal backend execution. </p>  <p>3. Once the schema update completes, deploy Tower following your usual procedure. </p>","title":"Custom deployment script"},{"location":"enterprise/release_notes/21.02/#questionsfeedback","text":"<p>Contact us at support@seqera.io. </p>","title":"Questions/Feedback"},{"location":"enterprise/release_notes/21.04/","text":"","title":"Release notes for version 21.04"},{"location":"enterprise/release_notes/21.04/#highlights","text":"<p>Tower 21.04.x brings the following changes:</p> <ul> <li>New Organizations feature</li> <li>New Teams feature</li> <li>New Workspace feature</li> <li>New Launchpad feature</li> <li>Add support for private Git repositories</li> <li>Add support for downloading Nextflow timelines</li> <li>Fix issues with Compute environment status reporting</li> <li>Update Nextflow runtime to version 21.04.0</li> </ul>","title":"Highlights"},{"location":"enterprise/release_notes/21.04/#updating-tower-deployment-from-version-2102x-to-2104x","text":"","title":"Updating Tower deployment from version 21.02.x to 21.04.x"},{"location":"enterprise/release_notes/21.04/#license-key","text":"<p>As of version <code>21.02.x</code>, a license key must be provided to enable the Tower  deployment feature. The license key should be specified using the configuration  variable <code>TOWER_LICENSE</code>. If you don't have a license key, contact sales@seqera.io. </p>","title":"License key"},{"location":"enterprise/release_notes/21.04/#compute-environments","text":"<p>The Tower compute environments to be made available  to users must be specified in the Tower configuration.</p> <p>The following ids options are available: </p> <ul> <li><code>awsbatch-platform</code>: AWS Batch cloud compute service</li> <li><code>gls-platform</code>: Google LifeSciences cloud compute service</li> <li><code>azbatch-platform</code>: Azure Batch cloud compute service </li> <li><code>lsf-platform</code>: IBM LSF batch scheduler </li> <li><code>slurm-platform</code>: Slurm batch scheduler</li> <li><code>altair-platform</code>: Altair PBS Pro batch scheduler</li> <li><code>univa-platform</code>: (Univa/Sun) GridEngine </li> <li><code>k8s-platform</code>: Kubernetes compute platform</li> <li><code>eks-platform</code>: AWS EKS compute platform</li> <li><code>gke-platform</code>: Google GKE compute platform</li> </ul> <p>Choose one or more of these platform ids and append to your current <code>MICRONAUT_ENVIRONMENTS</code>  variable, separating them via a comma.</p>","title":"Compute environments"},{"location":"enterprise/release_notes/21.04/#database-schema","text":"<p>This Tower version requires a database schema update which requires the scheduling of several minutes of service downtime. </p> <p>PLEASE FOLLOW THESE STEPS TO UPGRADE YOUR TOWER VERSION: </p> <ul> <li>Stop the current Tower service </li> <li>Make a full database backup</li> <li>Update your environment with the new Tower enterprise containers</li> <li>Restore the service</li> </ul> <p>Find below more detailed instructions for your reference deployment.</p>","title":"Database schema"},{"location":"enterprise/release_notes/21.04/#kubernetes-based-deployment","text":"<p>1. Stop the Tower service using the following command:</p> <pre>1\n2\n3</pre><pre><code>kubectl delete deployment frontend\nkubectl delete deployment backend\nkubectl delete deployment cron\n</code></pre> <p>2. Update the Tower container images in the Kubernetes manifest yaml files to: </p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v21.04.9\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v21.04.9\n</code></pre> <p>If you are using AWS Batch with a custom launcher job definition, you will need to update it to use the following container image (please refer the configuration section for details): </p> <pre>1</pre><pre><code>public.ecr.aws/seqera-labs/tower/nf-launcher:21.04.0\n</code></pre> <p>Refer to the manifests included in the Kubernetes section for details. </p> <p>3. Apply the changes for the Tower cron service using the following command: </p> <pre>1</pre><pre><code>kubectl apply -f tower-cron.yml\n</code></pre>  <p>Note</p> <p>This task will automatically run the Tower database schema update tool. It may take some minutes to complete.</p>  <p>4. Once the <code>cron</code> container is in <code>running</code> status, deploy the Tower backend and frontend services using the following command: </p> <pre>1</pre><pre><code>kubectl apply -f tower-svc.yml\n</code></pre>","title":"Kubernetes based deployment"},{"location":"enterprise/release_notes/21.04/#docker-compose-deployment","text":"<p>1. Stop the current deployment using the command <code>docker-compose stop</code>. 2. Make backup of the Tower database.  3. Update the docker-compose.yml file with the following container images:</p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v21.04.9\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v21.04.9\n</code></pre> <p>4. Restart the service using the command <code>docker-compose start</code>.</p>","title":"Docker compose deployment"},{"location":"enterprise/release_notes/21.04/#custom-deployment-script","text":"<p>1. Stop the current Tower deployment. 2. Make a Tower database backup. 3. Pull or update the Tower container images references in your  deployment script(s) to:</p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v21.04.9\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v21.04.9\n</code></pre> <p>If you are using AWS Batch with a custom launcher job definition you need to update it to use the following container image (please refer the configuration section for details): </p> <pre>1</pre><pre><code>public.ecr.aws/seqera-labs/tower/nf-launcher:21.04.0\n</code></pre> <p>4. Update the Tower database schema by running the <code>/migrate-db.sh</code> provided in the backend container. </p>  <p>Note</p> <p>Make sure to include the identical environment as used in the normal backend execution. </p>  <p>5. Once the schema update completes, deploy Tower following your usual procedure. </p>","title":"Custom deployment script"},{"location":"enterprise/release_notes/21.04/#questionsfeedback","text":"<p>Contact us at support@seqera.io. </p>","title":"Questions/Feedback"},{"location":"enterprise/release_notes/21.06/","text":"","title":"Release notes for version 21.06"},{"location":"enterprise/release_notes/21.06/#highlights","text":"<p>Tower 21.06.x brings the following changes:</p> <ul> <li>Add Support for AWS Host credentials and role-base permissions</li> <li>Add Support for AWS EFS storage</li> <li>Add ability to specify custom AWS cli path</li> <li>Add AWS regions eu-south-1 and af-south-1</li> <li>Add uploadChunkSize configuration parameter to abstract k8 provider (#1820)</li> <li>Limit compute env error message length</li> <li>Invalidate compute envs associated to deleted credentials</li> <li>Fix launch form pipelineParameters after navigating to pipeline input form (#1847)</li> <li>Fix error report for missing invalid/creds</li> <li>Fix GitHub action creation</li> <li>Fix Prevent GH delete action hook exception</li> <li>Display team id in team page</li> <li>Disable index.html caching in nginx.config</li> <li>Bump nextflow launcher 21.04.3</li> <li>Bump groovy 3.0.8</li> </ul>","title":"Highlights"},{"location":"enterprise/release_notes/21.06/#updating-tower-deployment-from-version-2104x-to-2106x","text":"<p>NOTE: If you are upgrading from a verion prior to <code>21.04.x</code>, update your installation to tower <code>21.04.0</code>, before installing this release. </p>","title":"Updating Tower deployment from version 21.04.x to 21.06.x"},{"location":"enterprise/release_notes/21.06/#license-key","text":"<p>As of version <code>21.02.x</code>, a license key must be provided to enable the Tower deployment feature. The license key should be specified using the configuration variable TOWER_LICENSE. If you don't have a license key, contact sales@seqera.io. </p>","title":"License key"},{"location":"enterprise/release_notes/21.06/#compute-environments","text":"<p>The Tower compute environments to be made available  to users must be specified in the Tower configuration.</p> <p>The following ids options are available: </p> <ul> <li><code>awsbatch-platform</code>: AWS Batch cloud compute service</li> <li><code>gls-platform</code>: Google LifeSciences cloud compute service</li> <li><code>azbatch-platform</code>: Azure Batch cloud compute service </li> <li><code>lsf-platform</code>: IBM LSF batch scheduler </li> <li><code>slurm-platform</code>: Slurm batch scheduler</li> <li><code>altair-platform</code>: Altair PBS Pro batch scheduler</li> <li><code>univa-platform</code>: (Univa/Sun) GridEngine </li> <li><code>k8s-platform</code>: Kubernetes compute platform</li> <li><code>eks-platform</code>: AWS EKS compute platform</li> <li><code>gke-platform</code>: Google GKE compute platform</li> </ul> <p>Choose one or more of these platform ids and append to your current <code>MICRONAUT_ENVIRONMENTS</code> variable, separating them via a comma <code>,</code>.</p>","title":"Compute environments"},{"location":"enterprise/release_notes/21.06/#database-schema","text":"<p>This Tower version requires a database schema update. Follow these steps to update your DB instance and the Tower installation.</p>","title":"Database schema"},{"location":"enterprise/release_notes/21.06/#docker-compose-deployment","text":"<p>1. Make a backup of the Tower database.</p> <p>2. Update the <code>docker-compose.yml</code> file with the following container images:</p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v21.06.2\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v21.06.2\n</code></pre> <p>If you are using AWS Batch with a custom launcher job definition you need to update it to use the following container images (please refer the configuration section for details):</p> <pre>1</pre><pre><code>public.ecr.aws/seqera-labs/tower/nf-launcher:21.04.3\n</code></pre> <p>3. Restart the service using the command <code>docker-compose restart</code>.</p>","title":"Docker compose deployment"},{"location":"enterprise/release_notes/21.06/#kubernetes-based-deployment","text":"<p>1. Make a backup of the Tower database.</p> <p>2. Update the Tower container images in the Kubernetes manifest yaml files to:</p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v21.06.2\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v21.06.2\n</code></pre> <p>If you are using AWS Batch with a custom launcher job definition you need to update it to use the following container image (please refer the configuration section for details):</p> <pre>1</pre><pre><code>public.ecr.aws/seqera-labs/tower/nf-launcher:21.04.3\n</code></pre> <p>Refer to the manifests included in the K8s instalation section for details.</p> <p>3. Update the Tower cron service using the following:</p> <pre>1</pre><pre><code>kubectl apply -f tower-cron.yml\n</code></pre>  <p>Note</p> <p>This task will automatically run the Tower database schema update tool.</p>  <p>4. Update the Tower backend and frontend services using the following command:</p> <pre>1</pre><pre><code>kubectl apply -f tower-svc.yml\n</code></pre>","title":"Kubernetes based deployment"},{"location":"enterprise/release_notes/21.06/#custom-deployment-script","text":"<p>1. Make a backup of the Tower database.</p> <p>2. Pull or update the Tower container images references in your    deployment script(s) to:</p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v21.06.2\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v21.06.2\n</code></pre> <p>If you are using AWS Batch with a custom launcher job definition you need to update it to use the following container image (please refer the configuration section for details):</p> <pre>1</pre><pre><code>public.ecr.aws/seqera-labs/tower/nf-launcher:21.04.3\n</code></pre> <p>3. Update the Tower database schema by running the <code>/migrate-db.sh</code> provided in the    backend container. </p>  <p>Note</p> <p>Make sure to include the identical environment as used in the normal backend execution.</p>  <p>4. Once the schema update completes, deploy Tower following your usual procedure.</p>","title":"Custom deployment script"},{"location":"enterprise/release_notes/21.06/#questionsfeedback","text":"<p>Contact us at support@seqera.io. </p>","title":"Questions/Feedback"},{"location":"enterprise/release_notes/21.10/","text":"<p>Nextflow Tower 21.10.x brings the following changes:</p> <ul> <li>Add Container registry creds for Azure</li> <li>Add Datasets feature</li> <li>Add Support custom CE environment variables</li> <li>New Workflows Runs list page</li> <li>Add support for custom landing page</li> <li>Add Display job info on workflow general panel (#2142) (#2151)</li> <li>Make hidden params a part of pipeline input form even if not shown + small fix (#2134)</li> <li>Parallelize Az metadata retrieval</li> <li>Fix Make hidden params a part of pipeline input form even if not shown + small fix (#2134)</li> <li>Fix Validate final values of config properties on startup (#2100)</li> <li>Fix redisson default connection pool size (#2229)</li> <li>Fix Return a bad request when workspaceId is not parsable (#2220) (#2205)</li> <li>Fix race condition on repo pull (#2110)</li> <li>Fix grid platform default launch dir (#2037)</li> <li>Fix Redirect to the Runs page after launch (#2057)</li> <li>Fix Discard deleted entities from name validation queries and rename them (#2052)</li> <li>Improve landing page config (#1996) (#748)</li> <li>Fix Download hangs when streaming a S3 file (#2005)</li> <li>Hide ebsBlockSize field from aws manual config (#2004)</li> <li>Refactor Google LifeScience head job execution (#1981)</li> <li>Make sure to authenticate the Google storage (#1984)</li> <li>Use amazoncorretto:11.0.13 as base image</li> <li>Minor schema fetching improvement (#2183)</li> <li>Make sure the workflows list query returns the workflows in a workspace even if they have been starred by other users (#2174)</li> <li>Bump nf-launcher 21.10.4</li> </ul>","title":"Release notes for version 21.10.1"},{"location":"enterprise/release_notes/21.10/#updating-tower-deployment-from-version-2106x-to-2110x","text":"<p>If you are upgrading from a version prior to <code>21.06.x</code>, update your installation to tower <code>21.04.0</code>, before installing this release. </p>","title":"Updating Tower deployment from version 21.06.x to 21.10.x"},{"location":"enterprise/release_notes/21.10/#license-key","text":"<p>As of version <code>21.02.x</code>, a license key must be provided to enable the Tower deployment feature. The license key should be specified using the configuration variable <code>TOWER_LICENSE</code>. If you don't have a license key, contact sales@seqera.io. </p>","title":"License key"},{"location":"enterprise/release_notes/21.10/#compute-environments","text":"<p>The Tower compute environments to be made available  to users must be specified in the Tower configuration.</p> <p>The following options are available: </p> <ul> <li><code>awsbatch-platform</code>: AWS Batch cloud compute service</li> <li><code>gls-platform</code>: Google LifeSciences cloud compute service</li> <li><code>azbatch-platform</code>: Azure Batch cloud compute service </li> <li><code>lsf-platform</code>: IBM LSF batch scheduler </li> <li><code>slurm-platform</code>: Slurm batch scheduler</li> <li><code>altair-platform</code>: Altair PBS Pro batch scheduler</li> <li><code>univa-platform</code>: (Univa/Sun) GridEngine </li> <li><code>k8s-platform</code>: Kubernetes compute platform</li> <li><code>eks-platform</code>: AWS EKS compute platform</li> <li><code>gke-platform</code>: Google GKE compute platform</li> </ul>","title":"Compute environments"},{"location":"enterprise/release_notes/21.10/#database-schema","text":"<p>This Tower version requires a database schema update. Follow these steps to update your DB instance and the Tower installation.</p>","title":"Database schema"},{"location":"enterprise/release_notes/21.10/#docker-compose-deployment","text":"<p>1. Make a backup of the Tower database.</p> <p>2. Update the <code>docker-compose.yml</code> file with the following container images:</p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v21.10.3\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v21.10.3\n</code></pre> <p>If you are using AWS Batch with a custom launcher job definition you need to update it to use the following container images (please refer the configuration section for details):</p> <pre>1</pre><pre><code>public.ecr.aws/seqera-labs/tower/nf-launcher:21.10.5\n</code></pre> <p>3. Restart the service using the command <code>docker-compose restart</code>.</p>","title":"Docker compose deployment"},{"location":"enterprise/release_notes/21.10/#kubernetes-based-deployment","text":"<p>1. Make a backup of the Tower database.</p> <p>2. Update the Tower container images in the Kubernetes manifest yaml files to:</p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v21.10.3\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v21.10.3\n</code></pre> <p>If you are using AWS Batch with a custom launcher job definition you need to update it to use the following container image (please refer the configuration section for details):</p> <pre>1</pre><pre><code>public.ecr.aws/seqera-labs/tower/nf-launcher:21.10.5\n</code></pre> <p>Refer to the manifests included in the K8s instalation section for details.</p> <p>3. Update the Tower cron service using the following:</p> <pre>1</pre><pre><code>kubectl apply -f tower-cron.yml\n</code></pre>  <p>Note</p> <p>This task will automatically run the Tower database schema update tool.</p>  <p>4. Update the Tower backend and frontend services using the following command:</p> <pre>1</pre><pre><code>kubectl apply -f tower-svc.yml\n</code></pre>","title":"Kubernetes based deployment"},{"location":"enterprise/release_notes/21.10/#custom-deployment-script","text":"<p>1. Make a backup of the Tower database.</p> <p>2. Pull or update the Tower container images references in your    deployment script(s) to:</p> <pre>1\n2</pre><pre><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v21.10.3\n195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v21.10.3\n</code></pre> <p>If you are using AWS Batch with a custom launcher job definition you need to update it to use the following container image (please refer the configuration section for details):</p> <pre>1</pre><pre><code>public.ecr.aws/seqera-labs/tower/nf-launcher:21.10.5\n</code></pre> <p>3. Update the Tower database schema by running the <code>/migrate-db.sh</code> provided in the    backend container. </p>  <p>Note</p> <p>Make sure to include the identical environment as used in the normal backend execution.</p>  <p>4. Once the schema update completes, deploy Tower following your usual procedure.</p>","title":"Custom deployment script"},{"location":"enterprise/release_notes/21.10/#questionsfeedback","text":"<p>Contact us at support@seqera.io. </p>","title":"Questions/Feedback"},{"location":"enterprise/release_notes/21.12/","text":"<p>The Seqera team is happy to announce the <code>21.12.x</code> release for Tower.</p>","title":"Release notes for version 21.12"},{"location":"enterprise/release_notes/21.12/#new-features-and-usability-improvements","text":"","title":"New features and usability improvements"},{"location":"enterprise/release_notes/21.12/#shared-workspaces","text":"<p>Shared workspaces are a solution to synchronization and resource-sharing within an organization in Tower. With a shared workspace, it is now possible to create and set up pipelines in a single place. This will make the pipeline accessible to all members of an organization.</p> <p>Learn more about the Shared workspaces feature.</p>","title":"Shared workspaces"},{"location":"enterprise/release_notes/21.12/#pipeline-reports-preview","text":"<p>First approach enabling Tower users to visualize and browse pipeline execution reports interactively inside Tower.</p> <p>Users can specify which files are pipeline reports using a new configuration field when defining a pipeline, making them available after the pipeline finished running. The supported file types that are rendered are: .pdf, .html and .txt. All reports, including those not in the rendered file formats, can be downloaded.</p>  <p>Warning</p> <p>This feature is undocumented because we plan to release soon a much improved version with extended rendering and robustness. </p>","title":"Pipeline reports [preview]"},{"location":"enterprise/release_notes/21.12/#add-dataset-public-apis-2240-942f3f4e3","text":"<p>Allow the definition of data inputs as samplesheets (CSVs/TSVs etc) which can be chosen by a launcher when launching a pipeline. The goal of this feature is to allow users to manage collections of data that can be used for pipeline executions submitted by Tower.</p> <p>A dataset is represented in a tabular manner, i.e. in rows and columns. Each row represents a generic record made up of several fields. Each field may contain a file path or other metadata of any type. The first row represents the fields/columns names.  A dataset is provided by the user as a CSV file (comma-separated values) or TSV file (tab-separated values).</p> <p>Learn more about Tower Datasets.</p>","title":"Add Dataset public APIs (#2240) [942f3f4e3]"},{"location":"enterprise/release_notes/21.12/#tower-agent-reverse-connection","text":"<p>Users with HPC clusters (e.g. SLURM) showed interest in being able to run jobs that track the specific user identity, rather than a general system-level user defined in the compute environment.</p> <p>A solution for this issue could be an agent-based solution, as it will enable further evolution later on.</p> <p>With the agent: The user will be able to run jobs on their HPC cluster via Tower, enabling them to track which runs have been executed by individual users using their internal management system (no need for extra steps to measure the quota used by each user).</p> <p>This process will be automatically taken care of by tower-client interaction, e.g. with an agent, eliminating the need of a multi factor authentication step.</p>","title":"Tower agent reverse connection"},{"location":"enterprise/release_notes/21.12/#minor-changes-improvements","text":"<ul> <li>Prevent the use of master as default branch (#2499) [791f45a11] : Due to the changes introduced by Github regarding the name of the default branch, Tower now acts intelligently about the name of the default branch.</li> <li>Allow S3 as a work directory for EFS and FSx [6199d8fd6] </li> <li>Starting from this release, it is now possible to specify an EFS or FSx mount as a work-directory on the AWS Batch platform.</li> <li>Bump nf-launcher: j17-21.10.6</li> </ul>","title":"Minor changes &amp; improvements"},{"location":"enterprise/release_notes/21.12/#changelog","text":"<p>For a detailed list of all changes, please consult the Nextflow Tower Changelog.</p>","title":"Changelog"},{"location":"enterprise/release_notes/21.12/#questionsfeedback","text":"<p>Contact us at support@seqera.io. </p>","title":"Questions/Feedback"},{"location":"enterprise/release_notes/22.1/","text":"<p>Nextflow Tower 22.1.x brings the following changes:</p> <ul> <li>Add Pipeline Secrets feature</li> <li>Add Custom workflows run name</li> <li>Add CSV and TSV reports rendering</li> <li>Add support for AWS agent and logging</li> <li>Add support for Moab platform</li> <li>Add GetLogsEvents perm to AWS Batch instance role</li> <li>Add support for local pipelines</li> <li>Add navigate back button to second level screens</li> <li>Add validation for SSH hostname and username</li> <li>Add credentials view page</li> <li>Add ECS pull strategy in user-data template</li> <li>Add root users environment when TOWER_ROOT_USERS variable is provided</li> <li>Add Tower system message</li> <li>Add config option to disable user private workspace</li> <li>Add support for JSON formatted logs</li> <li>Stability improvements</li> <li>Security improvements as for penetration test performed on March 2022</li> </ul>","title":"Release notes for version 22.1.x"},{"location":"enterprise/release_notes/22.1/#new-features-and-usability-improvements","text":"<p>We are pleased to announce the <code>22.1</code> release of Tower. This release brings long-awaited features including a customizable run name for workflows, enhanced secrets handling, better reports and improved HPC scheduler support.</p>  <p>Tip</p> <p>The release naming convention is changed and it reflects the year and the quarter, so our customers know to expect 4 enterprise releases per year.</p>","title":"New features and usability improvements"},{"location":"enterprise/release_notes/22.1/#customizable-workflow-run-name","text":"<p>From this release, Tower allows users to specify a custom name for each workflow run. This name will override the default runName parameter for the Nextflow run name. This feature allows to identify each run at a glance, facilitating the retrieval of a specific run from the list presented in the Runs page from the application.</p> <p>More than 60% of new workflow runs in tower.nf already make use of the custom run name.</p> <p></p>","title":"Customizable workflow run name"},{"location":"enterprise/release_notes/22.1/#pipeline-secrets-improved","text":"<p>Starting from 21.12, Tower uses the concept of Secrets to store the keys and tokens used by workflow tasks to interact with external systems e.g. a password to connect to an external database or an API token. In 22.1 we have improved the Secrets management by:</p> <ul> <li>Introducing the possibility for users to create user-level secrets</li> <li>Implementing a selection mechanism so users can dynamically select which secrets will be used by each pipeline launch</li> <li>Enable Tower to capture specific OIDC attributes as secrets  </li> </ul> <p>Full information about Secrets is available here </p>","title":"Pipeline secrets - improved"},{"location":"enterprise/release_notes/22.1/#improved-web-reports","text":"<p>Most Nextflow pipelines will generate reports or output files which are useful to inspect at the end of the pipeline execution. Since 21.12, Tower has a Reports feature that allows to directly visualise supported file types or to download them directly via the user interface.</p> <p>The key improvements in 22.1 - Reports are configurable and are no longer limited to workdir subdirectories - Tower enables the rendering of CSV and TSV files </p> <p>A comprehensive documentation on how to setup and constraints is available here.</p>","title":"Improved web reports"},{"location":"enterprise/release_notes/22.1/#enhanced-hpc-support","text":"<p>Tower <code>22.1</code> allows working with HPC MOAB platform, expanding the supported alternatives for HPC computing.</p> <p>We have also implemented a first version of a much requested feature from HPC users: Launch local repositories. Using this feature, users can instruct Tower to source the pipeline code stored as a Git bare repository and launch the workflows.</p>","title":"Enhanced HPC support"},{"location":"enterprise/release_notes/22.1/#notes","text":"<ol> <li> <p>As of version 22.1.x, Nextflow Tower Enterprise will follow a three month release cadence, using the following version number scheme: <code>YY.Q.PATCH</code>, where <code>YY</code> represents the year, <code>Q</code> represents the quarter and <code>PATCH</code> the incremental patch number.</p> </li> <li> <p>As of version 21.02.x, a license key must be provided to enable the Tower deployment feature. The license key should be specified using the configuration variable <code>TOWER_LICENSE</code>. If you don't have a license key, contact sales@seqera.io.</p> </li> </ol>","title":"Notes"},{"location":"enterprise/release_notes/22.1/#warnings","text":"<ol> <li> <p>This version now expects the use of HTTPS by default for all browser client connections. </p> <p>If your Tower installation requires the use of unsecured HTTP, set the following environment variable in the infrastructure hosting the Tower application: <code>TOWER_ENABLE_UNSAFE_MODE=true</code>.</p> </li> <li> <p>If you are upgrading from a version of Tower prior to <code>21.04.x</code>, please update your implementation to <code>21.04.x</code> before installing this release.</p> </li> </ol>","title":"Warnings"},{"location":"enterprise/release_notes/22.1/#database-schema","text":"<p>This Tower version requires a database schema update. Follow these steps to update your DB instance and the Tower installation.</p> <ol> <li> <p>Make a backup of the Tower database.</p> </li> <li> <p>Download and update your container versions to:</p> <ul> <li><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v22.1.5</code></li> <li><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v22.1.5</code></li> </ul> </li> <li> <p>Redeploy the Tower application:</p> <ul> <li> <p>docker-compose:</p> <ol> <li>Restart the application with <code>docker-compose restart</code>. This will automatically migrate the database schema.</li> </ol> </li> <li> <p>kubernetes: </p> <ol> <li>Update the cron service with <code>kubectl apply -f tower-cron.yml</code>. This will automatically migrate the database schema.</li> <li>Update the frontend and backend services with <code>kubectl apply -f tower-srv.yml</code>.</li> </ol> </li> <li> <p>custom deployment</p> <ol> <li>Run the <code>/migrate-db.sh</code> script provided in the <code>backend</code> container. This will migrate the database schema.</li> <li>Deploy Tower following your usual procedures.</li> </ol> </li> </ul> </li> </ol>","title":"Database Schema"},{"location":"enterprise/release_notes/22.1/#nextflow-launcher-image","text":"<p>If you must host your container images on a private image registry: </p> <ol> <li> <p>Copy the the nf-launches image to your private registry:</p> <ul> <li><code>quay.io/seqeralabs/nf-launcher:j17-22.04.0</code></li> </ul> </li> <li> <p>Update your <code>tower.env</code> with the following environment variable:</p> <ul> <li><code>TOWER_LAUNCH_CONTAINER=&lt;FULL_PATH_TO_YOUR_PRIVATE_IMAGE&gt;</code></li> </ul>  <p>Warning</p> <p>If using AWS Batch, you will need to configure a custom job-definition and populate the <code>TOWER_LAUNCH_CONTAINER</code> with the job-definition name instead. </p>  </li> </ol>","title":"Nextflow Launcher Image"},{"location":"enterprise/release_notes/22.1/#compute-environments","text":"<p>The Tower compute environments to be made available to users must be specified in the Tower configuration.</p> <p>The following ids options are available:</p> <ul> <li><code>awsbatch-platform</code>: AWS Batch cloud compute service</li> <li><code>gls-platform</code>: Google LifeSciences cloud compute service</li> <li><code>azbatch-platform</code>: Azure Batch cloud compute service</li> <li><code>lsf-platform</code>: IBM LSF batch scheduler</li> <li><code>slurm-platform</code>: Slurm batch scheduler</li> <li><code>altair-platform</code>: Altair PBS Pro batch scheduler</li> <li><code>univa-platform</code>: (Univa/Sun) GridEngine</li> <li><code>moab-platform</code>: Adaptive Computing Moab batch scheduler</li> <li><code>k8s-platform</code>: Kubernetes compute platform</li> <li><code>eks-platform</code>: AWS EKS compute platform</li> <li><code>gke-platform</code>: Google GKE compute platform</li> </ul> <p>Choose one or more of these platform ids specify them via the environment variable <code>TOWER_ENABLE_PLATFORMS</code>, separating them via a comma.</p>","title":"Compute environments"},{"location":"enterprise/release_notes/22.1/#changelog","text":"<p>For a detailed list of all changes, please consult the Nextflow Tower Changelog.</p>","title":"Changelog"},{"location":"enterprise/release_notes/22.1/#questionsfeedback","text":"<p>Contact us at support@seqera.io. </p>","title":"Questions/Feedback"},{"location":"enterprise/release_notes/22.2/","text":"<p>Nextflow Tower 22.2.x brings the following changes:</p> <ul> <li>Added support for Illumina DRAGEN</li> <li>Added support to mysql8</li> <li>Allow access remote pipelines via Tower Agent</li> <li>Feature 3025 reports download limit</li> <li>Adds used datasets tab to run details page</li> <li>Add support for redis password</li> <li>pipeline reports index page</li> <li>Feature 2663 / Labels</li> <li>Add support for AWS CodeCode repositories</li> <li>runName filled with random run name by default if not in relaunch mode</li> <li>Allow the ability to send cluster options from head queue to child nodes</li> <li>Add advanced search capabilities to runs page</li> </ul>","title":"Release notes for version 22.2.x"},{"location":"enterprise/release_notes/22.2/#new-features-and-usability-improvements","text":"<p>We are pleased to announce the <code>22.2</code> release of Tower. This release brings a wealth of improvements to user-interaction and management of pipelines in a workspace at scale.</p>","title":"New features and usability improvements"},{"location":"enterprise/release_notes/22.2/#tower-labels-to-organize-runs","text":"<p>Tower now provides an answer to the users needs for categorization and retrieval with the introduction of Labels. Tower Labels are free text annotations that can be attached to Tower entities like pipelines, actions, or workflow runs either during or after creation.</p> <p>Labels help organize the runs in a workspace, they facilitate project management by enabling categorization, search and retrieval of key content.</p> <p>Detailed information can be found here https://help.tower.nf/22.2/labels/overview/</p> <p></p>","title":"Tower labels to organize runs"},{"location":"enterprise/release_notes/22.2/#complex-run-filtering","text":"<p>Tower 22.2 enables easy run retrieval with complex queries. Together with the categorization allowed by labels, users can now filter runs with complex queries using a combination of free text with keyword:value filters, such as the run status or the launch date intervals.</p> <p></p>","title":"Complex run filtering"},{"location":"enterprise/release_notes/22.2/#tower-forge-support-for-dragen","text":"<p>We have extended the Tower Forge feature for AWS Batch to support DRAGEN. Tower Forge ensures that all of the appropriate components and settings are automatically provisioned when creating a Compute Environment for executing pipelines.</p> <p>When deploying data analysis workflows, some tasks will need to use normal instance types (e.g. for non-DRAGEN processing of samples) and others will need to be executed on F1 instances. If the DRAGEN feature is enabled, Tower Forge will create an additional AWS Batch compute queue which only uses F1 instances, to which DRAGEN tasks will be dispatched</p> <p>Full details are available here.</p>","title":"Tower Forge support for DRAGEN"},{"location":"enterprise/release_notes/22.2/#enhanced-reports-view","text":"<p>Available reports are listed in a Reports tab on the Runs page. Users can select a report from the table to open or download. See more detail here.</p> <p></p>","title":"Enhanced reports view"},{"location":"enterprise/release_notes/22.2/#notes","text":"<ol> <li> <p>As of version 22.1.x, Nextflow Tower Enterprise will follow a three month release cadence, using the following version number scheme: <code>YY.Q.PATCH</code>, where <code>YY</code> represents the year, <code>Q</code> represents the quarter and <code>PATCH</code> the incremental patch number.</p> </li> <li> <p>As of version 21.02.x, a license key must be provided to enable the Tower deployment feature. The license key should be specified using the configuration variable <code>TOWER_LICENSE</code>. If you don't have a license key, contact sales@seqera.io.</p> </li> </ol>","title":"Notes"},{"location":"enterprise/release_notes/22.2/#warnings","text":"<ol> <li> <p>This version requires all database connections to use the following configuration value: <code>TOWER_DB_DRIVER=org.mariadb.jdbc.Driver</code>.     Please update your configuration if you are upgrading. All other database configuration values should remain unchanged.</p> </li> <li> <p>This version expects the use of HTTPS by default for all browser client connections.     If your Tower installation requires the use of unsecured HTTP, set the following environment variable in the infrastructure hosting the Tower application: <code>TOWER_ENABLE_UNSAFE_MODE=true</code>.</p> </li> <li> <p>If you are upgrading from a version of Tower prior to <code>21.04.x</code>, please update your implementation to <code>21.04.x</code> before installing this release.</p> </li> </ol>","title":"Warnings"},{"location":"enterprise/release_notes/22.2/#database-schema","text":"<p>This Tower version requires a database schema update. Follow these steps to update your DB instance and the Tower installation.</p> <ol> <li> <p>Make a backup of the Tower database.</p> </li> <li> <p>Download and update your container versions to:</p> </li> <li> <p><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/backend:v22.2.1</code></p> </li> <li> <p><code>195996028523.dkr.ecr.eu-west-1.amazonaws.com/nf-tower-enterprise/frontend:v22.2.1</code></p> </li> <li> <p>Redeploy the Tower application:</p> </li> <li> <p>docker-compose:</p> <ol> <li>Restart the application with <code>docker-compose restart</code>. This will automatically migrate the database schema.</li> </ol> </li> <li> <p>kubernetes:</p> <ol> <li>Update the cron service with <code>kubectl apply -f tower-cron.yml</code>. This will automatically migrate the database schema.</li> <li>Update the frontend and backend services with <code>kubectl apply -f tower-srv.yml</code>.</li> </ol> </li> <li> <p>custom deployment</p> <ol> <li>Run the <code>/migrate-db.sh</code> script provided in the <code>backend</code> container. This will migrate the database schema.</li> <li>Deploy Tower following your usual procedures.</li> </ol> </li> </ol>","title":"Database Schema"},{"location":"enterprise/release_notes/22.2/#nextflow-launcher-image","text":"<p>If you must host your container images on a private image registry:</p> <ol> <li> <p>Copy the the nf-launches image to your private registry:</p> </li> <li> <p><code>quay.io/seqeralabs/nf-launcher:j17-22.06.1-edge</code></p> </li> <li> <p>Update your <code>tower.env</code> with the following environment variable:</p> </li> <li> <p><code>TOWER_LAUNCH_CONTAINER=&lt;FULL_PATH_TO_YOUR_PRIVATE_IMAGE&gt;</code></p> <p>!!! warning  If using AWS Batch, you will need to configure a custom job-definition and populate the <code>TOWER_LAUNCH_CONTAINER</code> with the job-definition name instead.</p> </li> </ol>","title":"Nextflow Launcher Image"},{"location":"enterprise/release_notes/22.2/#compute-environments","text":"<p>The Tower compute environments to be made available to users must be specified in the Tower configuration.</p> <p>The following ids options are available:</p> <ul> <li><code>awsbatch-platform</code>: AWS Batch cloud compute service</li> <li><code>gls-platform</code>: Google LifeSciences cloud compute service</li> <li><code>azbatch-platform</code>: Azure Batch cloud compute service</li> <li><code>lsf-platform</code>: IBM LSF batch scheduler</li> <li><code>slurm-platform</code>: Slurm batch scheduler</li> <li><code>altair-platform</code>: Altair PBS Pro batch scheduler</li> <li><code>univa-platform</code>: (Univa/Sun) GridEngine</li> <li><code>moab-platform</code>: Adaptive Computing Moab batch scheduler</li> <li><code>k8s-platform</code>: Kubernetes compute platform</li> <li><code>eks-platform</code>: AWS EKS compute platform</li> <li><code>gke-platform</code>: Google GKE compute platform</li> </ul> <p>Choose one or more of these platform ids and specify them via the environment variable <code>TOWER_ENABLE_PLATFORMS</code>, separating them via a comma.</p>","title":"Compute environments"},{"location":"enterprise/release_notes/22.2/#changelog","text":"<p>For a detailed list of all changes, please consult the Nextflow Tower Changelog.</p>","title":"Changelog"},{"location":"enterprise/release_notes/22.2/#questionsfeedback","text":"<p>Contact us at support@seqera.io. </p>","title":"Questions/Feedback"},{"location":"enterprise/release_notes/22.3/","tags":["releqsae","enterprise",22.3],"text":"","title":"Release notes for version 22.3.x"},{"location":"enterprise/release_notes/22.3/#new-features","tags":["releqsae","enterprise",22.3],"text":"","title":"New features"},{"location":"enterprise/release_notes/22.3/#resource-labels","tags":["releqsae","enterprise",22.3],"text":"<p>Tower now supports applying resource labels to compute environments and other Tower elements. This offers a flexible tagging system for annotation and tracking of the cloud services consumed by a run.</p> <p>Resource labels are sent to the service provider for each cloud compute environment in <code>key=value</code> format. They can be created, edited, and applied by a workspace admin or owner.</p> <p>Note: Resource labels modified on your cloud provider platform do not update in Tower automatically.</p>","title":"Resource Labels"},{"location":"enterprise/release_notes/22.3/#dashboard","tags":["releqsae","enterprise",22.3],"text":"<p></p> <p>Tower 22.3 introduces a dashboard interface to view total runs and run status, filtered by organization or user workspace. This facilitates overall run status monitoring and early detection of execution issues.</p> <p>Select Dashboard from your user avatar menu in the top right corner of the Tower web UI.</p>","title":"Dashboard"},{"location":"enterprise/release_notes/22.3/#google-batch-support","tags":["releqsae","enterprise",22.3],"text":"<p>Tower now supports Google Cloud Batch compute environments. Google Cloud Batch is a comprehensive cloud service suitable for multiple use cases, including HPC, AI/ML, and data processing. Tower now provides an integration with your existing Google Cloud account via the Batch API. While it is similar to the Google Cloud Life Sciences API, Google Cloud Batch offers a broader set of capabilities.</p> <p>Google Cloud Batch automatically provisions resources, manages capacity, and allows batch workloads to run at scale. The API has built-in support for data ingestion from Google Cloud Storage buckets. This makes data ingestion and sharing datasets efficient and reliable.</p> <p>This is a Beta Tower feature \u2014 more capability will be added as Nextflow Google Cloud Batch support evolves.</p>","title":"Google Batch support"},{"location":"enterprise/release_notes/22.3/#admin-panel-enhancements","tags":["releqsae","enterprise",22.3],"text":"<p>The Tower admin panel now provides additional user and organization management features.</p> <p>From the Users tab, admins can view all users, assign or remove users, and change user roles within an organization.</p> <p>From the Organizations tab, admins can view organizations, assign or remove users, and manage the user roles within an organization.</p>","title":"Admin panel enhancements"},{"location":"enterprise/release_notes/22.3/#resource-optimization-technology-preview","tags":["releqsae","enterprise",22.3],"text":"<p>Tower Cloud now supports cloud resource optimization when running pipelines. Using the extensive resource usage data which Tower already collects for each pipeline run, a set of per-process resource recommendations is generated and can be applied to subsequent runs. This feature is geared to optimize resource use significantly, while being conservative enough to ensure that pipelines run reliably.</p> <p>This feature is currently only available on Tower Cloud (tower.nf). For more information about this optional feature, contact us.</p>","title":"Resource optimization (technology preview)"},{"location":"enterprise/release_notes/22.3/#wave-containers-technology-preview","tags":["releqsae","enterprise",22.3],"text":"<p>Tower now supports the Nextflow Wave container provisioning and augmentation service. When a pipeline is run in Nextflow using Wave, the Wave service uses a Dockerfile stored in the process directory to build a container in the target registry. When the container to be used for process execution is returned, the Wave service can add functional layers and data on-the-fly before it is returned to Nextflow for actual process execution.</p> <p>Wave also enables the use of private container registries in Nextflow \u2014 registry credentials stored in Tower are used to authenticate to private container registries with the Wave service.</p> <p>The Wave container provisioning service is available free of charge as a technology preview to all Nextflow and Tower users. During the preview period, anonymous users can build up to 10 container images per day and pull 100 containers per hour. Tower authenticated users can build 100 container images per hour and pull 1000 containers per minute. After the preview period, we plan to make the Wave service available free of charge to academic users and open-source software (OSS) projects.</p> <p>See here for an introductory overview of Wave containers on the Nextflow blog, and here for a live demo and introduction of Wave from the Nextflow 2022 Summit, by Seqera Labs co-founder and CTO Paolo di Tommaso.</p> <p>This feature is currently only available on Tower Cloud (tower.nf). For more information about this optional feature, contact us.</p>","title":"Wave containers (technology preview)"},{"location":"enterprise/release_notes/22.3/#fusion-file-system-technology-preview","tags":["releqsae","enterprise",22.3],"text":"<p>Fusion is a virtual distributed file system which allows data hosted in AWS S3 buckets to be accessed directly by the file system interface used by pipeline tools. This means that Nextflow pipelines can use an S3 bucket as the work directory and pipeline tasks can access the S3 bucket natively as a local file system path.</p> <p>Fusion, as used by the Wave container provisioning service, is available free of charge as a technology preview to all Nextflow and Tower users. After the preview period, we plan to make the service available free of charge to academic users and open-source software (OSS) projects.</p> <p>This feature is currently only available on Tower Cloud (tower.nf). For more information about this optional feature, contact us.</p>","title":"Fusion file system (technology preview)"},{"location":"enterprise/release_notes/22.3/#other-enhancements","tags":["releqsae","enterprise",22.3],"text":"<ul> <li>Owners have full permissions for all workspaces in their organization</li> <li>Navigation restyling</li> <li>Launch/relaunch form allows head node resource customization</li> <li>Runs page supports task name search in Tasks table</li> <li>Expand boot EBS volume size</li> <li>Label and resource label APIs are now exposed</li> <li>The amount of usable datasets (and dataset versions) per organization has been raised to 100 records by default</li> <li>Customize head node resources from the launch/relaunch form<ul> <li>As a user (with maintainer permissions) it is now possible to launch a pipeline in a Tower cloud environment, specifying the head node resources (memory and CPU) from the launch form. This allows you to properly dimension resources and avoid pipeline crashes. This feature is available for AWS, Google Life Sciences, and Kubernetes Compute Environments.</li> </ul> </li> <li>The Revision field in the launch form has been extended to allow a maximum length of 100 characters</li> <li>Improve SSH connector resilience + UGE qstat</li> </ul>","title":"Other enhancements"},{"location":"enterprise/release_notes/22.3/#fixes","tags":["releqsae","enterprise",22.3],"text":"<ul> <li>BitBucketServer Git provider</li> <li>Container registry name</li> <li>Missing file existence check for Google Life Sciences</li> <li>Resume functionality on Google Life Sciences</li> <li>Improved error traceability when an exception occurs in the prerun script block</li> <li>Fixed a bug that prevented a run to be resumed for users with launch permissions</li> <li>Saving status for a job fails when a DB exception occurs</li> <li>Escape qstat command for Altair PBS batch scheduler</li> </ul>","title":"Fixes"},{"location":"enterprise/release_notes/22.3/#breaking-changes-warnings","tags":["releqsae","enterprise",22.3],"text":"","title":"Breaking changes &amp; warnings"},{"location":"enterprise/release_notes/22.3/#breaking-changes","tags":["releqsae","enterprise",22.3],"text":"<ul> <li> <p>In previous versions, some assets required by Tower Forge were downloaded from S3 bucket named <code>nf-xpack.s3.eu-west-1.amazonaws.com</code>. As of version 22.3.x, those assets are now downloaded from <code>[https://nf-xpack.seqera.io](https://nf-xpack.seqera.io/)</code>. Make sure your network policy allows access to the seqera.io domain.</p> </li> <li> <p>Use of the resource labels feature with AWS Batch requires an update of the IAM policy used by the account running Tower. The required changes can be found here.</p> </li> <li> <p>In previous versions, if Tower was configured to authenticate to AWS via instance role, Batch Forge would assign this same IAM Role as the Head Job role and Compute Job role of the AWS Batch compute environment it created. As of version 22.3.1, you must explicitly assign these job roles during the AWs Batch compute environment creation process.</p> </li> </ul>","title":"Breaking changes"},{"location":"enterprise/release_notes/22.3/#warnings","tags":["releqsae","enterprise",22.3],"text":"<ol> <li> <p>This version requires all database connections to use the following configuration value: <code>TOWER_DB_DRIVER=org.mariadb.jdbc.Driver</code>.     Please update your configuration if you are upgrading. All other database configuration values should remain unchanged.</p> </li> <li> <p>This version expects the use of HTTPS by default for all browser client connections.     If your Tower installation requires the use of unsecured HTTP, set the following environment variable in the infrastructure hosting the Tower application: <code>TOWER_ENABLE_UNSAFE_MODE=true</code>.</p> </li> <li> <p>If you are upgrading from a version of Tower prior to <code>21.04.x</code>, please update your implementation to <code>21.04.x</code> before installing this release.</p> </li> </ol>","title":"Warnings"},{"location":"enterprise/release_notes/22.3/#database-schema","tags":["releqsae","enterprise",22.3],"text":"<p>This Tower version requires a database schema update. Follow these steps to update your DB instance and the Tower installation.</p> <ol> <li> <p>Make a backup of the Tower database.</p> </li> <li> <p>Download and update your container versions.</p> </li> <li> <p>Redeploy the Tower application:</p> <ul> <li> <p>docker-compose:</p> <ol> <li>Restart the application with <code>docker-compose restart</code>. This will automatically migrate the database schema.</li> </ol> </li> <li> <p>kubernetes:</p> <ol> <li>Update the cron service with <code>kubectl apply -f tower-cron.yml</code>. This will automatically migrate the database schema.</li> <li>Update the frontend and backend services with <code>kubectl apply -f tower-srv.yml</code>.</li> </ol> </li> <li> <p>custom deployment</p> <ol> <li>Run the <code>/migrate-db.sh</code> script provided in the <code>backend</code> container. This will migrate the database schema.</li> <li>Deploy Tower following your usual procedures.</li> </ol> </li> </ul> </li> </ol>","title":"Database Schema"},{"location":"enterprise/release_notes/22.3/#nextflow-launcher-image","tags":["releqsae","enterprise",22.3],"text":"<p>If you must host your nf-launcher container image on a private image registry:</p> <ol> <li> <p>Copy the nf-launcher image to your private registry.</p> </li> <li> <p>Update your <code>tower.env</code> with the following environment variable:</p> <p><code>TOWER_LAUNCH_CONTAINER=&lt;FULL_PATH_TO_YOUR_PRIVATE_IMAGE&gt;</code></p> </li> </ol>  <p>Warning</p>  <p>If using AWS Batch, you will need to configure a custom job-definition and populate the <code>TOWER_LAUNCH_CONTAINER</code> with the job-definition name instead.</p>","title":"Nextflow Launcher Image"},{"location":"enterprise/release_notes/22.3/#new-compute-environments","tags":["releqsae","enterprise",22.3],"text":"<p>New compute environment options are available:</p> <ul> <li><code>googlebatch-platform</code>: Google Batch cloud compute service</li> </ul>","title":"New Compute environments"},{"location":"enterprise/release_notes/22.3/#changelog","tags":["releqsae","enterprise",22.3],"text":"<p>For a detailed list of all changes, please consult the Nextflow Tower Changelog.</p>","title":"Changelog"},{"location":"enterprise/release_notes/22.3/#sharing-feedback","tags":["releqsae","enterprise",22.3],"text":"<p>You can share your feedback via the https://support.seqera.io.</p>","title":"Sharing feedback"},{"location":"enterprise/release_notes/22.4/","tags":["release","enterprise",22.4],"text":"","title":"Release notes for version 22.4.x"},{"location":"enterprise/release_notes/22.4/#new-features","tags":["release","enterprise",22.4],"text":"","title":"New features"},{"location":"enterprise/release_notes/22.4/#resource-labels","tags":["release","enterprise",22.4],"text":"<p>In Nextflow Tower 22.3, Seqera Labs introduced resource labels \u2014 a flexible tagging system for the cloud services consumed by a run. Workspace administrators can now customize the resource labels associated with pipelines, actions, and runs. This improves the feature\u2019s flexibility as resource labels are no longer inherited only from the compute environment.</p> <p>In Tower Enterprise 22.4, an administrator can now:</p> <ul> <li>Override and save the resource labels automatically assigned to a pipeline.<ul> <li>The pipeline will have a different resource label set from its associated compute environment. Resource labels added to the pipeline propagate to the cloud provider, without being permanently associated with the compute environment in Tower.</li> <li>If a maintainer edits a pipeline and changes the compute environment, the resource labels field is updated with the resource labels of the new compute environment.</li> </ul> </li> <li>Override and save the resource labels associated with an action, following the same logic as pipelines above.</li> <li>Override the resource labels associated with a workflow run before launch, enabling job-level tagging.<ul> <li>The resource labels tied to a workflow run are associated with specific cloud resources that do not include all resources tagged when a compute environment is created.</li> </ul> </li> </ul>","title":"Resource Labels"},{"location":"enterprise/release_notes/22.4/#all-runs-view","tags":["release","enterprise",22.4],"text":"<p>A comprehensive new view of All runs accessible to each user across the entire Tower instance is now available. This feature is especially useful for monitoring multiple workspaces at once and identifying execution patterns across workspaces and organizations.</p> <p>Segmented by organizations and workspaces, the interface facilitates overall status monitoring and early detection of execution issues, such as pipeline-related problems or infrastructure issues that can affect multiple workspaces simultaneously.</p> <p>The All runs view is accessible via the user top-right menu \u2014 select \u201dAll runs\u201d.</p>","title":"All runs view"},{"location":"enterprise/release_notes/22.4/#wave-support-for-tower-enterprise","tags":["release","enterprise",22.4],"text":"<p>All Tower instances with internet access can now connect to the Seqera Labs Wave container service to leverage its container augmentation and Fusion v2 file system capabilities. See the Wave containers documentation for more information about Wave containers.</p> <p>The Wave integration also allows for the secure transfer of credentials required to access private registries between services. See the Tower documentation to learn how to use the feature in your enterprise installation.</p>","title":"Wave support for Tower Enterprise"},{"location":"enterprise/release_notes/22.4/#fusion-file-system-support","tags":["release","enterprise",22.4],"text":"<p>Tower 22.4 adds official support for the Fusion file system. Fusion file system is a lightweight client that enables containerized tasks to access data in Amazon S3 (and other object stores in future) using POSIX file access semantics. Depending on your data handling requirements, Fusion 2.0 improves pipeline throughput and/or reduces cloud computing costs. For additional information on Fusion 2.0 and newly published benchmark results, see the recent article Breakthrough performance and cost-efficiency with the new Fusion file system. The Wave service is a prerequisite for using the Fusion file system.</p>","title":"Fusion file system support"},{"location":"enterprise/release_notes/22.4/#resuming-runs-on-a-different-compute-environment","tags":["release","enterprise",22.4],"text":"<p>Tower 22.4 allows users with sufficient permissions to change their compute environment when resuming a run. Users with a maintainer role or above can now select a new compute environment when resuming a run.</p> <p>This is especially useful if the original run failed due to infrastructure limitations of the compute environment, such as insufficient memory being available to a task. Now, it is possible to select a new compute environment when the run is resumed, without the need to restart from the first task.</p> <p>The only requirement is that the new compute environment has access to the original run workdir.</p>","title":"Resuming runs on a different compute environment"},{"location":"enterprise/release_notes/22.4/#other-enhancements","tags":["release","enterprise",22.4],"text":"<ul> <li>Update to Java 17</li> <li>Support for Gitea credentials and repositories</li> <li>UI fixes in the run detail page<ul> <li>Alphabetical sorting for reports</li> <li>Horizontal scrolling for log window</li> </ul> </li> <li>ECS configuration in the advanced setup for AWS compute environments</li> <li>Nextflow: support for S3 Glacier file retrieval</li> <li>Nextflow: define the storage class for published files</li> <li>Actions: duplicate the launch for every run from an action to ease management and retrieval (this change is not retroactive \u2014 old actions\u2019 runs need to be relaunched for changes to take effect)</li> </ul>","title":"Other enhancements"},{"location":"enterprise/release_notes/22.4/#breaking-changes-warnings","tags":["release","enterprise",22.4],"text":"","title":"Breaking changes &amp; warnings"},{"location":"enterprise/release_notes/22.4/#warnings","tags":["release","enterprise",22.4],"text":"<ol> <li> <p>The default <code>nf-launcher</code> image includes a <code>curl</code> command which will fail if your Tower is secured with a private TLS certificate. To mitigate this problem, please see these instructions.</p> </li> <li> <p>This version requires all database connections to use the following configuration value: <code>TOWER_DB_DRIVER=org.mariadb.jdbc.Driver</code>.     Please update your configuration if you are upgrading. All other database configuration values should remain unchanged.</p> </li> <li> <p>This version expects the use of HTTPS by default for all browser client connections.     If your Tower installation requires the use of unsecured HTTP, set the following environment variable in the infrastructure hosting the Tower application: <code>TOWER_ENABLE_UNSAFE_MODE=true</code>.</p> </li> <li> <p>If you are upgrading from a version of Tower prior to <code>21.04.x</code>, please update your implementation to <code>21.04.x</code> before installing this release.</p> </li> </ol>","title":"Warnings"},{"location":"enterprise/release_notes/22.4/#upgrade-steps","tags":["release","enterprise",22.4],"text":"<p>This Tower version requires a database schema update. Follow these steps to update your DB instance and the Tower installation.</p>  <p>Warning</p> <p>To ensure no loss of data, the database volume must be persistent on the local machine. Use the <code>volumes</code> key in the <code>db</code> or <code>redis</code> section of your docker-compose.yml file to specify a local path to the DB or Redis instance.</p>  <ol> <li> <p>Make a backup of the Tower database.</p> </li> <li> <p>Download and update your container versions.</p> </li> <li> <p>Redeploy the Tower application:</p> <ul> <li> <p>docker-compose:</p> <ol> <li>To migrate the database schema, restart the application with <code>docker-compose down</code>, then <code>docker-compose up</code>.</li> </ol> </li> <li> <p>kubernetes:</p> <ol> <li>Update the cron service with <code>kubectl apply -f tower-cron.yml</code>. This will automatically migrate the database schema.</li> <li>Update the frontend and backend services with <code>kubectl apply -f tower-srv.yml</code>.</li> </ol> </li> <li> <p>custom deployment:</p> <ol> <li>Run the <code>/migrate-db.sh</code> script provided in the <code>backend</code> container. This will migrate the database schema.</li> <li>Deploy Tower following your usual procedures.</li> </ol> </li> </ul> </li> </ol>","title":"Upgrade steps"},{"location":"enterprise/release_notes/22.4/#nextflow-launcher-image","tags":["release","enterprise",22.4],"text":"<p>If you must host your nf-launcher container image on a private image registry:</p> <ol> <li> <p>Copy the nf-launcher image to your private registry.</p> </li> <li> <p>Update your <code>tower.env</code> with the following environment variable:</p> <p><code>TOWER_LAUNCH_CONTAINER=&lt;FULL_PATH_TO_YOUR_PRIVATE_IMAGE&gt;</code></p> </li> </ol>  <p>Warning</p> <p>If using AWS Batch, you will need to configure a custom job definition and populate the <code>TOWER_LAUNCH_CONTAINER</code> with the job definition name instead.</p>","title":"Nextflow launcher image"},{"location":"enterprise/release_notes/22.4/#changelog","tags":["release","enterprise",22.4],"text":"<p>For a detailed list of all changes, see the Nextflow Tower Changelog.</p>","title":"Changelog"},{"location":"enterprise/release_notes/22.4/#sharing-feedback","tags":["release","enterprise",22.4],"text":"<p>Share your feedback via support.seqera.io.</p>","title":"Sharing feedback"},{"location":"enterprise/release_notes/23.1/","text":"","title":"Release notes for version 23.1.x"},{"location":"enterprise/release_notes/23.1/#new-features","text":"","title":"New features"},{"location":"enterprise/release_notes/23.1/#launchpad-redesign-and-pipeline-enhancements","text":"<p>To enhance pipeline search and navigation capabilities, we now support  a new list view to complement the existing card view. The list view allows users to efficiently search for and navigate to their pipeline of choice, while also ensuring that the most relevant information is visible and the relationships between pipelines are clear. With this new feature, users can access their pipelines in either card or list view, making them easier to manage.</p> <p>We have also  introduced a new pipeline detail view that shows in-depth information about each pipeline without needing to access the edit screen.</p>","title":"Launchpad redesign and pipeline enhancements"},{"location":"enterprise/release_notes/23.1/#enhanced-support-for-fusion-file-system","text":"<p>Tower 23.1 introduces support for the Fusion file system in Google Cloud Batch environments. Fusion is a distributed, lightweight file system for cloud-native pipelines that has been shown to improve performance by up to ~2.2x compared to cloud native object storage. </p> <p>With this new integration, Google Cloud Batch users can enjoy a faster, more efficient, and cheaper processing experience. Fusion offers many benefits, including faster real-time data processing, batch processing, and ETL operations, making it a valuable tool for managing complex data pipelines. By using Fusion with Google Cloud Batch, users can run their data integration workflows directly against data residing in Google Cloud Storage. This integration will allow Google users to streamline their data processing workflows, increase productivity, reduce cloud spending, and achieve better outcomes.</p>","title":"Enhanced support for Fusion file system"},{"location":"enterprise/release_notes/23.1/#wave-websockets-support","text":"<p>We have added a new secure way to connect two elements, Tower and Wave, using WebSockets. This is an important addition for our enterprise customers as it ensures connection safety, improved efficiency, and better control over traffic sent between Tower and Wave. This connection will help facilitate the adoption of Fusion by enterprise customers, as it provides a more secure and reliable way to manage their data integration workflows. With WebSockets, users can easily connect their Tower and Wave instances and take advantage of the many benefits that Fusion has to offer.</p>","title":"Wave WebSockets support"},{"location":"enterprise/release_notes/23.1/#other-enhancements","text":"<ul> <li>Save executed runs as pipelines</li> <li>Improved allruns list view and filtering</li> <li>Filter runs by label</li> <li>Admin panel enhancements: team and workspace management</li> <li>Additional dashboard enhancements:<ul> <li>Export dashboard data to CSV.</li> <li>Improved date fiiltering</li> </ul> </li> <li>Default resource labels for compute environments per workspace</li> <li>Fusion log download</li> <li>Upgrade Micronaut to 3.8.5</li> <li>Tower Agent connection sharing</li> <li>Customizable log format</li> <li>AWS Parameter store support (distributed config values)</li> <li>Azure Repos credential support</li> <li>Fusion v2 EBS disk optimized configuration</li> </ul>","title":"Other enhancements"},{"location":"enterprise/release_notes/23.1/#breaking-changes-and-warnings","text":"<p>Breaking changes and instructions listed here apply when updating from Tower version 22.4. If you are updating from an earlier version, see the release notes of previous versions for a complete picture of changes that may affect you. </p>","title":"Breaking changes and warnings"},{"location":"enterprise/release_notes/23.1/#updated-aws-permissions-policies","text":"<p>Several new Tower features over the last few releases require updated AWS IAM permissions policies. Retrieve and apply the latest policy files here.</p>","title":"Updated AWS permissions policies"},{"location":"enterprise/release_notes/23.1/#wave-requires-container-registry-credentials","text":"<p>The Wave containers service uses container registry credentials in Tower to authenticate to your (public or private) container registries. This is separate from your existing cloud provider credentials stored in Tower. </p> <p>This means that, for example, AWS ECR (Elastic Container Registry) authentication requires an ECR container registry credential if you are running a compute environment with Wave enabled, even if your existing AWS credential in Tower has IAM access to your ECR. </p> <p>See the relevant container registry credentials page for provider-specific instructions. </p>","title":"Wave requires container registry credentials"},{"location":"enterprise/release_notes/23.1/#upgrade-steps","text":"<p>This Tower version requires a database schema update. Follow these steps to update your DB instance and the Tower installation.</p>  <p>Warning</p> <p>To ensure no loss of data, the database volume must be persistent on the local machine. Use the <code>volumes</code> key in the <code>db</code> or <code>redis</code> section of your docker-compose.yml file to specify a local path to the DB or Redis instance.</p>  <ol> <li> <p>Make a backup of the Tower database.</p> </li> <li> <p>Download and update your container versions.</p> </li> <li> <p>Redeploy the Tower application:</p> <ul> <li> <p>docker-compose:</p> <ol> <li>To migrate the database schema, restart the application with <code>docker-compose down</code>, then <code>docker-compose up</code>.</li> </ol> </li> <li> <p>kubernetes:</p> <ol> <li>Update the cron service with <code>kubectl apply -f tower-cron.yml</code>. This will automatically migrate the database schema.</li> <li>Update the frontend and backend services with <code>kubectl apply -f tower-srv.yml</code>.</li> </ol> </li> <li> <p>custom deployment:</p> <ol> <li>Run the <code>/migrate-db.sh</code> script provided in the <code>backend</code> container. This will migrate the database schema.</li> <li>Deploy Tower following your usual procedures.</li> </ol> </li> </ul> </li> </ol>","title":"Upgrade steps"},{"location":"enterprise/release_notes/23.1/#nextflow-launcher-image","text":"<p>If you must host your nf-launcher container image on a private image registry:</p> <ol> <li> <p>Copy the nf-launcher image to your private registry.</p> </li> <li> <p>Update your <code>tower.env</code> with the launch container environment variable:</p> <p><code>TOWER_LAUNCH_CONTAINER=&lt;FULL_PATH_TO_YOUR_PRIVATE_IMAGE&gt;</code></p> </li> </ol>  <p>Warning</p> <p>If using AWS Batch, you will need to configure a custom job definition and populate the <code>TOWER_LAUNCH_CONTAINER</code> with the job definition name instead.</p>","title":"Nextflow launcher image"},{"location":"enterprise/release_notes/23.1/#changelog","text":"<p>For a detailed list of all changes, see the Nextflow Tower Changelog.</p>","title":"Changelog"},{"location":"enterprise/release_notes/23.1/#sharing-feedback","text":"<p>Share your feedback via support.seqera.io.</p>","title":"Sharing feedback"},{"location":"enterprise/release_notes/changelog/","tags":["changelog"],"text":"","title":"Changelog"},{"location":"enterprise/release_notes/changelog/#2023","tags":["changelog"],"text":"","title":"2023"},{"location":"enterprise/release_notes/changelog/#2310-28-apr-2023","tags":["changelog"],"text":"<ul> <li>Add Fusion logs download (#4385) [49eb6dbe]</li> <li>Add Fusion support for Google Batch (#4654) [968d9fb1]</li> <li>Add missing launch option in pipeline action menu (#4441) [56313780]</li> <li>Add source reference to launch entity (#4527) [bd073128]</li> <li>Add support for AWS Parameters Store (#4563) [0f9f5400]</li> <li>Add teams management to admin panel (#4553) [8e019921]</li> <li>Add Save run as pipeline (#4610) [a14e1280]</li> <li>Add workspace selection in All runs page [b574db06]</li> <li>Add Launchpad redesign with list and cards views (#4110) [92345120]</li> <li>Add ability to export dashboard data as CSV (#4463) [765931ad]</li> <li>Add azure repos credentials (#4012) [f03f8a55]</li> <li>Add the possibility to customize the log format (#4558) [3891345c]</li> <li>Add Wave pairing via websockets (#4624) [cf16292e]</li> <li>Add dashboard stats date filter (#4575) [86e95d3e]</li> <li>Add AWS_MAX_ATTEMPTS and AWS_RETRY_MODE to Batch launch environment (#4738) [e7ec2c96]</li> <li>Allow S3 PutObjectTagging to instance role created by Tower Forge (#4511) [c8c8e76a]</li> <li>Allow exact match search filters (#4396) [d90acc18]</li> <li>Allow to share a Tower Agent connection (#4395) [1cfaee91]</li> <li>Allow the customisation prefix of Tower Forge resources (#4693) [67072462]</li> <li>Move workflow deletion audit event to service method (#4531) [9b56ad79]</li> <li>Remove required check from \"headQueue\" field in grid platform providers (#4655) [782ab02d]</li> <li>Improve Fusion v2 support for EBS disk (#4740) [e1d280d1]</li> <li>Improve config properties documentation reference (#4757) [01d08d9d]</li> <li>Improve support for AWS SSM as Params store (#4824) [3e2c568d]</li> <li>Improve trace service removing blocking queue (#4427) [1c788612]</li> <li>Increase 10 min length for pwd hint (#4606) [6adaba95]</li> <li>Deprecate Fusion v1 (#4694) [74fb5bd6]</li> <li>Fix: partial failure workflow status icon shows green check (#4371) [981aeb26]</li> <li>Fix: Missing AWS Cloudstream logs (#4476) [3d88a618]</li> <li>Fix: NPE when retrieving progress usage data  (#4621) [85c4836c]</li> <li>Fix: bug that throws ConcurrentModifactionException while cancelling tasks (#4656) [9d0eda97]</li> <li>Fix: cancellation of a workflow already terminated (#4622) [d665beeb]</li> <li>Fix: \"Row size too large\" MySQL problem (#4688) [793471da]</li> <li>Fix: Incorrect loading of Runs page after launching a pipeline (#4530) [b297bf42]</li> <li>Fix: make OAuth 2 cookies secured (#4478) [904e1e2d]</li> <li>Fix: Return HTTP 503 error when Redis is not available (#4605) [fa88e17d]</li> <li>Fix: Set name on FSx file system create by Tower (#4393) [cb631a72]</li> <li>Fix: Dataset's page CSV viewer crashes if there is an empty column and first row as header is checked (#4489) [21275c9a]</li> <li>Fix: Handle unexpected error when accessing Azure repos with node creds (#4707) [6df882f0]</li> <li>Fix: Relaunch workflow form does not populate the CE field if initial CE was deleted (#4538) [1fc7494f]</li> <li>Fix: do not show incomplete text on Launchpad for Launcher users (#4495) [1ed88626]</li> <li>Fix: navigate to Pipeline detail from \"Pipeline successfully saved\" notification (#4774) [0420c0d8]</li> <li>Fix: prevent changing launch work dir inside pipeline input form (#4408) [496827ea]</li> <li>Fix: properly display default Launchpad sort option (#4492) [1e712aa3]</li> <li>Fix: remove duplicate ECS config input in AWS CE form (#4423) [52d057ab]</li> <li>Fix: remove secrets controls if CE does not support them (#4714) [f3137ca7]</li> <li>Fix: restore Launchpad loading indicator (#4509) [9b3fbc26]</li> <li>Fix: sanitize characters in job and workflow error text messages (#4712) [0e6b2b7d]</li> <li>Fix: tag correctly compute environment and service role when resource (#4379) [ed96b5a6]</li> <li>Fix: workflow deletion failure when has a launch record associated (#4786) [9778e579]</li> <li>Fix: workflow launch form autoselects CE when workspace is shared (#4744) [9b6c5df8]</li> <li>Bump version nf-launcher:j17-23.04.1 [72eaa795]</li> <li>Bump Micronaut to version 3.8.5 (#4324) [79c1e50c]</li> </ul>","title":"23.1.0 - 28 Apr 2023"},{"location":"enterprise/release_notes/changelog/#2242-21-feb-2023","tags":["changelog"],"text":"<ul> <li>Fix: issue retrieving execution logs from CloudWatch (#4476) [638513b7]</li> <li>Fix: issue setting AWS CloudWatch custom log group name (#4475) [f020719c]</li> <li>Chore: Improve cloudwatch labels (#4498) [69b790fa]</li> <li>Bump: nf-launcher:j17-22.10.7 [a7b6fd26]</li> </ul>","title":"22.4.2 - 21 Feb 2023"},{"location":"enterprise/release_notes/changelog/#2241-10-feb-2023","tags":["changelog"],"text":"<ul> <li>fix: add auto height to selectable columns (#4409) [e4f62488]</li> <li>fix: remove duplicate ECS config input in AWS CE form (#4423) [3dbb0dad]</li> </ul>","title":"22.4.1 - 10 Feb 2023"},{"location":"enterprise/release_notes/changelog/#2240-6-feb-2023","tags":["changelog"],"text":"<ul> <li>Feat: All workflow runs page (#3777) [b89ba895]</li> <li>Feat: Refresh the dashboard data every 5 seconds (#3935) [dd65935d]</li> <li>Feat: Support for Gitea provider (#3995) [c1640d7b]</li> <li>Feat: Allow resume workflow in different CEs having compatible work directory (#4169) [ae782606]</li> <li>Feat: AWS Batch ECS custom configuration [1a5faf12]</li> <li>Feat: Wave pairing naming refactor (#4300) [c9c9dc8f]</li> <li>Feat: Pipeline and workflow resource labels customization (#3955) [b1fa9756]</li> <li>Add <code>europe-west2</code> location for Google Batch (#4203) [8bfda45c]</li> <li>Add missing lvm2 package to be able to mount multiple NVMe disks as a single volume (#4091) [4ffba872] [80c72e1c]</li> <li>Add support for custom CloudWatch logs group name (#3866) [97156c57]</li> <li>Add missing sourceWorkspaceId OpenAPI parameters (#4050) [72fa1822]</li> <li>Add Fusion NVMe support (#3942) [9ea72cc9]</li> <li>Add credentials/keys endpoint [25906e00]</li> <li>chore: gh actions workflows updates to suppress deprecation warnings (#4342) [b9bce117]</li> <li>chore: implements patch gcp registry credentials to remove newlines (#4307) [aed573ff]</li> <li>chore: Increase pipeline/projectName limit to 200 chars (#4317) [beb79d8d]</li> <li>chore: Run status time enhancements (#4289) [69848871]</li> <li>chore: Task 2882/add validation for custom role (#4068) [86967166]</li> <li>chore: Increase prod labels limit to 1k [9e5cccd9]</li> <li>chore: Update AWS regions (#4118) [99d7f6bb]</li> <li>chore: Revert FSx unmount (#4177) [199eb24b]</li> <li>chore: When running with Gitpod, create valid AWS credentials with assume role (#4114) [7ed1491f]</li> <li>chore: Improve task duration stats (#4106) [4a51d956]</li> <li>chore: update workflow status timing messages (#4075) [9939145b]</li> <li>chore: [BREAKING] remove autoinjection of roles when allowInstanceCredentials property is true (#4093) [5de61137]</li> <li>chore: Limit the time range selection when querying stats  (#3993) [e273130d]</li> <li>chore: Cache restore and backup via Tower plugin (#3599) [719442fb]</li> <li>chore: Set BEST_FIT_PROGRESSIVE as default AWS Batch allocation strategy (#3956) [6442dcd8]</li> <li>test: Create Playwright e2e tests for Google Life Sciences CEs (#3899) [ba1c1254]</li> <li>Fix: prevent calling BE with undefined workspaceId (#4349) [25943dd3]</li> <li>Fix: Re-launch of Tower actions should preserve parameters (#4270) [2a232104]</li> <li>Fix iframe for HTML reports (#4135) [bb878118]</li> <li>Fix: Azure CE creation fails in CI because of auto-scaling formula (#4180) [35543dd4]</li> <li>Fix: replace clr running color with the proper primary one (#4222) [8e424cf5]</li> <li>Fix: add explicit 'Authorization' header as param name to security scheme in order to fix issue with wrong header in OpenAPI GUI requests (#4218) [48fdb2fa]</li> <li>Fix: improve search syntax error handling (#4020) [a19ffbd5]</li> <li>Fix AWS Batch kernel issue causing OOM error (#4015) [7a8c5488]</li> <li>Fix: move authentication method to private app for hubspot (#3960) [a8aa6e79]</li> <li>Fix: additional joins for audit publisher entity (#3934) [23deb598]</li> <li>bump Upgrade to Java 17  (#3973) [2e915336]</li> <li>Bump nextflow 22.10.6 in get started page [28f44796]</li> <li>Bump nf-launcher:j17-22.10.6 [570658c5]</li> <li>Bump Upgrade backend to Micronaut 3.7 (#3876) [11203a05]</li> </ul>","title":"22.4.0 - 6 Feb 2023"},{"location":"enterprise/release_notes/changelog/#2232-9-feb-2022","tags":["changelog"],"text":"<ul> <li>nf-launcher:j17-tw-22.3-nf-22.10.6</li> </ul>","title":"22.3.2 - 9 Feb 2022"},{"location":"enterprise/release_notes/changelog/#2022","tags":["changelog"],"text":"","title":"2022"},{"location":"enterprise/release_notes/changelog/#2231-12-dec-2022","tags":["changelog"],"text":"<ul> <li>Bump nf-launcher:j17-22.10.4 [26da757f]</li> <li>Fix: Remove autoinjection of roles when allowInstanceCredentials property is true [BREAKING] (#4093) [1d6adc9f]</li> <li>Fix AWS Batch kernel issue causing OOM error (#4015) [f59b9edd]</li> </ul>","title":"22.3.1 - 12 Dec 2022"},{"location":"enterprise/release_notes/changelog/#2230-4-nov-2022","tags":["changelog"],"text":"<ul> <li>Add support for Google Batch (#3532) [ba641280]</li> <li>Add support for Resource Labels (#3511) [1fa2dc7e]</li> <li>Add support for Resource Labels for Google Batch (#3836) [157f3cd8]</li> <li>Add support for Wave + Fusion (#3713) [0f49a7cb]</li> <li>Add users and orgs management to admin panel (#3659) [9fda24b6]</li> <li>Add ability to  expand boot EBS volume size (#3299) (#3425) [b523c5dc]</li> <li>Add Runs dashboard page (#3734) [35073fdb]</li> <li>Add support for txt reports preview (#3862) [bba73371]</li> <li>Add confirmation dialog enhancements (#3470) [bd19b70d]</li> <li>Add unmount FSx lustre filesystem on SPOT instance termination (#3430) [155c8a7b]</li> <li>Add run detail page link to both html and txt email templates (#3907) [58f5ef4e]</li> <li>Add Allow organization owners to access all workspaces in the organization (#3703) [a0fad25f]</li> <li>Fix: 3423 optimization configuration not retained on re launch (#3841) [19b4bbe4]</li> <li>Fix: 3654 regression optimization column in workflow list lost (#3655) [10471ade]</li> <li>Fix: 3769 delete confirmation message allows prompts to be bypassed without entering delete in the text box (#3770) [ba442e24]</li> <li>Fix: 3773 invalid unit for vol ctxt and inv ctxt at tasks table (#3774) [0e34ae0e]</li> <li>Fix: BitBuckerServer Git provider #3670 [c91635b0]</li> <li>Fix: container registry name (#3708) [1f42959e] [9dd37809]</li> <li>Fix: missing file existance check for GLS in nf-launcher [7ca43e51]</li> <li>Fix: resume functionality on Google Life Sciences (#3539) [10419c93]</li> <li>Fix: stalling on failing local submit (#3492) [ea82e5f4]</li> <li>Fix: \"Pre-run script\" errors are not displayed in the logs (#3484) [65134954]</li> <li>Fix: Cannot add optimization status to unknown response object (#3450) [ac1fd478]</li> <li>Fix: Invalid unit in the tasks table (#3714) [53399902]</li> <li>Fix: Resume does not work when user has \"launch\" permission (#3072) [15433b31]</li> <li>Fix: Unable to save status for job when a DB exception occurs (#3490) [9788ace0]</li> <li>Fix: escape qstat command for Altair PBS batch scheduler (#3489) [adb2b773]</li> <li>Fix: failing test due to phantom job interval on Mysql (#3537) [b4249066]</li> <li>Fix: trim sub-second precision from dates (mysql compat) (#3788) [2ade7174]</li> <li>Fix: disallow dashes in secret names (#3643) (#3644) [81d09056]</li> <li>Fix: invalid job transition to unknown status [ci fast] [65f44fc2]</li> <li>Fix: Resource label input parses whole word before '=' (#3847) [12d6b09d]</li> <li>Make stage url config (#3700) [b7219259]</li> <li>Open up all endpoints and parameters related to labels and resource labels (#3814) [bf9a30e8]</li> <li>Restyling of workflow detail header (#3547) [d2024f66]</li> <li>Update xpack urls [BREAKING] [700436e5]</li> <li>[BREAKING] Add batch:TagResource to Batch instance role [dba6cb34]</li> <li>chore: restore workflow reports messaging (#3802) [9640254a]</li> <li>chore: Bad request response when query parameters are malformed (#3649) [376def9c]</li> <li>fix: admin tests race condition (#3868) [977fbff1]</li> <li>fix: Gray screen when navigating back after opening a task detail (#3873) [3a04872a]</li> <li>fix: add ListWorkspaceSettings permission to admin and maintainer (#3453) [d9bae03a]</li> <li>fix: added new query for star row deletion and modified test (#3514) [459bc3e4]</li> <li>fix: broken labels input formcontrol binding (#3656) [139e633a]</li> <li>fix: broken quick-launch page layout on personal workspace (#3495) [ebd0cf11]</li> <li>fix: Increase the quota limit for datasets (and dataset versions) per workspace to 100 (#3673) [818c4bf5]</li> <li>fix: bypass name checks if the label name has not changed (case-insensitive) (#3578) [293e5478]</li> <li>fix: case-insensitive search for orgs and users (#3739) [766d8056]</li> <li>fix: datasets suggestions for pipelines with schemas that expect tsv type (#3582) [1f48e0de]</li> <li>fix: disable ngx-bootstrap collapsible component animation (#3727) [a13dcecc]</li> <li>fix: highlight support nav button when in welcome page (#3798) [312f1c91]</li> <li>fix: humanize values for duration and realtime in tasks table (#3707) [30819908]</li> <li>fix: include personal workspace as possible value for last accessed workspace item in local storage (#3885) [151b708a]</li> <li>fix: inconsistent navigation to an organization when the organization name matches a resource label name [e2e] (#3685) [d045ac0c]</li> <li>fix: inherit from DataSpecification (#3745) [be04f004]</li> <li>fix: lazy load workflow details page main tabs (#3857) [8e504625]</li> <li>fix: make routing service always get routeContext from params when requested [e2e] [ci fast] [a6baca64]</li> <li>fix: Check for <code>workspace id</code> in the endpoint URL of an action in the workspace context (#3464) [db80231f]</li> <li>fix: move credentials keys patching/removal logic into credentials component base (#3765) [4537df99]</li> <li>fix: prevent double task endpoint invocation (#3830) [49a14f6e]</li> <li>fix: prevent null reference exception on humanizeCounter formatter util (#3785) [e5810dec]</li> <li>fix: redirect to personal workspace if user is not a participant in any workspace (#3683) [21e98d3c]</li> <li>fix: redirect to the last route on login after jwt token failed to refresh (#3619) [7ec1cfe3]</li> <li>fix: remove deprecated share button (#3496) [55b47cd3]</li> <li>fix: restore inline credentials creation functionality for grid platforms (#3542) [8707a05e]</li> <li>fix: restore moab platform icon (#3821) [e6688528]</li> <li>fix: restore tasks table column formatters after migration to mat table [e2e] (#3787) [009f5e76]</li> <li>fix: restore vertical scroll inside inputs (#3853) [df65253c]</li> <li>fix: set resume param depending on workflow completion status [e2e] (#3572) [177a7805]</li> <li>fix: show actionable error message on unparsable config file (#3451) [3cfd96d6]</li> <li>fix: small visual bugs fixes (#3837) [ci fast] [1b0a685b]</li> <li>fix: wrong launchpad layout when pipeline names are long (#3527) [318b02b3]</li> <li>tweak: switch typing method to help prompt display (#3698) [c31b02f3]</li> <li>tweak: Required/optional field labels enhancement (#3544) [e7f08557]</li> <li>tweak: allow path variables for grid platform launch directory field (#3883) [6ed1eba6]</li> <li>tweak: apply standard glob sorrounding to task list search (#3672) [d66c8740]</li> <li>tweak: Check that users with invalid names are not rejected when registering (#3816) [02e89664]</li> <li>tweak: move repo link to repo name in workflow detail header (#3564) [44fdd0ba]</li> <li>tweak: pass date filters when clicking on the run stat inside the dashboard page (#3901) [5cfda8b6]</li> <li>tweak: remove confirmation input from cancel workflow prompt [5d3aca10]</li> <li>tweak: remove redundant logs.length from log view (#3446) [794cbe3b]</li> <li>tweak: set max length of revision field to 100 characters (#3882) [4040166c]</li> <li>tweak: Enable angular strict template checking (#3596) [a569e5fe]</li> <li>tweak: Display provider icon in credentials/CE selection dropdowns, encapsulate in icon component (#3690) [8a4c7ffd]</li> <li>tweak: Do not allow email using a top-level domain hostname (#3526) [0bae08ca]</li> <li>tweak: Email validators are out of sync (FE side) (#3778) [0d564656]</li> <li>tweak: Establish use of english locale globally in tower-web (#3679) [a40d0483]</li> <li>tweak: customize the head node resources in the launch/relaunch form (#3448) (#3449) [42caa475]</li> <li>tweak: update pages layout (#3481) [24fc32cf]</li> <li>tweak: Improve SSH connector resilience + UGE qstat [cbdab74d]</li> <li>Bump nf-launcher:j17-22.10.1 [ci fast] [bfc1ea0d]</li> <li>Bump angular 14 (#3660) [130f0ffc]</li> </ul>","title":"22.3.0 - 4 Nov 2022"},{"location":"enterprise/release_notes/changelog/#2224-2-sept-2022","tags":["changelog"],"text":"<ul> <li>Bump the quota limit for dataset per workspace to 100 (#3673) [c8df0e6]</li> <li>Fix BitBuckerServer Git provider #3670 [3b4172b]</li> </ul>","title":"22.2.4 - 2 Sept 2022"},{"location":"enterprise/release_notes/changelog/#2223-11-aug-202","tags":["changelog"],"text":"<ul> <li>Rollback to nf-launcher:j17-22.06.1-edge [135f5d59]</li> </ul>","title":"22.2.3 - 11 Aug 202"},{"location":"enterprise/release_notes/changelog/#2222-8-aug-202","tags":["changelog"],"text":"<ul> <li>Bump nf-launcher@22.08.0-edge [786d43be]</li> <li>Fix resume functionality on Google Life Sciences (#3539) [5b2a50b7]</li> <li>fix: remove deprecated share button (#3496) [5af149f8]</li> </ul>","title":"22.2.2 - 8 Aug 202"},{"location":"enterprise/release_notes/changelog/#2218-8-aug-2022","tags":["changelog"],"text":"<ul> <li>Fix resume functionality on Google Life Sciences (#3539) [5b389773]</li> </ul>","title":"22.1.8 - 8 Aug 2022"},{"location":"enterprise/release_notes/changelog/#2221-5-aug-2022","tags":["changelog"],"text":"<ul> <li>feat: unmount FSx lustre filesystem on SPOT instance termination</li> <li>fix: escape qstat command for Altair PBS batch scheduler</li> <li>fix: improve SSH connector resilience + UGE qstat</li> <li>fix: Patch invalid job transition to unknown status</li> </ul>","title":"22.2.1 - 5 Aug 2022"},{"location":"enterprise/release_notes/changelog/#2217-25-jul-2022","tags":["changelog"],"text":"<ul> <li>Improve SSH connector resilience + UGE qstat [755b6ce4][8e876d22]</li> </ul>","title":"22.1.7 - 25 Jul 2022"},{"location":"enterprise/release_notes/changelog/#2220-15-jul-2022","tags":["changelog"],"text":"","title":"22.2.0 - 15 Jul 2022"},{"location":"enterprise/release_notes/changelog/#breaking-changes","tags":["changelog"],"text":"<ul> <li>The MySql DB driver com.mysql.cj.jdbc.Driver has been replaced by org.mariadb.jdbc.Driver.   Env variable <code>TOWER_DB_DRIVER</code> referencing the first should be changed with the latter.</li> </ul>","title":"Breaking Changes"},{"location":"enterprise/release_notes/changelog/#other-changes","tags":["changelog"],"text":"<ul> <li>feat: Added support for Illumina DRAGEN</li> <li>feat: Added support to mysql8</li> <li>feat: Allow access remote pipelines via Tower Agent</li> <li>feat: Feature 3025 reports download limit</li> <li>feat: Adds used datasets tab to run details page</li> <li>feat: Add support for redis password</li> <li>feat: pipeline reports index page</li> <li>feat: Feature 2663 / Labels</li> <li>feat: Add support for AWS CodeCode repositories</li> <li>feat: runName filled with random run name by default if not in relaunch mode</li> <li>feat: Allow the ability to send cluster options from head queue to child nodes</li> <li>feat: Add advanced search capabilities to runs page</li> <li>fix: error when trying to remove unexistent csv renderer options component</li> <li>fix: invalid escape of blank chars in URL [ci fast]</li> <li>fix: issue download report with blanks</li> <li>fix: Nginx proxy pass decoding</li> <li>fix: Solves #3077 by modifying the validation logic</li> <li>fix: set dataset file mime type depending on file extension</li> <li>fix: Use mdiag command to check MOAB platform</li> <li>fix: Do not force a main.nf file at default branch when creating a pipeline</li> <li>fix: suggest valid runName when launcher resumes</li> <li>fix: Populate timestamps for partial workflow progress updates</li> <li>fix: Enable maintainers to create workspace secrets</li> <li>fix: prevent infinite redirection when landingUrl = applicationUrl</li> <li>fix: Change MOAB queue status command</li> <li>fix: hide workflow run datasets tab in the personal workspace context</li> <li>fix: Add the support for USR2 signal for grid providers launcher script</li> <li>fix: Fix perms for encrypted bucket</li> <li>fix: missing dataset in workflow run page</li> <li>fix 3309 compute environment not visible when viewing actions</li> <li>fix: multiple dropdown menus remain when selecting</li> <li>fix: IllegalArgumentException on empty config file</li> <li>fix: Can't re-launch failed workflow without commit</li> <li>fix: cancel button malfunctions in most menus where elements get added</li> <li>fix: prevent the deletion of a CE when the status is CREATING.</li> <li>fix: produce two different entries for custom user config and optimized config.</li> <li>fix: tweak: remove \"None\" item from select inputs when the field is required</li> <li>fix: fixed the case when optimization config was not shown for workflow details page</li> <li>fix: Disallow relative path workdir</li> <li>fix: Use NotFound exception at Google LS provider</li> <li>chore: update ENVIRONMENTVARIABLE_NAME regex to allow NXF env variables</li> <li>chore: update computeJobRole and headJobRole validation</li> <li>chore:Bump ebs-autoscale to version 2.4.6-6ce65d32 [ci fast]</li> <li>chore: Add KMS permissions required by EBS autoscale</li> <li>chore: Upgrade to Micronaut 3.4.x</li> <li> <p>chore: typography sync between tower and design</p> </li> <li> <p>Full Changelog: v22.1.5-enterprise...v22.2.0-rc0-enterprise</p> </li> </ul>","title":"Other Changes"},{"location":"enterprise/release_notes/changelog/#2216-15-jul-2022","tags":["changelog"],"text":"<ul> <li>Patch invalid job transition to unknown status [5ac1a4fd]</li> </ul>","title":"22.1.6 - 15 Jul 2022"},{"location":"enterprise/release_notes/changelog/#2215-7-jun-2022","tags":["changelog"],"text":"<ul> <li>Fix perms for encrypted bucket [96f00f39]</li> <li>Add the support for USR2 signal to launcher script [40c4ab68]</li> </ul>","title":"22.1.5 - 7 Jun 2022"},{"location":"enterprise/release_notes/changelog/#2214-1-jun-2022","tags":["changelog"],"text":"<ul> <li>Enable maintainers to create workspace secrets [2d1a225f]</li> <li>Forward revision when creating a pipeline (#3203) [2ff2f171]</li> <li>Change MOAB queue status command (#3219) [4eda7f90]</li> </ul>","title":"22.1.4 - 1 Jun 2022"},{"location":"enterprise/release_notes/changelog/#2213-18-may-2022","tags":["changelog"],"text":"<ul> <li>Update Nextflow to 22.04.3</li> <li>Bump nf-launcher:j17-22.04.3</li> <li>Bump nf-jdk:corretto-11.0.15_up1</li> </ul>","title":"22.1.3 - 18 May 2022"},{"location":"enterprise/release_notes/changelog/#2212-9-may-2022","tags":["changelog"],"text":"<ul> <li>fix: Add KMS permissions required by EBS autoscale with encrypted volumes [387ed6c3]</li> <li>fix: Update HTTP content security policy to allow host URLs for frames and workers [327b27ac]</li> <li>fix: Minor navigation error when removing unexistent CSV renderer component [a501225c]</li> <li>fix: Issue downloading a report with containing a blank character [70ab2033]</li> <li>fix: Nginx proxy pass decoding break query parameters with blank character [48a2ef6b]</li> <li>fix: Kubernetes control plan URL only allow host name [1b4b1240][da552afc]</li> <li>Bump ebs-autoscale to version 2.4.6-6ce65d32 [d52de172]</li> </ul>","title":"22.1.2 - 9 May 2022"},{"location":"enterprise/release_notes/changelog/#2211-25-apr-2022","tags":["changelog"],"text":"<ul> <li>Add EBS encrypt role policy at AWS forge creation (#2817) [ci fast]</li> <li>Add TOWER_ENABLE_UNSAFE_MODE setting to allow cookies over HTTP (#3023) [69100d51]</li> <li>Allow NXF env variables (#3026) [553015d8]</li> <li>Fix azcopy cache commands (#3022) [4932a54d]</li> <li>Fix AES regular expression [a7b1ed02]</li> <li>Fix update CSP to allow captcha frame [a984ec39]</li> <li>Fix download of task log files (#3004) [3389a8d7]</li> <li>Fix Dataset table is not rendered in Safari [ab297ff4]</li> <li>Fix avoid analytics service making calls when there's no analytics url (#2991) [fd56afab]</li> <li>Fix NF version in welcome page [c5c8492c]</li> <li>Remove log trace from workflow limiter [64f8161e]</li> <li>Improve cloud price download logs [a0e69b57]</li> <li>Bump nf-launcher:j17-22.04.0 [9e55c873]</li> <li>Bump nf-jdk:corretto-11.0.15 as base image [1832c282]</li> </ul>","title":"22.1.1 - 25 Apr 2022"},{"location":"enterprise/release_notes/changelog/#2210-12-apr-2022","tags":["changelog"],"text":"<ul> <li>Add secure cookies [e28a3388]</li> <li>Add GetLogsEvents perm to AWS Batch instance role [04b18668]</li> <li>Add credentials view page [f3c63483]</li> <li>Add ECS pull strategy in user-data template [e1b4914a]</li> <li>Add root users environment when <code>TOWER_ROOT_USERS</code> variable is provided [e09db3e5]</li> <li>Add Tower system message</li> <li>Add support for JSON formatted logs [92122adb]</li> <li>Add support for AWS agent and logging [6e68fd98][c080e9d4]</li> <li>Add navigate back button to second level screens (#2578) (#2623) (5 weeks ago)</li> <li>Add validation for SSH hostname and username [d0115de0][efb962bf]</li> <li>Add config option to disable user private workspace [9e667bc0]</li> <li>Add share run deprecation banner</li> <li>Allow partial searches [b8788b38]</li> <li>Allow the use S3 bucket work dir along with EFS or FSx mounts [368d5caa]</li> <li>Upload encrypted files at AWS S3 [40b87a2e]</li> <li>Use default listening port (80) [a64852d9]</li> <li>Improve secrets obfuscation in log file [7e52c76b]</li> <li>Improve EBS autoscaling [fe7fe728]</li> <li>Increase tower config max size to 3500 character [a01ee72c]</li> <li>Disable resume for failed workflows [3c2c7ad3]</li> <li>Set max length validator to the workflow launch form fields [5326114b]</li> <li>Check valid EFS and FSx mount points [633fdcd8]</li> <li>Make Dataset api public (#2240) [2fd32c51]</li> <li>Upgrade Angular 13</li> <li>Upgrade Micronaut 3.x (#2364)</li> <li>Upgrade logback to version 1.2.8 (#2418)</li> <li>Bump log4js from 6.3.0 to 6.4.0 in /tower-web (#2535)</li> <li>Bump base image nf-jdk:corretto-11.0.14_2</li> <li>Bump nf-launcher 22.03.1-edge</li> <li>Increase agent websockets payload size to 5Mb [5f3e5428]</li> <li>484005bd - Always retry NF process when using AWS sport instances</li> <li>fe7fe728 - Improve EBS autoscaling (8 days ago)</li> <li>8dc800c2 - feature: improve parse the pipeline schema</li> <li>Default to Nextflow DSL version 1 [e88a3e59]</li> <li>Fix job status is updated in the in-memory tracker before running the job in the local CE platform (#2460) (3 months ago)</li> <li>Fix normalize dataset name [fcbe417d]</li> <li>Fix Allow dot in AWS ARN string [d5c5cd9e]</li> <li>Fix issue with K8s compute env stalling in creating status [72c03cd9]</li> <li>Fix set cookie acceptance cookie path to / e2e [ba0cae7a]</li> <li>Fix EFS and FSx permission when job run with non-root user (#2659) [0e169bb9]</li> <li>Fix reports at grid and agent platforms [ba397137]</li> <li>Fix load SLURM CE details in view mode [80cc0e9b]</li> <li>Fix Display dates with YYYY-MM-DD format on runs page [830606af]</li> <li>Fix Unable to download execution log from a workflow with working directory specified just as \"bucket\" name [d025917c]</li> <li>Fix Prevent the creation of Spot fleet role [95acea2c]</li> <li>Fix Prevent deletion of an active workflow run [ba1f1ce9]</li> <li>Fix Prevent XSS attacks when uploading a datatable file (#2944)[6d98210c]</li> <li>8759d92e - Validate launch/re-launch action depending the user role</li> <li>d6113805 - Bump base image nf-jdk:corretto-11.0.14_2 [ci fast]</li> </ul>","title":"22.1.0 - 12 Apr 2022"},{"location":"enterprise/release_notes/changelog/#21123-31-mar-2022","tags":["changelog"],"text":"<ul> <li>Bump base image nf-jdk:corretto-11.0.14_2 [8cc71b91]</li> </ul>","title":"21.12.3 - 31 Mar 2022"},{"location":"enterprise/release_notes/changelog/#21122-31-mar-2022","tags":["changelog"],"text":"<ul> <li>Create /.nextflow folder in backend container [333a8a68]</li> <li>Fix: issue with K8s compute env stalling in creating status [3745e793]</li> <li>Fix: Upload encrypted files at AWS S3 [716d2938]</li> <li>Fix: EFS/FSx permission when using non-root container (#2659) [002f4426]</li> <li>Bump nf-launcher:j17-21.10.6 [8b3d6490]</li> </ul>","title":"21.12.2 - 31 Mar 2022"},{"location":"enterprise/release_notes/changelog/#21121-3-feb-2022","tags":["changelog"],"text":"<ul> <li>Disable H8 stats verbose logging [7e5e08b0]</li> <li>Enable root users environment when TOWER_ROOT_USERS variable is provided [390e079a]</li> <li>Fix: reports endpoint exception on NF cli workflows [c310c3cf]</li> </ul>","title":"21.12.1 - 3 Feb 2022"},{"location":"enterprise/release_notes/changelog/#21120-17-jan-2022","tags":["changelog"],"text":"<ul> <li>Add shared workspace feature</li> <li>Add Pipeline reports feature [preview]</li> <li>Add Dataset public APIs</li> <li>Add Tower agent reverse connection</li> <li>Add Dataset api public ci fast [942f3f4e3]</li> <li>Prevent the use of master as default branch (#2499) [791f45a11]</li> <li>Update resources descriptions [538069df9]</li> <li>Allow the use of S3 as work directory when using EFS and FSx mounts [6199d8fd6]</li> <li>Display dates with YYYY-MM-DD format on runs page [5d2f3215a]</li> <li>Change email template office address [5edfa3a26]</li> <li>Increase agent websockets payload size to 5Mb [4ce9af8f2]</li> <li>Fix: auto-normalize inline credentials name (#2405) [9d5453716]</li> <li>Fix: prevent making multiple get pipeline info requests in workflow launch form ci skip [e21619bd1]</li> <li>Fix: set 'Launch.resumeLaunchId' only if it's a resume. (#2427) [a784e635b]</li> <li>Fix: possible connectionId null reference exception ci skip</li> <li>Bump nf-launcher 21.10.4 based on correto:17.0.1 based image</li> </ul>","title":"21.12.0 - 17 Jan 2022"},{"location":"enterprise/release_notes/changelog/#21103-3-feb","tags":["changelog"],"text":"<ul> <li>Enable root users environment when <code>TOWER_ROOT_USERS</code> variable is provided [0ba7190e0]</li> </ul>","title":"21.10.3 - 3 Feb"},{"location":"enterprise/release_notes/changelog/#2021","tags":["changelog"],"text":"","title":"2021"},{"location":"enterprise/release_notes/changelog/#21102-10-dec","tags":["changelog"],"text":"<ul> <li>Fix NPE error when marking unknown status</li> <li>Bump nf-launcher 21.10.5</li> </ul>","title":"21.10.2 - 10 Dec"},{"location":"enterprise/release_notes/changelog/#21101-8-dec","tags":["changelog"],"text":"<p>Note</p>  <p>The <code>21.10.x</code> release series starts with <code>v21.10.1</code>.</p> <ul> <li>Add Container registry creds for Azure</li> <li>Add Datasets feature</li> <li>Add Support custom CE environment variables</li> <li>New Workflows Runs list page</li> <li>Add support for custom landing page</li> <li>Add Display job info on workflow general panel (#2142) (#2151)</li> <li>Make hidden params a part of pipeline input form even if not shown + small fix (#2134)</li> <li>Parallelize Az metadata retrieval</li> <li>Fix Make hidden params a part of pipeline input form even if not shown + small fix (#2134)</li> <li>Fix Validate final values of config properties on startup (#2100)</li> <li>Fix redisson default connection pool size (#2229)</li> <li>Fix Return a bad request when workspaceId is not parsable (#2220) #2205</li> <li>Fix race condition on repo pull (#2110)</li> <li>Fix grid platform default launch dir (#2037)</li> <li>Fix Redirect to the Runs page after launch (#2057)</li> <li>Fix Discard deleted entities from name validation queries and rename them (#2052)</li> <li>Improve landing page config #1996 #748</li> <li>Fix Download hangs when streaming a S3 file (#2005)</li> <li>Hide ebsBlockSize field from aws manual config (#2004)</li> <li>Refactor Google LifeScience head job execution (#1981)</li> <li>Make sure to authenticate the Google storage (#1984)</li> <li>Use amazoncorretto:11.0.13 as base image</li> <li>Minor schema fetching improvement (#2183)</li> <li>Make sure the workflows list query returns the workflows in a workspace even if they have been starred by other users (#2174)</li> <li>Bump nf-launcher 21.10.4</li> </ul>","title":"21.10.1 - 8 Dec"},{"location":"enterprise/release_notes/changelog/#21065-10-dec","tags":["changelog"],"text":"<ul> <li>Increasing the throttling rate on the ECS agent metadata endpoint (#2338) [51a519691]</li> <li>Bump nf-launcher 21.10.5</li> </ul>","title":"21.06.5 - 10 Dec"},{"location":"enterprise/release_notes/changelog/#21064-25-nov","tags":["changelog"],"text":"<ul> <li>Increasing the throttling rate on the ECS agent metadata endpoint (#2338)</li> <li>Bump nf-launcher 21.10.3</li> </ul>","title":"21.06.4 - 25 Nov"},{"location":"enterprise/release_notes/changelog/#21063-19-nov","tags":["changelog"],"text":"<ul> <li>Bump nf-launcher 21.10.1</li> </ul>","title":"21.06.3 - 19 Nov"},{"location":"enterprise/release_notes/changelog/#21062-5-oct","tags":["changelog"],"text":"<ul> <li>Fix race condition on repo pull when using Kubernetes platform (#2110) [90b1dbe7c]</li> <li>Hide <code>ebsBlockSize</code> field from aws manual config (#2004) [24650f47c]</li> <li>Connection pool properties log can leak sensitive data [1878a2e4f]</li> <li>Changing workspace multiple requests fix (#1985) [5f66be4b0]</li> <li>Make sure to authenticate the Google storage (#1984) [398897422]</li> <li>Fix Altair infoCli method [f5226d03d]</li> </ul>","title":"21.06.2 - 5 Oct"},{"location":"enterprise/release_notes/changelog/#21061-27-aug","tags":["changelog"],"text":"<ul> <li>Fix OpenId attributes blows up response header [7b3c5ae58]</li> <li>Fix Mention in the Get started page how setup tower workspace id [b16ead35a]</li> <li>Fix Tune AWS client caching timeout f4ffe10d5</li> <li>Fix Pipeline params JSON parsing on Windows client #1949</li> <li>Fix Add better control of cron/redis config (#1944) [45579b5bd]</li> <li>Fix DB migration detected table on foreign schema - Bump migtool 1.0.2 705c905db</li> <li>Fix for the case when blur event handler was executed before chip selection event handler #1932</li> <li>Fix Bug 1926/Modify bootDiskSize Config Param #1931</li> </ul>","title":"21.06.1 - 27 Aug"},{"location":"enterprise/release_notes/changelog/#21060-26-jul","tags":["changelog"],"text":"<ul> <li>Add Support for AWS Host credentials and role-base permissions</li> <li>Add Support for AWS EFS storage</li> <li>Add ability to specify custom AWS cli path</li> <li>Add AWS regions eu-south-1 and af-south-1</li> <li>Add uploadChunkSize configuration parameter to abstract k8 provider (#1820)</li> <li>Limit compute env error message length</li> <li>Invalidate compute envs associated to deleted credentials</li> <li>Fix launch form pipelineParameters after navigating to pipeline input form (#1847)</li> <li>Fix error report for missing invalid/creds</li> <li>Fix GitHub action creation</li> <li>Fix Prevent GH delete action hook exception</li> <li>Display team id in team page</li> <li>Disable index.html caching in nginx.config</li> <li>Bump nextflow launcher 21.04.3</li> <li>Bump groovy 3.0.8</li> </ul>","title":"21.06.0 - 26 Jul"},{"location":"enterprise/release_notes/changelog/#21049-2-aug","tags":["changelog"],"text":"<ul> <li>Fix Nextflow plugins deps</li> </ul>","title":"21.04.9 - 2 Aug"},{"location":"enterprise/release_notes/changelog/#21048-14-jul","tags":["changelog"],"text":"<ul> <li>Improve error report for missing/invalid AWS creds [b5c550236]</li> <li>Do not trigger config profiles field reset after patching the workflow launch form (#1836) [f863c8cbb]</li> </ul>","title":"21.04.8 - 14 Jul"},{"location":"enterprise/release_notes/changelog/#21047-21-jun","tags":["changelog"],"text":"<ul> <li>Add head service account to deployment pod (#1773) [6e0e7281f]</li> <li>Parse profiles using the correct revision at launch time (#1572) [e78eda2f2]</li> </ul>","title":"21.04.7 - 21 Jun"},{"location":"enterprise/release_notes/changelog/#21046-21-jun","tags":["changelog"],"text":"<ul> <li>Change schema and default params usage (#1737) [f938c7236]</li> <li>Fix GitHub action creation (#1721) [b24f0cfb8]</li> <li>K8s use deployment for service pod (#1735) [1af27810c]</li> <li>Fixed the case when the dropdown was over-shadowing other fields (#1714) [baacf92cc]</li> </ul>","title":"21.04.6 - 21 Jun"},{"location":"enterprise/release_notes/changelog/#21045-8-jun","tags":["changelog"],"text":"<ul> <li>k8s head pod custom specs (#1668) [d82a864c8]</li> <li>Fix action update settings (#1679) [a485c5d75]</li> <li>Allow selecting empty values for schema dropdown fields (#1674) [d27e5b905]</li> </ul>","title":"21.04.5 - 8 Jun"},{"location":"enterprise/release_notes/changelog/#21044-3-jun","tags":["changelog"],"text":"<ul> <li>Fix missing scm server &amp; platform 3dc89ece5</li> </ul>","title":"21.04.4 - 3 Jun"},{"location":"enterprise/release_notes/changelog/#21043-2-jun","tags":["changelog"],"text":"<ul> <li>Fix pattern test validator when the value is empty (#1652) [3cc2923ec]</li> <li>Fix navigation dropdown display when user has no CreateOrganization permission (#1655) [5018fd956]</li> </ul>","title":"21.04.3 - 2 Jun"},{"location":"enterprise/release_notes/changelog/#21042-1-jun","tags":["changelog"],"text":"<ul> <li>Fix FSx creation failure</li> </ul>","title":"21.04.2 - 1 Jun"},{"location":"enterprise/release_notes/changelog/#21041-31-may","tags":["changelog"],"text":"<ul> <li>Add timeout on AWS Forge waiters</li> <li>Add log response to UI error message (#1602) [2d705289c]</li> <li>Reorganize login page (#1635) [9a46f393a]</li> <li>Add support for BitBucket server [8c09241e3]</li> <li>Fix Missing GitLab token creds (#1631) [c188a76b2]</li> <li>Fix action launch user (#1615) #1611 [691fe4c9d]</li> <li>Use RegExp.test for json schema pattern validation + small pipeline input form fix (#1619) [d9d2cd317]</li> <li>Do not config mail proxy using global proxy settings (#1626) [e1c8b1dab]</li> <li>Fix admin project security vulnerabilities (#1637) [972255faf]</li> </ul>","title":"21.04.1 - 31 May"},{"location":"enterprise/release_notes/changelog/#21040-21-may","tags":["changelog"],"text":"<ul> <li>New organizations feature</li> <li>New teams feature</li> <li>New workspace feature</li> <li>New launchpad feature</li> <li>Add support for private Git repositories</li> <li>Add support for Nextflow timeline downloads</li> <li>Fix issues with Compute environment status reporting</li> <li>Update Nextflow runtime to version 21.04.0</li> </ul>","title":"21.04.0 - 21 May"},{"location":"enterprise/release_notes/changelog/#21025-12-may","tags":["changelog"],"text":"<ul> <li>Fix S3 log downloads when using fusion feature</li> </ul>","title":"21.02.5 - 12 May"},{"location":"enterprise/release_notes/changelog/#21024-29-apr","tags":["changelog"],"text":"<ul> <li>Bump nexflow 21.04.0-edge required to fix Bitbucket server</li> </ul>","title":"21.02.4 - 29 Apr"},{"location":"enterprise/release_notes/changelog/#21023-21-apr","tags":["changelog"],"text":"<ul> <li>Fix cloud price downloader using Seqera Labs endpoint</li> <li>Fix Error message when S3 bucket is not accessible</li> </ul>","title":"21.02.3 - 21 Apr"},{"location":"enterprise/release_notes/changelog/#21022-14-apr","tags":["changelog"],"text":"<ul> <li>Fix missing commit ID when resuming execution fails to start</li> </ul>","title":"21.02.2 - 14 Apr"},{"location":"enterprise/release_notes/changelog/#21021-12-apr","tags":["changelog"],"text":"<ul> <li>Fix support for custom bitbucket servers</li> <li>Bump Nextflow launcher 21.04.0-edge</li> </ul>","title":"21.02.1 - 12 Apr"},{"location":"enterprise/release_notes/changelog/#21020-16-mar","tags":["changelog"],"text":"<ul> <li>Add Azure Batch provider</li> <li>Add Altair PBS pro provider</li> <li>Add sessionId to workflows search-box criteria</li> <li>Add support for multiple GLS zones</li> <li>Add Grid provider head job options</li> <li>Add support for AWS Batch cost percentage</li> <li>Add Azure Batch Forge</li> <li>Add support for Grid Engine batch scheduler</li> <li>Add K8s service pod</li> <li>Add support for Tower license</li> <li>Improve detection of NF config profiles #1074</li> <li>Fix issue on work dir path composition with ending slash</li> <li>Fix issue when retrieving non-existing file via SSH/SCP</li> <li>Fix issue resolving non-canonical GitHub/Gitlab project name #353</li> <li>Fix issue with AWS Batch allocation strategy #931</li> <li>Fix phantom job unknown status</li> <li>Fix Prevent requeuing of mail with invalid addresses</li> <li>Fix issue on creating AWS CE with manual config</li> <li>Update backend base image to corretto:11.0.10</li> <li>Upgrade to Angular 11</li> <li>Use Kubernetes Java-client 10.0.1</li> <li>Update nf-launcher to 21.03.0-edge</li> </ul>","title":"21.02.0 - 16 Mar"},{"location":"enterprise/release_notes/changelog/#20124-23-march","tags":["changelog"],"text":"<ul> <li>Add support for AWS marketplace</li> </ul>","title":"20.12.4 - 23 March"},{"location":"enterprise/release_notes/changelog/#20122-feb-16","tags":["changelog"],"text":"<ul> <li>Fix phantom job status</li> <li>Fix invalid email rejection</li> </ul>","title":"20.12.2 - Feb 16"},{"location":"enterprise/release_notes/changelog/#20121-21-jan","tags":["changelog"],"text":"<ul> <li>Fix head job submission to head queue when using batch schedulers eg. Slurm, GridEngine, LSF.</li> </ul>","title":"20.12.1 - 21 Jan"},{"location":"enterprise/release_notes/changelog/#20120-11-jan","tags":["changelog"],"text":"<ul> <li>Add support for Kubernetes clusters</li> <li>Add support for AWS EKS clusters</li> <li>Add support for Google GKE clusters</li> <li>Add support for Launch stub-run feature</li> <li>Add AWS Batch Fusion mounts</li> <li>Enhanced security, API uses bearer auth</li> <li>System security improvements</li> <li>Upgrade Java runtime to version 11</li> <li>Upgrade Micronaut runtime to version 2.1</li> <li>Upgrade Nextflow launcher to version 20.12.0-edge</li> </ul>","title":"20.12.0 - 11 Jan"},{"location":"enterprise/release_notes/changelog/#20104-11-jan","tags":["changelog"],"text":"<ul> <li>Backend container security fixes</li> <li>Improved SSH client debugging</li> </ul>","title":"20.10.4 - 11 Jan"},{"location":"enterprise/release_notes/changelog/#2020","tags":["changelog"],"text":"","title":"2020"},{"location":"enterprise/release_notes/changelog/#20103-30-nov","tags":["changelog"],"text":"<ul> <li>Fix migration tool when using MariaDB</li> <li>Fix execution issue with Batch forge when creating a new FSx file system</li> <li>Fix invalid warn message</li> </ul>","title":"20.10.3 - 30 Nov"},{"location":"enterprise/release_notes/changelog/#20102-2-nov","tags":["changelog"],"text":"<ul> <li>Add support for TOWER_LAUNCH_CONTAINER env var [6fd06581f]</li> <li>Allow pre-run script to modify launch env [56ed5cca1]</li> <li>Fix ebs autoexpand volume issue + add ebs block size [cbdb8b1af]</li> <li>Disable cache on problematic cached query (#608) [11ef28e10]</li> </ul>","title":"20.10.2 - 2 Nov"},{"location":"enterprise/release_notes/changelog/#20101-27-oct","tags":["changelog"],"text":"<ul> <li>Updated Nextflow launcher container</li> </ul>","title":"20.10.1 - 27 Oct"},{"location":"enterprise/release_notes/changelog/#20100-22-oct","tags":["changelog"],"text":"<ul> <li>Add Workflow sharing feature</li> <li>Add support for Slurm batch cluster</li> <li>Add support for IBM LSF batch cluster</li> <li>Add customizable navbar menu</li> <li>Add built-in support for MariaDB</li> <li>Add built-in support for Google SSO</li> <li>Add auth allow-list emails</li> <li>Update Java mail 1.6.2</li> <li>System security improvements</li> </ul>","title":"20.10.0 - 22 Oct"},{"location":"enterprise/release_notes/changelog/#20080-28-aug","tags":["changelog"],"text":"<ul> <li>Add support for AWS FSx mount name to Launch feature</li> <li>Add Batch Forge option to to Launch feature</li> <li>Add support for GPU instances to Launch feature</li> <li>Add Execution and tasks logs view &amp; downloads features</li> <li>Add support for Compute env AWS advanced options</li> <li>Add Compute environment primary option feature</li> <li>Add Pipeline Actions</li> <li>Add support for GA4GH WES API (beta)</li> <li>Add status &amp; process name filtering to dashboard</li> <li>Add favicon for dark theme</li> <li>Add login pass-through mechanism</li> <li>Fix AWS Batch workflow cancellation</li> <li>Fix issue when launching projects w/o config file</li> <li>Fix issue on port and scheme forwarding</li> <li>Fix local repositories configuration issue</li> <li>Improve Workflow general stats tooltips</li> <li>Update launch base image to AWS <code>corretto:262</code></li> <li>Update MN version 1.3.7</li> </ul>","title":"20.08.0 - 28 Aug"},{"location":"enterprise/release_notes/changelog/#20061-12-aug","tags":["changelog"],"text":"<ul> <li>Fix Reverse proxy scheme and port forwarding when using local docker environment</li> </ul>","title":"20.06.1 - 12 Aug"},{"location":"enterprise/release_notes/changelog/#20060-16-jun","tags":["changelog"],"text":"<ul> <li>Add Pipeline Launch feature</li> <li>Add Pipeline execution cancellation</li> <li>UI looks &amp; feel improvements</li> <li>Security improvements</li> <li>OAuth login improvements</li> <li>Add Tomcat DB connection pool</li> <li>Upgrade Micronaut runtime to 1.3.3</li> </ul>","title":"20.06.0 - 16 Jun"},{"location":"enterprise/release_notes/changelog/#20051-12-may","tags":["changelog"],"text":"<ul> <li>Path OpenID connect upgrading MN security to 1.2.3</li> <li>Add TOWER_SECURITY_LOGLEVEL env variable for security module debugging</li> </ul>","title":"20.05.1 - 12 May"},{"location":"enterprise/release_notes/changelog/#20050-28-apr","tags":["changelog"],"text":"<ul> <li>Add support for OpenId connect</li> <li>Improve K8s deployment descriptors</li> <li>Fix critical issue saving tasks</li> <li>Add limit to max records returned</li> <li>Fix invalid tag deserialization error</li> </ul>","title":"20.05.0 - 28 Apr"},{"location":"functionality_matrix/functionality_matrix/","tags":["compatibility","nextflow","nf-launcher"],"text":"","title":"Functionality matrix"},{"location":"functionality_matrix/functionality_matrix/#tower-nextflow-version-compatibility","tags":["compatibility","nextflow","nf-launcher"],"text":"<p>Each Tower version makes use of <code>nf-launcher</code> to determine the Nextflow version used as its baseline. This Nextflow version can be overridden with the <code>NXF_VER</code> environment variable in your <code>nextflow.conf</code> file, but note that Tower may not work reliably with Nextflow versions other than the baseline version.</p> <p>We officially support the two latest Tower major releases (22.3.x, 22.4.x, etc) at any given time.</p> <p>nf-launcher versions prefixed with j17 refer to Java version 17; j11 refers to Java 11.</p>    Tower version nf-launcher version Nextflow version     22.4.1 j17-22.10.6 22.10.6   22.4.0 j17-22.10.6 22.10.6   22.3.1 j17-22.10.4 22.10.4   22.3 j17-22.10.1 22.10.1   22.2.4 j17-22.06.1-edge 22.06.1-edge   22.2.3 j11-22.06.1-edge 22.06.1-edge   22.2.2 j17-22.08.0-edge 22.08.0-edge     <p>If no Nextflow version is specified in your configuration, Tower defaults to the baseline version outlined above.</p>","title":"Tower / Nextflow version compatibility"},{"location":"getting-started/deployment-options/","tags":["deployment"],"text":"<p>Tower is available in three versions:</p> <ul> <li> <p>Tower Cloud: The hosted version of Tower is available free of charge at tower.nf. This version is for individuals and organizations that want to get set up fast. It is the recommended way for users to become familiar with Tower. The service is hosted by Seqera Labs.</p> </li> <li> <p>Tower Community: The Community edition of Tower is open-source and can be deployed by anyone on their own infrastructure. The community edition has basic features for the monitoring of pipelines by an individual user.</p> </li> <li> <p>Tower Enterprise: The Enterprise edition of Tower contains the latest features and can be deployed in an organization's own cloud or on-premises infrastructure. This option includes dedicated support from Seqera Labs and is recommended for production environments.</p> </li> </ul>","title":"Deployment options"},{"location":"getting-started/deployment-options/#tower-cloud","tags":["deployment"],"text":"<p>To try Tower Cloud, visit tower.nf and log in with your GitHub or Google credentials. The Launching pipelines page provides step-by-step instructions to launch your first pipeline. Tower Cloud has a limit of five concurrent workflow runs per user.</p> <p></p>","title":"Tower Cloud"},{"location":"getting-started/deployment-options/#tower-community","tags":["deployment"],"text":"<p>For instructions to install the Community edition of Tower, visit the GitHub repository.</p> <p></p>  <p>Warning</p> <p>Tower Community does not include all the features of Tower Cloud and Tower Enterprise, such as Tower Launch, Organizations, and Workspaces.</p>","title":"Tower Community"},{"location":"getting-started/deployment-options/#tower-enterprise","tags":["deployment"],"text":"<p>Tower Enterprise is installed in an organization's own cloud or on-premises infrastructure. It includes:</p> <ul> <li>Monitoring, logging, and observability</li> <li>Pipeline execution launchpad</li> <li>Cloud resource provisioning</li> <li>Pipeline actions and event-based execution</li> <li>LDAP &amp; OpenID authentication</li> <li>Enterprise role-based access control (RBAC)</li> <li>Full-featured API</li> <li>Technical support for Nextflow and Tower</li> </ul> <p>To install Tower in your organization, contact Seqera Labs for a demo to discuss your requirements.</p> <p></p>","title":"Tower Enterprise"},{"location":"getting-started/usage/","tags":["deployment"],"text":"<p>You can use Tower through the web interface, the API, the CLI, or in Nextflow directly using the <code>-with-tower</code> option.</p>","title":"Usage"},{"location":"getting-started/usage/#tower-web-interface","tags":["deployment"],"text":"<ol> <li> <p>Create an account and log in to Tower, available free of charge at tower.nf.</p> </li> <li> <p>Create and configure a new compute environment.</p> </li> <li> <p>Start launching pipelines.</p> </li> </ol>","title":"Tower web interface"},{"location":"getting-started/usage/#tower-api","tags":["deployment"],"text":"<p>See API.</p>","title":"Tower API"},{"location":"getting-started/usage/#tower-cli","tags":["deployment"],"text":"<p>See CLI.</p>","title":"Tower CLI"},{"location":"getting-started/usage/#nextflow-with-tower","tags":["deployment"],"text":"<ol> <li> <p>Create an account and log in to Tower.</p> </li> <li> <p>Create a new token. You can access your tokens from the Settings drop-down menu:</p> </li> </ol> <p></p> <ol> <li>Name your token.</li> </ol> <p></p> <ol> <li>Store your token securely.</li> </ol> <p></p>   <p>Note</p> <p>The token will only be displayed once. You must copy and save the token before closing the Personal Access Token window.</p>  <ol> <li>Open a terminal and enter the following commands:</li> </ol> <pre>1\n2</pre><pre><code>export TOWER_ACCESS_TOKEN=eyxxxxxxxxxxxxxxxQ1ZTE=\nexport NXF_VER=22.10.6\n</code></pre> <p>Where <code>eyxxxxxxxxxxxxxxxQ1ZTE=</code> is the token you just created.</p> <p>!!! note \"Nextflow version\"    Bearer token support requires Nextflow version 20.10.0 or later, set with the second command above.</p> <p>To submit a pipeline to a workspace using Nextflow, add the workspace ID to your environment:</p> <pre>1</pre><pre><code>export TOWER_WORKSPACE_ID=000000000000000\n</code></pre> <p>The workspace ID can be found on the organization workspaces overview page.</p> <ol> <li>Run your Nextflow pipeline with the <code>-with-tower</code> flag:</li> </ol> <pre>1</pre><pre><code>nextflow run hello.nf -with-tower\n</code></pre> <p>You can now monitor your workflow runs in Tower.</p> <p>To configure and execute Nextflow pipelines in cloud environments, see Compute Environments.</p>   <p>Tip</p> <p>See the Nextflow documentation for further run configuration options using Nextflow configuration files.</p>","title":"Nextflow <code>-with-tower</code>"},{"location":"getting-started/workspace/","tags":["workspaces"],"text":"<p>Each user has a unique workspace to manage all resources, such as pipelines, compute environments, and credentials.</p>   <p>Tip</p> <p>You can create multiple workspaces within an organization context and associate each of these workspaces with dedicated teams of users, while providing a fine-grained access control model for each of the teams. See Orgs and teams.</p>  <p>The core components of a workspace are:</p>","title":"Workspaces"},{"location":"getting-started/workspace/#launchpad","tags":["workspaces"],"text":"<p>The Launchpad offers a streamlined UI for launching and managing pipelines and their associated compute environments and credentials. Using the Launchpad, you can create a curated set of pipelines (including variations of the same pipeline) that are ready to be executed on the associated compute environments, while allowing the user to customize the pipeline-level parameters if needed.</p>","title":"Launchpad"},{"location":"getting-started/workspace/#runs","tags":["workspaces"],"text":"<p>The Runs section monitors a launched workflow with real-time execution metrics, such as the number of pending or completed processes.</p> <p>See Launch.</p>","title":"Runs"},{"location":"getting-started/workspace/#actions","tags":["workspaces"],"text":"<p>You can trigger pipelines based on specific events, such as a version release on Github or a general Tower webhook.</p> <p>See Pipeline actions.</p>","title":"Actions"},{"location":"getting-started/workspace/#compute-environments","tags":["workspaces"],"text":"<p>Tower uses the concept of a Compute environment to define an execution platform for pipelines. Tower supports launching pipelines into a growing number of cloud (AWS, Azure, GCP) and on-premises (Slurm, IBM LSF, Grid Engine, etc.) infrastructures.</p> <p>See Compute environments.</p>","title":"Compute environments"},{"location":"getting-started/workspace/#credentials","tags":["workspaces"],"text":"<p>The Credentials section allows users to set up the access credentials for various platforms (Github, Gitlab, BitBucket, etc.) and compute environments (cloud, Slurm, Kubernetes, etc.) See Compute environments and Git integration for information on your infrastructure.</p> <p>See Credentials.</p>","title":"Credentials"},{"location":"git/overview/","tags":["git"],"text":"","title":"Git integration"},{"location":"git/overview/#overview","tags":["git"],"text":"<p>Data pipelines can be composed of many assets (pipeline scripts, configuration files, dependency descriptors such as for Conda or Docker, documentation, etc). By managing complex data pipelines as Git repositories, all assets can be versioned and deployed with a specific tag, release or commit id. Version control, combined with containerization, is crucial for enabling reproducible pipeline executions, and it provides the ability to continuously test and validate pipelines as the code evolves over time.</p> <p>Nextflow has built-in support for Git and several Git-hosting platforms. Nextflow pipelines can be pulled remotely from both public and private Git-hosting providers, including the most popular platforms: GitHub, GitLab, and BitBucket.</p>","title":"Overview"},{"location":"git/overview/#public-repositories","tags":["git"],"text":"<p>You can use a publicly hosted Nextflow pipeline by specifying the Git repository URL in the Pipeline to launch field.</p> <p>When specifying the Revision number, the list of available revisions are automatically pulled using the Git provider's API. By default, the default branch (usually <code>main</code> or <code>master</code>) will be used.</p> <p></p>   <p>Tip</p> <p>nf-core is a great resource for public Nextflow pipelines.</p>    <p>API Rate Limits</p> <p>The GitHub API imposes rate limits on API requests. You can increase your rate limit by adding GitHub credentials to your workspace as shown below.</p>","title":"Public repositories"},{"location":"git/overview/#private-repositories","tags":["git"],"text":"<p>In order to access private Nextflow pipelines, you must add credentials for your private Git hosting provider.</p>   <p>Note</p> <p>All credentials are (AES-256) encrypted before secure storage and are not exposed in an unencrypted way by any Tower API.</p>","title":"Private repositories"},{"location":"git/overview/#multiple-credential-filtering","tags":["git"],"text":"<p>When your Tower instance has multiple stored credentials, selection of the most relevant credential for your repository takes precedence in the following order:</p> <ol> <li> <p>Tower evaluates all the stored credentials available to the current Workspace.</p> </li> <li> <p>Credentials are filtered by Git provider (GitHub, GitLab, Bitbucket, etc.)</p> </li> <li> <p>Tower selects the credential with a Repository base URL most similar to the target repository.</p> </li> <li> <p>If no Repository base URL values are specified in the Workspace credentials, the the most long-lived credential is selected.</p> </li> </ol> <p>Example:</p> <p>Workspace A contains 4 credentials:</p> <p>Credential A</p> <pre>1\n2\n3</pre><pre><code>Type: GitHub\n\nRepository base URL:\n</code></pre> <p>Credential B</p> <pre>1\n2\n3</pre><pre><code>Type: GitHub\n\nRepository base URL: https://github.com/\n</code></pre> <p>Credential C</p> <pre>1\n2\n3</pre><pre><code>Type: GitHub\n\nRepository base URL: https://github.com/pipeline-repo\n</code></pre> <p>Credential D</p> <pre>1\n2\n3</pre><pre><code>Type: GitLab\n\nRepository base URL: https://gitlab.com/repo-a\n</code></pre> <p>If you launch a pipeline with a Nextflow workflow residing in https://github.com/pipeline-repo, Tower will use Credential C.</p> <p>To ensure automatic selection of the most appropriate credential for your repository, we recommend that you:</p> <ul> <li> <p>Specify Repository base URL values as precisely as possible for each Git credential used in the Workspace.</p> </li> <li> <p>Favor the use of service account type credentials where possible (such as GitLab group access tokens).</p> </li> <li> <p>Avoid the use of multiple user-based tokens with similar permissions.</p> </li> </ul>","title":"Multiple credential filtering"},{"location":"git/overview/#azure-devops-repositories","tags":["git"],"text":"<p>You can authenticate to Azure Devops repositories using a personal access token (PAT). </p> <p>Once you have created and copied your access token, create a new credential in Tower using these steps:</p> <ol> <li> <p>Navigate to the Credentials tab. If you are using your personal workspace, select Your credentials from the user icon menu (top right).</p> </li> <li> <p>Select Add Credentials.</p> </li> <li> <p>Enter a Name for the new credentials.</p> </li> <li> <p>Select \"Azure DevOps\" as the Provider.</p> </li> <li> <p>Enter your Username and Access token.</p> </li> <li> <p>Enter the Repository base URL for which the credentials should be applied (recommended). This option can be used to apply the provided credentials to a specific repository, e.g. <code>https://dev.azure.com/{your organization}/{your project}</code>.</p> </li> </ol>","title":"Azure DevOps repositories"},{"location":"git/overview/#github","tags":["git"],"text":"<p>To connect a private GitHub repository, personal (classic) or fine-grained access tokens can be used.</p>   <p>Note</p> <p>A personal access token (classic) can access every repository that the user it belongs to can access. GitHub recommends that you use fine-grained personal access tokens (currently in beta) instead, which you can restrict to specific repositories. Fine-grained personal access tokens also enable you to specify granular permissions instead of broad scopes.</p>  <p>For personal (classic) tokens, you must grant access to the private repository by selecting the main <code>repo</code> scope when the token is created. See here for instructions to create your personal access token (classic).</p> <p>For fine-grained tokens, the repository's organization must opt in to the use of fine-grained tokens. Tokens can be restricted by Resource owner (organization), Repository access, and Permissions. See here for instructions to create your fine-grained access token.</p> <p>Once you have created and copied your access token, create a new credential in Tower using these steps:</p> <ol> <li> <p>Navigate to the Credentials tab. If you are using your personal workspace, select Your credentials from the user icon menu (top right).</p> </li> <li> <p>Select Add Credentials.</p> </li> <li> <p>Enter a Name for the new credentials.</p> </li> <li> <p>Select \"GitHub\" as the Provider.</p> </li> <li> <p>Enter your Username and Access token.</p> </li> <li> <p>Enter the Repository base URL for which the credentials should be applied (recommended). This option can be used to apply the provided credentials to a specific repository, e.g. <code>https://github.com/seqeralabs</code>.</p> </li> </ol>","title":"GitHub"},{"location":"git/overview/#gitlab","tags":["git"],"text":"<p>GitLab supports Personal, Group, and Project access tokens for authentication. Your access token should have the <code>api</code>, <code>read_api</code>, and <code>read_repository</code> scopes in order to work with Tower. For all three token types, the token value is used for both the Password and Access token fields in the Tower credential creation form.</p> <p>To connect Tower to a private GitLab repository:</p> <ol> <li> <p>Navigate to the Credentials tab. If you are using your personal workspace, select Your credentials from the user icon menu (top right).</p> </li> <li> <p>Select Add Credentials.</p> </li> <li> <p>Enter a Name for the new credentials.</p> </li> <li> <p>Select \"GitLab\" as the Provider.</p> </li> <li> <p>Enter your Username. For Group and Project access tokens, the username can be any non-empty value.</p> </li> <li> <p>Enter your token value in the Password and Access token fields.</p> </li> <li> <p>Enter the Repository base URL (recommended). This option is used to apply the credentials to a specific repository, e.g. <code>https://gitlab.com/seqeralabs</code>.</p> </li> </ol>","title":"GitLab"},{"location":"git/overview/#gitea","tags":["git"],"text":"<p>Available from Tower 22.4.X</p> <p>To connect to a private Gitea repository, supply your Gitea user credentials to create a new credential in Tower with these steps:</p> <ol> <li> <p>Navigate to the Credentials tab. If you are using your personal workspace, select Your credentials from the user icon menu (top right).</p> </li> <li> <p>Select Add Credentials.</p> </li> <li> <p>Enter a Name for the new credentials.</p> </li> <li> <p>Select \"Gitea\" as the Provider.</p> </li> <li> <p>Enter your Username.</p> </li> <li> <p>Enter your Password.</p> </li> <li> <p>Enter your Repository base URL (required).</p> </li> </ol>","title":"Gitea"},{"location":"git/overview/#bitbucket","tags":["git"],"text":"<p>To connect to a private BitBucket repository, refer to the BitBucket documentation to learn how to create a BitBucket App password. Then, create a new credential in Tower using these steps:</p> <ol> <li> <p>Navigate to the Credentials tab. If you are using your personal workspace, select Your credentials from the user icon menu (top right).</p> </li> <li> <p>Select Add Credentials.</p> </li> <li> <p>Enter a Name for the new credentials.</p> </li> <li> <p>Select \"BitBucket\" as the Provider.</p> </li> <li> <p>Enter your Username and Password.</p> </li> <li> <p>Enter the Repository base URL (recommended). This option can be used to apply the provided credentials to a specific repository, e.g. <code>https://bitbucket.org/seqeralabs</code>.</p> </li> </ol>","title":"Bitbucket"},{"location":"git/overview/#aws-codecommit","tags":["git"],"text":"<p>To connect to a private AWS CodeCommit repository, refer to the AWS documentation to learn more about IAM permissions for CodeCommit. Then, supply the IAM account access key and secret key to create a credential in Tower using these steps:</p> <ol> <li> <p>Navigate to the Credentials tab. If you are using your personal workspace, select Your credentials from the user icon menu (top right).</p> </li> <li> <p>Select Add Credentials.</p> </li> <li> <p>Enter a Name for the new credentials.</p> </li> <li> <p>Select \"CodeCommit\" as the Provider.</p> </li> <li> <p>Enter the Access key and Secret key of the AWS IAM account that will be used to access the desired CodeCommit repository.</p> </li> <li> <p>Enter the Repository base URL for which the credentials should be applied (recommended). This option can be used to apply the provided credentials to a specific region, e.g. <code>https://git-codecommit.eu-west-1.amazonaws.com</code>.</p> </li> </ol>","title":"AWS CodeCommit"},{"location":"git/overview/#self-hosted-git","tags":["git"],"text":"<p>It is also possible to specify Git server endpoints for Tower Enterprise. For more information, refer to the Tower Install Documentation.</p>","title":"Self-hosted Git"},{"location":"installation/system-deployment/","tags":["installation","deployment"],"text":"<p>Tip</p> <p>It is highly recommended to first Sign up and try the hosted version of Tower for free or request a demo for a deployment in your own on-premises or cloud environment.</p>  <p>Nextflow Tower is a web application server based on a microservice oriented architecture and designed to maximize the portability, scalability and security of the application.</p> <p>The application is composed of a variety of modules that can be configured and deployed depending on organization's requirements.</p> <p>All components for the Enterprise release are packaged as Docker container images which are hosted and security validated by the Amazon ECR service. The community version can be accessed via GitHub.</p>","title":"System deployment"},{"location":"installation/system-deployment/#deployment-configurations","tags":["installation","deployment"],"text":"<p>Warning</p> <p>To install Nextflow Tower on private infrastructure, you'd need a license key. Please contact us at sales@seqera.io to get your license key.</p>","title":"Deployment configurations"},{"location":"installation/system-deployment/#basic-deployment","tags":["installation","deployment"],"text":"<p>The minimal Tower configuration only requires the front-end, backend and database modules.</p> <p>These can be executed as Docker containers or as native services running in the hosting environment. Such a minimal configuration is only suggested for evaluation purposes or for a small number of users.</p>","title":"Basic deployment"},{"location":"installation/system-deployment/#kubernetes-deployment","tags":["installation","deployment"],"text":"<p>Kubernetes cluster management is emerging as the technology of choice for the deployment of applications requiring high-availability, scalability and security.</p> <p>Nextflow Tower Enterprise includes configuration manifests for the deployment in the Kubernetes environment.</p> <p>This diagram shows the system architecture for the reference deployment on AWS.</p> <p></p>","title":"Kubernetes deployment"},{"location":"installation/system-deployment/#tower-modules","tags":["installation","deployment"],"text":"<p>The application is composed of a number of modules that can be configured and deployed depending on user requirements.</p> <p>All components are packaged as Docker container images which are hosted and security validated by the Amazon ECR service.</p>","title":"Tower Modules"},{"location":"installation/system-deployment/#backend-module","tags":["installation","deployment"],"text":"<p>The backend is implemented as a JVM-based application server based on the Micronaut framework which provides a modern and secure backbone for the application server.</p> <p>The backend module requires OpenJDK 8 or later.</p> <p>The backend layer implements the main application logic organised in a service layer, which is then exposed via a REST API and defined via an OpenAPI schema.</p> <p>The backend module uses JPA/Hibernate/JDBC API industry standards to connect the underlying relational database.</p> <p>The backend is designed to run standalone or as multiple replicas for scalability when deployed in high-availability mode.</p>","title":"Backend module"},{"location":"installation/system-deployment/#frontend-module","tags":["installation","deployment"],"text":"<p>The frontend module is composed by Angular 8 application which is served by an Nginx web server.</p> <p>The frontend can be configured to expose the application directly to the user/DMZ via an HTTPS connection or through a load balancer.</p>","title":"Frontend module"},{"location":"installation/system-deployment/#storage","tags":["installation","deployment"],"text":"<p>Nextflow Tower requires a relational database as its primary storage.</p> <p>It is suggested to use MySQL 5.6, however, any SQL database compatible with JPA/JDBC industry-standards is supported.</p>","title":"Storage"},{"location":"installation/system-deployment/#caching","tags":["installation","deployment"],"text":"<p>Tower provides an optional caching module for configurations requiring high availability.</p> <p>This module requires a Redis 5.0 in-memory database.</p>","title":"Caching"},{"location":"installation/system-deployment/#authentication-module","tags":["installation","deployment"],"text":"<p>Nextflow Tower supports enterprise authentication mechanisms such as OAuth and LDAP.</p> <p>Third-party authority providers and custom single-sign-on flow can be developed depending on exact customer requirements.</p>","title":"Authentication module"},{"location":"installation/system-deployment/#cron-scheduler","tags":["installation","deployment"],"text":"<p>Tower implements a cron service which takes care of executing periodical activities, such as sending e-mail notifications and cleaning up.</p> <p>The cron service can be configured to run as an embedded backend service or an independent service.</p>","title":"Cron scheduler"},{"location":"labels/overview/","tags":["labels"],"text":"","title":"Tower labels"},{"location":"labels/overview/#overview","tags":["labels"],"text":"<p>Use labels to organize your work and filter key information. Labels are free-text annotations that can be applied to pipelines, actions, or workflow runs either during creation or afterward.</p> <p>Labels are workspace-specific (each workspace has an independent set of labels) and are not propagated to Nextflow during workflow execution.</p>","title":"Overview"},{"location":"labels/overview/#create-and-apply-labels","tags":["labels"],"text":"<p>Labels can be created, applied, and edited by a workspace maintainer, admin, or owner. When applying a label, users can select from existing labels or add new ones on the fly.</p> <p></p>","title":"Create and apply labels"},{"location":"labels/overview/#labels-applied-to-a-pipeline","tags":["labels"],"text":"<p>Warning</p> <p>Labels are applied to elements in a workspace-specific context. This means that labels applied to a shared pipeline in workspace A will not be shown when viewing the pipeline from workspace B.</p>  <p>The labels applied to each pipeline are displayed in both list and card views on the Launchpad screen. Select a pipeline to view all applied labels.</p> <p></p> <p>Apply a label when adding a new pipeline or editing an existing pipeline.</p> <p>If a label was applied to a pipeline, all workflow runs of this pipeline will inherit the label. If the labels applied to the pipeline are changed, this change will not be reflected on previously executed workflow runs, only future runs.</p> <p></p>","title":"Labels applied to a pipeline"},{"location":"labels/overview/#labels-applied-to-an-action","tags":["labels"],"text":"<p>Labels applied to an action are displayed in the action card on the Actions screen. To see all labels, hover over a label with the \u201c+\u201d character.</p> <p>Apply a label when adding a new action or editing an existing action.</p> <p>If a label is applied to an action, all workflow runs triggered by this action will inherit the label. If the labels applied to the action are changed, this change will not be reflected on previously executed workflow runs, only future runs.</p>","title":"Labels applied to an action"},{"location":"labels/overview/#labels-applied-to-a-workflow-run","tags":["labels"],"text":"<p>Labels applied to a workflow run are displayed on the runs list screen and on the workflow run detail screen. To see all labels, hover over a label with the \u201c+\u201d character. Apply a label to a workflow run when launching a workflow run, on the workflow runs list screen, or on the run detail screen.</p> <p></p>","title":"Labels applied to a workflow run"},{"location":"labels/overview/#search-and-filter-with-labels","tags":["labels"],"text":"<p>Search and filter pipelines and workflow runs using one or more labels. Filter and search are complementary.</p> <p></p>","title":"Search and filter with labels"},{"location":"labels/overview/#overview-of-labels-in-a-workspace","tags":["labels"],"text":"<p>All labels used in a workspace can be viewed, added, edited, and deleted by a maintainer, admin, or workspace owner on the workspace settings screen. If a label is edited or deleted on this screen, the change is propagated to all items where the label was used. Such a change is irreversible.</p> <p></p>","title":"Overview of labels in a workspace"},{"location":"labels/overview/#limits","tags":["labels"],"text":"<p>Warning</p>  <p>Label names must contain a minimum of 2 and a maximum of 39 alphanumeric characters, separated by dashes or underscores, and must be unique in each workspace.</p> <ul> <li>Label names cannot begin or end with dashes <code>-</code> or underscores <code>_</code>.</li> <li>Label names cannot contain a consecutive combination of <code>-</code> or <code>_</code> characters (<code>--</code>, <code>__</code>, <code>-_</code>, etc.)</li> <li>A maximum of 25 labels can be applied to each resource.</li> <li>A maximum of 100 labels can be used in each workspace.</li> </ul>","title":"Limits"},{"location":"launch/advanced/","tags":["advanced","launch"],"text":"<p>Advanced launch options allow users to modify the configuration and execution of the pipeline.</p>","title":"Advanced options"},{"location":"launch/advanced/#nextflow-config-file","tags":["advanced","launch"],"text":"<p>The Nextflow config field allows the addition of settings to the Nextflow configuration file.</p> <p>This text should follow the same syntax as the Nextflow configuration file.</p> <p>In the example below, we can modify the manifest section to give the pipeline a name and description which will show up in the Tower monitoring section.</p> <p></p>","title":"Nextflow config file"},{"location":"launch/advanced/#pre-post-run-scripts","tags":["advanced","launch"],"text":"<p>It is possible to run custom code either before or after the execution of the Nextflow script. These fields allow users to enter shell commands.</p>","title":"Pre &amp; post-run scripts"},{"location":"launch/advanced/#pull-latest","tags":["advanced","launch"],"text":"<p>Enabling this option ensures Nextflow pulls the latest version from the Git repository. This is equivalent to using the <code>-latest</code> flag.</p> <p></p>","title":"Pull latest"},{"location":"launch/advanced/#main-script","tags":["advanced","launch"],"text":"<p>Nextflow will attempt to run the script named <code>main.nf</code> in the project repository by default. This can be changed via either the <code>manifest.mainScript</code> option or by providing the script filename to run in this field.</p>","title":"Main script"},{"location":"launch/advanced/#workflow-entry-name","tags":["advanced","launch"],"text":"<p>Nextflow DSL2 provides the ability to launch specific-named workflows. Enter the name of the workflow to be executed in this field.</p>","title":"Workflow entry name"},{"location":"launch/launch/","tags":["launch"],"text":"","title":"Launch form"},{"location":"launch/launch/#pipeline-launch-form","tags":["launch"],"text":"<p>The Launch Form can be used for launching pipelines and for adding pipelines to the Launchpad.</p> <p>To launch a pipeline:</p> <ol> <li> <p>Select Start Quick Launch in the navigation bar. The Launch Form will appear.</p> </li> <li> <p>Select a Compute Environment from the available options.</p> </li> </ol> <p>Visit the Compute Environment documentation to learn how to create an environment for your preferred execution platform.</p> <ol> <li>Enter a repository URL for the Pipeline to launch (e.g. <code>https://github.com/nf-core/rnaseq.git</code>).</li> </ol>   <p>Tip</p> <p>Nextflow pipelines are just Git repositories and they can reside on any public or private Git-hosting platform. See Git Integration in the Tower docs and Pipeline Sharing in the Nextflow docs for more details.</p>  <ol> <li>You can select a Revision number to use a specific version of the pipeline.</li> </ol> <p>The Git default branch (e.g. <code>main</code> or <code>master</code>) or <code>manifest.defaultBranch</code> in the Nextflow configuration will be used by default.</p> <ol> <li>Enter the Work directory, which corresponds to the Nextflow work directory.</li> </ol> <p>The default work directory of the compute environment will be used by default.</p>   <p>Warning</p> <p>The credentials associated with the compute environment must be able to access the work directory (e.g. an S3 bucket).</p>  <ol> <li> <p>Select any Config profiles you would like to use.</p> <p>Visit the Nextflow Config profiles documentation for more details.</p> </li> <li> <p>Enter any Pipeline parameters in YAML or JSON format.</p> <pre>1\n2\n3\n4\n5</pre><pre><code>YAML example:\n```yaml\nreads: 's3://nf-bucket/exome-data/ERR013140_{1,2}.fastq.bz2'\npaired_end: true\n```\n</code></pre> </li> </ol>   <p>Tip</p> <p>In YAML, quotes should be used for paths but not for numbers or Booleans.</p>  <ol> <li>Select Launch to launch the pipeline.</li> </ol>","title":"Pipeline launch form"},{"location":"launch/launchpad/","tags":["launchpad"],"text":"","title":"Launchpad"},{"location":"launch/launchpad/#overview","tags":["launchpad"],"text":"<p>Launchpad makes it easy for any workspace user to launch a pre-configured pipeline. Use the Sort by: drop-down to sort pipelines, either by name or most-recently updated. </p> <p>The list layout is the default Launchpad view. Use the toggle next to the Add pipeline button to switch between the list and tile views. Both views display the compute environment of each pipeline for easy reference.</p> <p></p> <p>A pipeline is a repository containing a Nextflow workflow, a compute environment, and pipeline parameters.</p>","title":"Overview"},{"location":"launch/launchpad/#pipeline-parameters-form","tags":["launchpad"],"text":"<p>Launchpad automatically detects the presence of a <code>nextflow_schema.json</code> in the root of the repository and dynamically creates a form where users can easily update the parameters.</p>   <p>Tip</p> <p>The parameter forms view will appear if the workflow has a Nextflow schema file for the parameters. See Nextflow Schema to learn more about the use cases and how to create them.</p>  <p>This makes it trivial for users without any expertise in Nextflow to enter their pipeline parameters and launch.</p> <p></p>","title":"Pipeline Parameters Form"},{"location":"launch/launchpad/#adding-a-new-pipeline","tags":["launchpad"],"text":"<p>Adding a pipeline to the workspace launchpad is similar to launching a pipeline. Instead of launching the pipeline, it gets added to the list of pipelines with pre-saved values, such as the pipeline parameters and revision number.</p>   <p>Tip</p> <p>To create your own customized Nextflow Schema for your pipeline, see the <code>nf-core</code> workflows that have adopted this.  nf-core/eager and nf-core/rnaseq are excellent examples.</p>","title":"Adding a New Pipeline"},{"location":"launch/notifications/","tags":["launch","notifications"],"text":"","title":"Notifications"},{"location":"launch/notifications/#email-notifications","tags":["launch","notifications"],"text":"<p>You can receive email notifications at the completion or a failure of a workflow execution.</p> <p>Navigate to your profile page using dropdown on your avatar in the top-right of the page. Select the Send notification email on workflow completion toggle option at the bottom of the profile settings page.</p> <p></p>","title":"Email Notifications"},{"location":"launch/relaunch/","tags":["launch","resume"],"text":"<p>Re-launching pipelines is a great way to quickly troubleshoot or make use of Nextflow's resume functionality and re-launch the same pipeline with different parameters.</p> <p>The Resume option is selected by default when re-launching a new pipeline from the Runs monitoring screen. In short, This option allows for the continuation of a workflow execution using Nextflow resume.</p>   <p>Nextflow resume</p> <p>For a detailed explanation of how the resume option works, please visit Part 1 and Part 2 of the Demystifying Nextflow resume description in the Nextflow blog.</p>","title":"Re-launching pipelines"},{"location":"launch/relaunch/#change-compute-environment-when-resuming-a-run","tags":["launch","resume"],"text":"<p>Available from Tower 22.4.0</p> <p>Users with appropriate permissions can change the compute environment when resuming a run. The new compute environment must have access to the original run work directory. This means that the new compute environment must have a work directory that matches the root path of the original pipeline work directory, e.g. if the original pipeline work directory is <code>s3://foo/work/12345</code>, the new compute environment must have access to <code>s3://foo/work</code>.</p>","title":"Change compute environment when resuming a run"},{"location":"limits/limits/","tags":["limits"],"text":"<p>Tower Cloud elements and features have default limits per workspace and organization.</p>","title":"Usage limits"},{"location":"limits/limits/#workspaces","tags":["limits"],"text":"Description Value     Active runs 5   Members 50   Participants 50   Pipelines 100   Datasets 100   Labels 1000","title":"Workspaces"},{"location":"limits/limits/#organizations","tags":["limits"],"text":"Description Value     Workspaces 50   Teams 20","title":"Organizations"},{"location":"limits/limits/#datasets","tags":["limits"],"text":"Description Value     File size 10 MB   Versions per dataset 100    <p>If you need higher limits and capabilities, contact us to discuss your application requirements.</p>","title":"Datasets"},{"location":"monitoring/aggregate_stats/","tags":["stats","resources","usage","monitoring"],"text":"<p>The Aggregate stats panel displays a real-time summary of the resources used by the workflow. These include total running time ('wall time'), aggregated CPU time (CPU hours), memory usage (GB hours), data i/o and cost.</p>","title":"Aggregate stats & load"},{"location":"monitoring/aggregate_stats/#estimated-cost","tags":["stats","resources","usage","monitoring"],"text":"<p>The estimated cost estimates the total compute cost of all tasks in the workflow run. The compute cost of a task is calculated as follows:</p> <p>( \\text{Task cost} = \\text{VM hourly rate} \\times \\text{VM fraction} \\times \\text{Task runtime} )</p> <p>( \\quad \\text{VM fraction} = \\text{max} ( \\frac{\\text{Task CPUs}}{\\text{VM CPUs}}, \\frac{\\text{Task memory}}{\\text{VM memory}} ) )</p> <p>( \\quad \\text{Task runtime} = ( \\text{Task complete} - \\text{Task start} ) )</p> <p>Tower uses a database of prices for AWS and Google Cloud, accross all instance types, regions, and zones, to fetch the VM price for each task. This database is updated periodically to reflect the most recent prices.</p>  <p>Note</p> <p>Prior to Tower Enterprise 22.4.x, the cost estimate used <code>realtime</code> instead of <code>complete</code> and <code>start</code> to measure the task runtime. The <code>realtime</code> metric tends to underestimate the billable runtime because it doesn't include the time required to stage input and output files.</p>  <p>The estimated cost is subject to several limitations:</p> <ul> <li> <p>It does not account for the cost of storage, network, the head job, or how tasks are mapped to VMs. As a result, it tends to underestimate the true cost of a workflow run.</p> </li> <li> <p>On a resumed workflow run, the cost of cached tasks is included in the estimated cost. As a result, the total cost of multiple attempts of a workflow run tends to overestimate the actual cost, because the cost of cached tasks may be counted multiple times.</p> </li> </ul> <p>For accurate cost accounting, you should use the cost reporting tools for your cloud provider (such as AWS Cost Explorer). You can use Resource labels in your compute environments to annotate and track the actual cloud resources consumed by a workflow run.</p> <p></p>","title":"Estimated cost"},{"location":"monitoring/aggregate_stats/#load-and-utilization","tags":["stats","resources","usage","monitoring"],"text":"<p>As processes are being submitted to the compute environment, the Load monitors how many cores and tasks are currently being used. For the cores gauge chart, the denominator is the maximum number of cores that have already been used at that moment during the execution of that specific pipeline.</p> <p>Utilization is calculated for memory and CPUs. This is the average value across all tasks and is calculated by dividing the memory (or CPUs) usage by the memory (or CPUs) requested.</p> <p></p>","title":"Load and Utilization"},{"location":"monitoring/execution/","tags":["logging","monitoring"],"text":"","title":"Execution details & logs"},{"location":"monitoring/execution/#run-execution-details","tags":["logging","monitoring"],"text":"<p>Selecting a workflow run from the Runs tab will display the workflow details. This view contains:</p> <ul> <li>Run information with command-line, parameters, configuration, and execution logs in real-time.</li> <li>Summary and status section.</li> <li>List of pipeline processes.</li> <li>Aggregated stats and load.</li> <li>Detailed list of individual tasks and metrics.</li> </ul>","title":"Run execution details"},{"location":"monitoring/execution/#run-information","tags":["logging","monitoring"],"text":"<p>This section is composed of several tabs containing details about the Nextflow execution:</p> <ul> <li> <p>The Nextflow Command line that was executed.</p> </li> <li> <p>The Parameters that were provided to the pipeline (taken from the configuration <code>params</code> scope).</p> </li> <li> <p>The Configuration files as well as the final resolved configuration.</p> </li> <li> <p>The Execution log from the main Nextflow process, which is updated in real time.</p> </li> </ul> <p></p>","title":"Run information"},{"location":"monitoring/execution/#save-run-as-pipeline","tags":["logging","monitoring"],"text":"<p>Available from version 23.1</p> <p>From the Runs list, any run can be saved as a new pipeline for future use (regardless of run status). Select the item menu to the right of any run in the list, then select Save as pipeline. In the dialog box shown, you can edit the pipeline name (the run name is pre-filled by default), add labels, and Save. To review and edit any run details prior to saving the pipeline, select review and edit near the top of the dialog window. Once saved, the new pipeline is listed on the Launchpad and can be run from the same workspace it was created in.</p>","title":"Save run as pipeline"},{"location":"monitoring/overview/","tags":["runs","monitoring"],"text":"<p>Jobs that have been submitted with Tower can be monitored wherever you have an internet connection.</p> <p>The Runs tab contains all previous job executions. Each new or resumed job will be given a random name, e.g., <code>grave_williams</code>.</p> <p></p> <p>The colors signify the completion status:</p> <ul> <li>Blue are running.</li> <li>Green are successfully executed.</li> <li>Red are successfully executed where at least one task failed with a \"terminate\" error strategy.</li> <li>Grey are jobs that were forced to stop during execution.</li> </ul> <p>Selecting any particular run from the panel will display that run's execution details.</p>","title":"Overview"},{"location":"monitoring/overview/#save-run-as-pipeline","tags":["runs","monitoring"],"text":"<p>Available from version 23.1</p> <p>From the Runs list, any run can be saved as a new pipeline for future use (regardless of run status). Select the item menu to the right of any run in the list, then select Save as pipeline. In the dialog box shown, you can edit the pipeline name (the run name is pre-filled by default), add labels, and Save. To review and edit any run details prior to saving the pipeline, select review and edit near the top of the dialog window. Once saved, the new pipeline is listed on the Launchpad and can be run from the same workspace it was created in. </p>","title":"Save run as pipeline"},{"location":"monitoring/overview/#all-runs-view","tags":["runs","monitoring"],"text":"<p>Available from version 22.4.0</p> <p>The All runs page, accessed from the user top-right menu, provides a comprehensive overview of the runs accessible to a user across the entire Tower instance. This facilitates overall status monitoring and early detection of execution issues from a single view split across organizations and workspaces.</p> <p>The All runs view defaults to all organizations and workspaces the user has access to. Select the drop-down next to View: to filter by specific organizations and workspaces, or to view runs from your personal workspace only. </p> <p>The Search workflow bar allows you to filter by by run name, project name, manifest name, or session ID.</p> <p>Filter by one or more \"keyword:value\" entries:</p> <ul> <li>status:</li> <li>label:</li> <li>workflowId:</li> <li>runName:</li> <li>username:</li> <li>projectName:</li> <li>after: YYYY-MM-DD</li> <li>before: YYYY-MM-DD</li> <li>sessionId:</li> </ul> <p>The search feature will populate with available suggestions when entering valid keywords. Suggested results for <code>label:</code> includes available labels from all workspaces and labels occurring in multiple workspaces will only be displayed for suggestion once. </p>","title":"All runs view"},{"location":"monitoring/overview/#search","tags":["runs","monitoring"],"text":"<p>Our integrated search covers all workflow runs inside a workspace, enabling easy retrieval of complex queries. To search and filter the runs in a workspace, the user can write a search query in the \"Search workflow\" textbox.</p> <p>The search text is interpreted by identifying all substrings formatted by <code>keyword:value</code> (this only applies to valid keywords shown below), combining all the rest in a single <code>Freetext</code> string, and then using all these search criteria to filter the runs.</p> <p>An example of a complex search query is the following:</p> <p><code>rnaseq username:john_doe status:succeeded after:2022-02-20</code>.</p> <p>This string will retrieve all runs from the workspace that:</p> <ul> <li>Ended successfully (<code>status:succeeded</code>)</li> <li>AND have been launched by user john_doe (<code>username:john_doe</code>)</li> <li>AND include \"rnaseq\" in the data fields covered by the free text search (e.g. the run name includes rnaseq)</li> <li>AND were submitted after February 20, 2022.</li> </ul> <p>The freetext search uses a partial match to find runs, meaning that it will search for \"<code>*freetext*</code>\" when looking for runs. The <code>keyword:value</code> item uses exact match to filter runs, so <code>username:john</code> will not retrieve runs launched by <code>john_doe</code></p>   <p>Warning</p> <p>The implemented logic combines all filtering elements with AND logic. This means that queries like <code>status:succeeded, status:submitted</code> are formally valid but will return and empty list because a workflow can only have one status.</p>    <p>Warning</p> <p>The freetext resulting after identifying all the <code>keyword:value</code> are merged into a unique string including spaces, which may result in an empty list of results if there are typos.</p>    <p>Note</p> <p>Keywords corresponding to dates (e.g. <code>after</code> or <code>before</code>) automatically convert the input date to valid ISO-8601, taking into account the user's timezone. Partial dates are also supported e.g. <code>before:2022-5</code> will automatically be converted to <code>before:2022-05-01T00:00:00.000Z</code> under the hood.</p>  <p>Tower will automatically auto-suggest matching keywords while you type into the search bar. Additionally it will suggest valid values for some keywords, when supported. </p>","title":"Search"},{"location":"monitoring/overview/#search-keywords","tags":["runs","monitoring"],"text":"","title":"Search keywords"},{"location":"monitoring/overview/#free-text","tags":["runs","monitoring"],"text":"<ul> <li>The search box allows searching for workflows by partial match with <code>project name</code>, <code>run name</code>, <code>session id</code> or <code>manifest name</code>. Moreover, wildcards can be used to filter the desired workflows such as using asterisks <code>*</code> before and after keyword to filter results.</li> </ul>","title":"Free text"},{"location":"monitoring/overview/#exact-match-keywords","tags":["runs","monitoring"],"text":"<ul> <li> <p><code>worlflowId:&lt;id&gt;</code>: search a workflow by its <code>id</code>.</p> <p>E.g: <code>workflowId:3b7ToXeH9GvESr</code></p> </li> <li> <p><code>runName:&lt;name&gt;</code>: search with a specific <code>run name</code>.</p> <p>E.g: <code>runName:happy_einstein</code></p> </li> <li> <p><code>sessionId:&lt;id&gt;</code>: search workflows with a specific <code>session id</code>.</p> <p>E.g: <code>sessionId:85d35eae-21ea-4294-bc92-e35a60efa1a4</code></p> </li> <li> <p><code>projectName:&lt;name&gt;</code>: search workflows with a specific <code>project name</code>.</p> <p>E.g: <code>projectName:nextflow-io/hello</code></p> </li> <li> <p><code>userName:&lt;name&gt;</code>: search workflows by a specific <code>user name</code>.</p> <p>E.g: <code>userName:john_doe</code></p> </li> <li> <p><code>status:&lt;value&gt;</code>: search workflows with a specific <code>status</code> (<code>submitted</code>, <code>running</code>, <code>succeeded</code>, <code>failed</code>, <code>cancelled</code>, <code>unknown</code>).</p> <p>E.g: <code>status:succeeded</code></p> </li> <li> <p><code>before:&lt;date&gt;</code>: search workflows submitted before the given date (<code>YYYY-MM-DD</code> format), this includes the specified date.</p> <p>E.g: <code>before:2022-04-07</code></p> </li> <li> <p><code>after:&lt;date&gt;</code>: search workflows submitted after the given date (<code>YYYY-MM-DD</code> format), this includes the specified date.</p> <p>E.g: <code>after:2022-04-06</code></p> </li> <li> <p><code>label:&lt;name&gt;</code>: search workflows with a specific label (combine multiple label keywords in order to search workflows associated with all of those labels).</p> <p>E.g: <code>label:label1 label:label2</code></p> </li> <li> <p><code>is:starred</code>: search workflows that have been starred by the user.   E.g: <code>is:starred</code></p> </li> </ul> <p></p>","title":"Exact match keywords"},{"location":"monitoring/processes/","tags":["processes","monitoring"],"text":"<p>In Nextflow, a process is the basic primitive to execute a block of code. The Processes section shows all processes and the status of the tasks.</p> <p>In the example below, there are four tasks of the fastqc process.</p> <p></p> <p>By selecting a process, the Tasks table is filtered below.</p>","title":"Processes"},{"location":"monitoring/summary/","tags":["run","status","monitoring"],"text":"","title":"Summary & status"},{"location":"monitoring/summary/#general","tags":["run","status","monitoring"],"text":"<p>The General summary displays information on the environment and the job being executed:</p> <ul> <li>Unique workflow run ID</li> <li>Workflow run name</li> <li>Date and time of job submission timestamp</li> <li>Project revision and Git commit ID</li> <li>Nextflow session ID</li> <li>Username of the launcher</li> <li>Work directory path</li> <li>Container image</li> <li>Executor</li> <li>Compute environment details</li> <li>Nextflow version</li> </ul>   <p>Tip</p> <p>Hover over with the mouse to get full details on the compute environment.</p>  <p></p>","title":"General"},{"location":"monitoring/summary/#task-status","tags":["run","status","monitoring"],"text":"<p>The Task status section shows in real time the statuses of your workflow tasks. The panel uses the same colour code as the pipelines in the navigation bar.</p> <p>The exact meaning of each status is dependant on the execution platform.</p> <p></p>","title":"Task status"},{"location":"monitoring/tasks/","tags":["tasks","metrics","monitoring"],"text":"","title":"Tasks & metrics"},{"location":"monitoring/tasks/#task-table","tags":["tasks","metrics","monitoring"],"text":"<p>The Tasks section shows all the tasks from an execution.</p> <p>You can use the <code>Search</code> bar to filter tasks by process name, tag, hash, status, etc.</p> <p>Selecting a status in status section filters the task table. E.g. clicking in the CACHED card in the status column.</p> <p></p> <p>Selecting a <code>process</code> in the Processes section above will filter all tasks for that specific process.</p> <p></p> <p>Selecting a task in the task table provides specific information about the task in the Task details dialog.</p> <p></p> <p>The task details dialog has the task information tab and the task Execution log tab.</p>","title":"Task table"},{"location":"monitoring/tasks/#task-information","tags":["tasks","metrics","monitoring"],"text":"<p>The task information tab contains the process name and task tag in the title. The tab includes:</p> <ul> <li>Command</li> <li>Status</li> <li>Work directory</li> <li>Environment</li> <li>Execution time</li> <li>Resources requested</li> <li>Resources used</li> </ul> <p></p>","title":"Task information"},{"location":"monitoring/tasks/#execution-log","tags":["tasks","metrics","monitoring"],"text":"<p>The Execution log provides a realtime log of the individual task of a Nextflow execution.</p> <p>This can be very helpful for troubleshooting. It is possible to download the log files including <code>stdout</code> and <code>stderr</code> from your compute environment.</p> <p></p>","title":"Execution log"},{"location":"monitoring/tasks/#resource-metrics","tags":["tasks","metrics","monitoring"],"text":"<p>This section displays plots with CPU, memory, task duration and I/O usage, grouped by process.</p> <p>These metrics can be used to profile an execution to ensure that the correct amount or resources are being requested for each process.</p> <p></p>   <p>Tip</p> <p>Hover the mouse over the box plots to display more details.</p>","title":"Resource metrics"},{"location":"orgs-and-teams/organizations/","tags":["organizations","administration"],"text":"","title":"Organizations"},{"location":"orgs-and-teams/organizations/#overview","tags":["organizations","administration"],"text":"<p>Organizations are the top-level structure and contain Workspaces, Members, Teams, and Collaborators.</p>","title":"Overview"},{"location":"orgs-and-teams/organizations/#create-an-organization","tags":["organizations","administration"],"text":"<p>To create a new organization:</p> <ol> <li> <p>Navigate to Your organizations and select Add Organization.</p> </li> <li> <p>Enter a Name and Full name for your organization.</p>  <p>Warning</p> <p>The organization name must follow a specific pattern. Refer to the UI for guidance.</p>  </li> <li> <p>Enter any other optional fields as needed: Description, Location, Website URL and Logo.</p> </li> <li> <p>Select Add.</p> </li> </ol> <p>You can view the list of all Members, Teams, and Collaborators in an organization on the organization's page. You can also edit any of the optional fields by selecting Edit from the organizations page or by selecting the Settings tab from the organization's page, provided that you are an Owner of the organization.</p>","title":"Create an organization"},{"location":"orgs-and-teams/organizations/#members","tags":["organizations","administration"],"text":"<p>Once an organization is created, the user who created the organization is the default owner of that organization. It is also possible to invite or add other members as well.</p> <p>Tower provides access control for members of an organization by classifying them either as an Owner or a Member. Each organization can have multiple owners and members.</p>  <p>Note</p> <p>Owners have full read/write access to modify members, teams, collaborators, and settings within a organization. Members are limited in their actions.</p>","title":"Members"},{"location":"orgs-and-teams/organizations/#create-a-new-member","tags":["organizations","administration"],"text":"<p>To add a new member to an organization:</p> <ol> <li>Go to the Members tab of the organization menu</li> <li>Click on Invite member</li> <li>Enter the email ID of user you'd like to add to the organization</li> </ol> <p>An e-mail invitiation will be sent which needs to be accepted by the user. Once they accept the invitation, they can switch to the organization (or organization workspace) using their workspace dropdown.</p>","title":"Create a new member"},{"location":"orgs-and-teams/organizations/#collaborators","tags":["organizations","administration"],"text":"<p>Collaborators are users who are invited to an organization's workspace, but are not members of that organization. As a result, their access is limited to only within that workspace.</p> <p>New collaborators to an organization's workspace can be added using Participants. To learn more about the various available access levels for Participants, please refer to the participant roles section.</p>  <p>Note</p> <p>Collaborators can only be added from a workspace. For more information, see workspace management.</p>","title":"Collaborators"},{"location":"orgs-and-teams/organizations/#teams","tags":["organizations","administration"],"text":"<p>Teams allow the organization owners to group members and collaborators together into a single unit and to manage them as a whole.</p>","title":"Teams"},{"location":"orgs-and-teams/organizations/#create-a-new-team","tags":["organizations","administration"],"text":"<p>To create a new team within an organization:</p> <ol> <li>Go to the Teams tab of the organization menu</li> <li>Click on New team</li> <li>Enter the Name of team</li> <li>Optionally, add the Description and the team's Avatar</li> <li>For the newly created team, click on View</li> <li>Click on Add team member and type in the name of the organization members or collaborators</li> </ol>","title":"Create a new team"},{"location":"orgs-and-teams/overview/","tags":["organizations","teams","users","administration"],"text":"","title":"Overview"},{"location":"orgs-and-teams/overview/#overview","tags":["organizations","teams","users","administration"],"text":"<p>Nextflow Tower simplifies the development and execution of workflows by providing a centralized interface for managing users and resources, while providing ready-to-launch workflows for users. This is achieved through the context of workspaces.</p>","title":"Overview"},{"location":"orgs-and-teams/overview/#organization-resources","tags":["organizations","teams","users","administration"],"text":"<p>Tower allows the creation of multiple organizations, each of which can contain multiple workspaces with shared users and resources. This allows any organization to customize and organize the usage of resources while maintaining an access control layer for users associated with a workspace.</p> <ul> <li> <p>For further information on organizations, see Organizations.</p> </li> <li> <p>For further information on organization workspaces, see Workspace management.</p> </li> </ul>","title":"Organization resources"},{"location":"orgs-and-teams/overview/#organization-users","tags":["organizations","teams","users","administration"],"text":"<p>Any user can be added or removed from an organization or workspace and can be allocated a specific access role within that workspace.</p> <p>Teams provide a way for organizations to group users and participants together into teams, such as <code>workflow-developers</code> or <code>analysts</code>, and apply access control for all users within this team.</p> <p>For further information on user and team creation, see User management.</p>","title":"Organization users"},{"location":"orgs-and-teams/shared-workspaces/","tags":["workspaces","teams","users","administration"],"text":"","title":"Shared workspaces"},{"location":"orgs-and-teams/shared-workspaces/#overview","tags":["workspaces","teams","users","administration"],"text":"<p>Nextflow Tower introduces the concept of shared workspaces as a solution for synchronization and resource sharing within an organization.</p> <p>A shared workspace enables the creation of pipelines in a centralized location, making them accessible to all members of an organization.</p> <p>The benefits of using a shared workspace within an organization include:</p> <ul> <li> <p>Define once and share everywhere: Set up shared resources once and automatically share them across the organization.</p> </li> <li> <p>Centralize the management of key resources: Organization administrators can ensure the correct pipeline configuration is used in all areas of an organization without needing to replicate pipelines across multiple workspaces.</p> </li> <li> <p>Immediate update adoption: Updated parameters for a shared pipeline become immediately available across the entire organization, reducing the risk of pipeline discrepancies.</p> </li> <li> <p>Computational resource provision: Pipelines in shared workflows can be shared along with the required computational resources. This eliminates the need to duplicate resource setup in individual workspaces across the organization. Shared workspaces in Tower centralize and simplify resource sharing within an organization.</p> </li> </ul>","title":"Overview"},{"location":"orgs-and-teams/shared-workspaces/#create-a-shared-workspace","tags":["workspaces","teams","users","administration"],"text":"<p>Creating a shared workspace is similar to the creation of a private workspace, with the exception of the Visibility option, which must be set to Shared.</p> <p></p>","title":"Create a shared workspace"},{"location":"orgs-and-teams/shared-workspaces/#create-a-shared-pipeline","tags":["workspaces","teams","users","administration"],"text":"<p>When creating a pipeline within a shared workspace, associating it with a compute environment is optional.</p> <p>If a compute environment from the shared workspace is associated with the pipeline, it will be available to users in other workspaces who can launch the shared pipeline using the provided environment by default.</p>","title":"Create a shared pipeline"},{"location":"orgs-and-teams/shared-workspaces/#use-shared-pipelines-from-a-private-workspace","tags":["workspaces","teams","users","administration"],"text":"<p>Once a pipeline is set up in a shared workspace and associated with a compute environment within that shared workspace, any user can launch the pipeline from a private workspace using the shared workspace's compute environment. This eliminates the need for users to replicate shared compute environments in their private workspaces.</p>   <p>Note</p> <p>The shared compute environment will not be available to launch other pipelines limited to that specific private workspace.</p>  <p>If a pipeline from a shared workspace is shared without an associated compute environment, users from other workspaces can run it from their local workspaces. By default, the primary compute environment of the local workspace will be selected.</p>","title":"Use shared pipelines from a private workspace"},{"location":"orgs-and-teams/shared-workspaces/#make-shared-pipelines-visible-in-a-private-workspace","tags":["workspaces","teams","users","administration"],"text":"<p>To view pipelines from shared workspaces, set the Filter -&gt; Pipelines from option to This and shared workspaces on the Launchpad.</p>   <p>Note</p> <p>Currently, the pipelines from all shared workspaces are visible when the visibility is set to \"Shared workspaces\".</p>  <p></p>","title":"Make shared pipelines visible in a private workspace"},{"location":"orgs-and-teams/workspace-management/","tags":["workspaces","teams","users","administration"],"text":"","title":"Workspace management"},{"location":"orgs-and-teams/workspace-management/#overview","tags":["workspaces","teams","users","administration"],"text":"<p>Organization workspaces extend the functionality of user workspaces by adding the ability to fine-tune access levels for specific members, collaborators, or teams. This is achieved by managing participants in the organization workspaces.</p> <p>Organizations consist of members, while workspaces consist of participants.</p>   <p>Note</p> <p>A workspace participant may be a member of the workspace organization or a collaborator within that workspace only. Collaborators count toward the total number of workspace participants. See Usage limits.</p>","title":"Overview"},{"location":"orgs-and-teams/workspace-management/#create-a-new-workspace","tags":["workspaces","teams","users","administration"],"text":"<p>Organization owners and admins can create a new workspace within an organization:</p> <ol> <li>Go to the Workspaces tab of the organization page.</li> <li>Select Add Workspace.</li> <li>Enter the Name and Full name for the workspace.</li> <li>Optionally, add a Description for the workspace.</li> <li>Under Visibility, select either Private or Shared. Private visibility means that workspace pipelines are only accessible to workspace participants.</li> <li>Select Add.</li> </ol>   <p>Tip</p> <p>Optional workspace fields can be modified after workspace creation, either by using the Edit option on the workspace listing for an organization or by accessing the Settings tab within the workspace page, provided that you are the Owner of the workspace.</p>  <p>Apart from the Participants tab, the organization workspace is similar to the user workspace. As such, the relation to runs, pipeline actions, compute environments and credentials is the same.</p>","title":"Create a new workspace"},{"location":"orgs-and-teams/workspace-management/#add-a-new-participant","tags":["workspaces","teams","users","administration"],"text":"<p>To add a new participant to a workspace:</p> <ol> <li>Go to the Participants tab in the workspace menu.</li> <li>Select Add participant.</li> <li>Enter the Name of the new participant.</li> <li>Optionally, update the participant role. For more information on roles, see participant roles.</li> </ol>   <p>Tip</p> <p>A new workspace participant can be an existing organization member, team, or collaborator.</p>","title":"Add a new participant"},{"location":"orgs-and-teams/workspace-management/#participant-roles","tags":["workspaces","teams","users","administration"],"text":"<p>Organization owners can assign role-based access levels to any of the workspace participants in an organization workspace.</p>   <p>Hint</p> <p>It is also possible to group members and collaborators into teams and apply a role to that team. Members and collaborators inherit the access role of the team.</p>  <p>There are five roles available for every workspace participant.</p> <ol> <li> <p>Owner: The participant has full permissions for all resources within the workspace, including the workspace settings.</p> </li> <li> <p>Admin: The participant has full permissions for resources associated with the workspace. They can create, modify, and delete pipelines, compute environments, actions, and credentials. They can add or remove users from the workspace but cannot access the workspace settings.</p> </li> <li> <p>Maintain: The participant can launch pipelines and modify pipeline executions (e.g., they can change the pipeline launch compute environments, parameters, pre/post-run scripts, and Nextflow configuration) and create new pipelines in the Launchpad. Users with maintain permissions cannot modify compute environments and credentials.</p> </li> <li> <p>Launch: The participant can launch pipelines and modify the pipeline input/output parameters in the Launchpad. They cannot modify the launch configuration or other resources.</p> </li> <li> <p>View: The participant can view workspace pipelines and runs in read-only mode.</p> </li> </ol>","title":"Participant roles"},{"location":"orgs-and-teams/workspace-management/#workspace-run-monitoring","tags":["workspaces","teams","users","administration"],"text":"<p>To allow users executing pipelines from the command-line to share their runs with a given workspace, see Getting started.</p>","title":"Workspace run monitoring"},{"location":"pipeline-actions/overview/","tags":["actions","webhooks","automation"],"text":"","title":"Pipeline actions"},{"location":"pipeline-actions/overview/#overview","tags":["actions","webhooks","automation"],"text":"<p>Pipeline actions allow launching of pipelines based on events.</p> <p>Tower currently offers support for native GitHub webhooks and a general Tower webhook that can be invoked programmatically. Support for Bitbucket and GitLab are coming soon.</p>","title":"Overview"},{"location":"pipeline-actions/overview/#github-webhooks","tags":["actions","webhooks","automation"],"text":"<p>A GitHub webhook listens for any changes made in the pipeline repository. When a change occurs, Tower triggers the launch of the pipeline automatically.</p> <p>To create a new Pipeline action, select the Actions tab and select Add Action.</p> <ol> <li> <p>Enter a Name for your Action.</p> </li> <li> <p>Select GitHub webhook as the Event source.</p> </li> </ol> <p></p> <ol> <li> <p>Select the Compute environment where the pipeline will be executed.</p> </li> <li> <p>Select the Pipeline to launch and (optionally) the Revision number.</p> </li> <li> <p>Enter the Work directory, the Config profiles, and the Pipeline parameters.</p> </li> <li> <p>Select Add.</p> </li> </ol> <p></p> <p>The pipeline action is now setup. When a new commit occurs for the selected repository and revision, an event will be triggered in Tower and the pipeline will be launched.</p> <p></p>","title":"GitHub webhooks"},{"location":"pipeline-actions/overview/#tower-launch-hooks","tags":["actions","webhooks","automation"],"text":"<p>A Tower launch hook creates a custom endpoint URL which can be used to trigger the execution of your pipeline programmatically from a script or web service.</p> <p>To create a new Pipeline action, select the Actions tab and select Add Action.</p> <ol> <li> <p>Enter a Name for your Action.</p> </li> <li> <p>Select Tower launch hook as the event source.</p> </li> </ol> <p></p> <ol> <li> <p>Select the Compute environment to execute your pipeline.</p> </li> <li> <p>Enter the Pipeline to launch and (optionally) the Revision number.</p> </li> <li> <p>Enter the Work directory, the Config profiles, and the Pipeline parameters.</p> </li> <li> <p>Select Add.</p> </li> </ol> <p></p> <p>The pipeline action has been created, and the new endpoint can be used to programmatically launch the corresponding pipeline. The snippet below shows an example <code>curl</code> command with the authentication token.</p> <p></p> <p>When you create a Tower launch hook, you also create an access token for launching pipelines through Tower. Access tokens can be managed on the tokens page, which is also accessible from the navigation menu.</p> <p></p>","title":"Tower launch hooks"},{"location":"pipeline-schema/overview/","tags":["pipeline","schema"],"text":"","title":"Pipeline schema"},{"location":"pipeline-schema/overview/#overview","tags":["pipeline","schema"],"text":"<p>Pipeline schema files describe the structure and validation constraints of your workflow parameters. They are used to validate parameters before launch to prevent software or pipelines from failing in unexpected ways at runtime.</p> <p>You can populate the parameters in the pipeline by uploading a YAML or JSON file, or in the Tower UI. Tower uses your pipeline schema to build a bespoke launchpad parameters form.</p> <p>See nf-core/rnaseq as an example of the pipeline parameters that can be represented by a JSON schema file.</p>","title":"Overview"},{"location":"pipeline-schema/overview/#building-pipeline-schema-files","tags":["pipeline","schema"],"text":"<p>The pipeline schema is based on json-schema.org syntax, with some additional conventions. While you can create your pipeline schema manually, we highly recommmend the use of nf-core tools, a toolset for developing Nextflow pipelines built by the nf-core community.</p> <p>When you run the <code>nf-core schema build</code> command in your pipeline root directory, the tool collects your pipeline parameters and gives you interactive prompts about missing or unexpected parameters. If no existing schema file is found, the tool creates one for you. <code>schema build</code> commands include the option to validate and lint your schema file according to best practice guidelines from the nf-core community.</p>","title":"Building pipeline schema files"},{"location":"pipeline-schema/overview/#customizing-pipeline-schema","tags":["pipeline","schema"],"text":"<p>Once the skeleton pipeline schema file has been built with <code>nf-core schema build</code>, the command line tool will prompt you to open a graphical schema editor on the nf-core website.</p> <p></p> <p>Leave the command-line tool running in the background - it checks the status of your schema on the website. When you select Finished on the schema editor page, your changes are saved to the schema file locally.</p>   <p>Note</p> <p>The schema builder is created by the nf-core community, but can be used any Nextflow pipeline.</p>","title":"Customizing pipeline schema"},{"location":"reports/overview/","tags":["pipeline","schema"],"text":"","title":"Pipeline reports"},{"location":"reports/overview/#overview","tags":["pipeline","schema"],"text":"<p>Most Nextflow pipelines will generate reports or output files which are useful to inspect at the end of the pipeline execution. Reports may be in various formats (e.g. HTML, PDF, TXT) and would typically contain quality control (QC) metrics that would be important to assess the integrity of the results. Tower has a Reports feature that allows you to directly visualise supported file types or to download them directly via the user interface (see Limitations). This saves users the time and effort from having to retrieve and visualise output files from their local storage.</p>","title":"Overview"},{"location":"reports/overview/#visualizing-reports","tags":["pipeline","schema"],"text":"<p>Available reports are listed in a Reports tab within the Runs page. Users can select a report from the table and open or download it (see Limitations for supported file types and sizes).</p> <p></p> <p>To open a report preview, the file must be smaller than 10MB.</p> <p></p> <p>Users can download a report directly from Tower or using the path. Download is not available if a report is larger than 25 MB. Option to download from path is suggested instead.</p> <p></p>","title":"Visualizing Reports"},{"location":"reports/overview/#providing-reports","tags":["pipeline","schema"],"text":"<p>To render reports users need to create a Tower config file that defines the paths to a selection of output files published by the pipeline. There are 2 ways users can provide the Tower config file both of which have to be in YAML format:</p> <ol> <li>Pipeline repository: If a file called tower.yml exists in the root of the pipeline repository then this will be fetched automatically before the pipeline execution.,</li> <li>Tower UI: Providing the YAML definition within the Advanced options &gt; Tower config file box when:    1.Creating a Pipeline in the Launchpad    2.Amending the Launch settings when launching a Pipeline. Users with Maintain role only.</li> </ol>   <p>Warning</p> <p>Any configuration provided in the Tower UI will completely override that which is supplied via the pipeline repository.</p>  <p></p>","title":"Providing Reports"},{"location":"reports/overview/#reports-implementation","tags":["pipeline","schema"],"text":"<p>Pipeline Reports need to be specified via YAML syntax</p> <pre>1\n2\n3\n4</pre><pre><code>reports:\n  &lt;path pattern&gt;:\n    display: text to display (required)\n    mimeType: file mime type (optional)\n</code></pre>","title":"Reports implementation"},{"location":"reports/overview/#path-pattern","tags":["pipeline","schema"],"text":"<p>Only the published files (using the Nextflow <code>publishDir</code> directive) are possible report candidates files. The path pattern is used to match published files to a report entry. It can be a partial path, a glob expression or just a file name.</p> <p>Examples of valid path patterns are:</p> <ul> <li><code>multiqc.html</code>: This will match all the published files with this name.</li> <li><code>**/multiqc.html</code>: This is a glob expression that matches any subfolder. It is equivalent to the previous expression.</li> <li><code>results/output.txt</code>: This will match all the <code>output.txt</code> files inside any results folder.</li> <li><code>*_output.tsv</code>: This will match any file that ends with \u201c_output.tsv\u201d</li> </ul>   <p>Warning</p> <p>When you use <code>*</code> it is important to also use double quotes, otherwise it is not a valid YAML.</p>","title":"Path pattern"},{"location":"reports/overview/#display","tags":["pipeline","schema"],"text":"<p>Display defines the title that will be shown on the website. If there are multiple files that match the same pattern an automatic suffix will be added. The suffix is the minimum difference between all the matching paths. For example given this report definition:</p> <pre>1\n2\n3</pre><pre><code>reports:\n  \"**/out/sheet.tsv\":\n    display: \"Data sheet\"\n</code></pre> <p>If you have these two paths <code>/workdir/sample1/out/sheet.tsv</code> and <code>/workdir/sample2/out/sheet.tsv</code> both of them will match the path pattern and their final display name will be Data sheet (sample1) and Data sheet (sample2).</p>","title":"Display"},{"location":"reports/overview/#mimetype","tags":["pipeline","schema"],"text":"<p>By default the mime type is deduced from the file extension, so in general you don\u2019t need to explicitly define it. Optionally, you can define it to force a viewer, for example showing a <code>txt</code> file as a <code>tsv</code>. It is important that it is a valid mime type text, otherwise it will be ignored and the extension will be used instead.</p>","title":"mimeType"},{"location":"reports/overview/#limitations","tags":["pipeline","schema"],"text":"<p>The current reports implementation limits the rendering to the following formats: HTML, CSV, tsv, pdf, and txt.</p> <p>In-page rendering/report preview is restricted to files smaller than 10MB to reduce the UI overload. Larger files need to be downloaded first.</p> <p>The download is restricted to files smaller than 25 MB to reduce the overload. Larger files need to be downloaded from the path.</p> <p>Currently, there is a YAML formatting validation in place checking both the tower.yml file inside the repository and the UI configuration box. The validation phase will emit an error message when users try to launch a pipeline with non-compliant YAML definitions.</p>","title":"Limitations"},{"location":"resource-labels/overview/","tags":["resource labels","labels"],"text":"","title":"Resource labels"},{"location":"resource-labels/overview/#overview","tags":["resource labels","labels"],"text":"<p>From version 22.3.0, Tower supports applying resource labels to compute environments and other Tower elements. This offers a flexible tagging system for annotation and tracking of the cloud services consumed by a run. Resource labels are sent to the service provider for each cloud compute environment in <code>key=value</code> format.</p> <p>Resource labels are applied to Tower elements during:</p> <ul> <li>compute environment creation with Forge</li> <li>submission</li> <li>and execution</li> </ul>","title":"Overview"},{"location":"resource-labels/overview/#create-and-apply-labels","tags":["resource labels","labels"],"text":"<p>Resource labels can be created, applied, and edited by a workspace admin or owner. When applying a label, users can select from existing labels or add new labels on the fly.</p>","title":"Create and apply labels"},{"location":"resource-labels/overview/#resource-labels-applied-to-a-compute-environment","tags":["resource labels","labels"],"text":"<p>Admins can assign a set of resource labels when creating a compute environment. All runs executed using the compute environment will be tagged with its resource labels. Resource labels applied to a compute environment are displayed on the compute environment details page.</p> <p>Apply a label when adding a new compute environment to the workspace.</p>   <p>Warning</p> <p>Once the compute environment has been created, its resource labels cannot be edited.</p>  <p>If a resource label is applied to a compute environment, all runs in that compute environment will inherit it. Likewise, all cloud resources generated during the workflow execution will be tagged with the same resource label.</p>","title":"Resource labels applied to a compute environment"},{"location":"resource-labels/overview/#resource-labels-applied-to-pipelines-actions-and-runs","tags":["resource labels","labels"],"text":"<p>Available from version 22.4.0</p> <p>Admins can override the default resource labels inherited from the compute environment when creating and editing pipelines, actions, and runs on the fly. The custom resource labels associated with each Tower element will propagate to the associated resources in the cloud environment without altering the default resource labels associated with the compute environment in Tower.</p> <p>When an admin adds or edits the resource labels associated with a pipeline, action, or run, the submission and execution time resource labels are altered. This does not affect the resource labels for resources spawned at (compute environment) creation time.</p> <p>For example, the resource label <code>name=ce1</code> is set during AWS Batch compute environment creation. If you create the resource label <code>pipeline=pipeline1</code> while creating a pipeline which uses the same AWS Batch compute environment, the EC2 instances associated with that compute environment still contain only the label <code>name=ce1</code>, while the Job Definitions associated with the pipeline will inherit the <code>pipeline=pipeline1</code> resource label.</p> <p>If a maintainer changes the compute environment associated with a pipeline or run, the resource labels field is updated with the resource labels from the new compute environment.</p>","title":"Resource labels applied to pipelines, actions, and runs"},{"location":"resource-labels/overview/#search-and-filter-with-labels","tags":["resource labels","labels"],"text":"<p>Search and filter pipelines and runs using one or more resource labels. The resource label search uses a <code>label:key=value</code> format.</p>","title":"Search and filter with labels"},{"location":"resource-labels/overview/#overview-of-resource-labels-in-a-workspace","tags":["resource labels","labels"],"text":"<p>All resource labels used in a workspace can be viewed in the workspace\u2019s Settings tab. Resource labels can only be edited or deleted by admins and only if they are not already associated with any Tower resource. This applies to resource labels associated with compute environments and runs. </p> <p>When you add or edit a resource label from the workspace settings menu, you can optionally set the \"Use as default in compute environment form\" option. Workspace default resource labels are prefilled in the resource labels field when creating a new compute environment in that workspace. </p> <p>The deletion of a resource label from a workspace has no influence on the cloud environment.</p>","title":"Overview of resource labels in a workspace"},{"location":"resource-labels/overview/#resource-label-propagation-to-cloud-environments","tags":["resource labels","labels"],"text":"<p>Note</p> <p>You cannot assign multiple resource labels, using the same key, to the same resource \u2014 regardless of whether this option is supported by the destination cloud provider.</p>  <p>Resource labels are only available for cloud environments that use a resource tagging system. Tower supports AWS, Google Life Sciences, Azure, and Kubernetes \u2014 HPC compute environments do not support resource labels.</p> <p>Note that the cloud provider credentials used by Tower must have the appropriate roles or permissions to tag resources in your environment.</p> <p>When a run is executed in a compute environment with associated resource labels, Tower propagates the labels to a set of resources (listed below), while Nextflow distributes the labels for the resources spawned at runtime.</p> <p>If the compute environment is created through Forge, the compute environment will propagate the tags to the resources generated by the Forge execution.</p>   <p>Warning</p> <p>Resource label propagation is one-way and not synchronized with the cloud environment. This means that Tower attaches tags to cloud resources, but is not aware if those tags are changed or deleted directly in the cloud environment.</p>","title":"Resource label propagation to cloud environments"},{"location":"resource-labels/overview/#aws","tags":["resource labels","labels"],"text":"<p>When the compute environment is created with Forge, the following resources will be tagged using the labels associated with the compute environment:</p> <p>Forge creation time</p> <ul> <li>FSX Filesystems (does not cascade to files)</li> <li>EFS Filesystems (does not cascade to files)</li> <li>Batch Compute Environment</li> <li>Batch Queue(s)</li> <li>ComputeResource (EC2 instances, excluding EBS volumes)</li> <li>Service Role</li> <li>Spot Fleet Role</li> <li>Execution Role</li> <li>Instance Profile Role</li> </ul> <p>Submission time</p> <ul> <li>Jobs and Job Definitions</li> <li>Tasks (via the propagateTags paramater on Job Definitions)</li> </ul> <p>Execution time</p> <ul> <li>Work Tasks (via the propagateTags paramater on Job Definitions)</li> </ul> <p>At execution time, when the jobs are submitted to Batch, the requests are set up to propagate tags to all the instances created by the head job.</p> <p>The <code>forge-policy.json</code> file contains the roles needed for Tower Forge-created AWS compute environments to tag AWS resources. Specifically, the required roles are <code>iam:TagRole</code>, <code>iam:TagInstanceProfile</code>, and <code>batch:TagResource</code>.</p> <p>To view and manage the resource labels applied to AWS resources by Tower and Nextflow, navigate to the AWS Tag Editor(as an administrative user) and follow these steps:</p> <ol> <li> <p>Under Find resources to tag, search for the resource label key and value in the relevant search fields under Tags. Your search can be further refined by AWS region and resource type. Then select Search resources.</p> </li> <li> <p>Resource search results displays all the resources tagged with your given resource label key and/or value.</p> </li> </ol> <p>To include the cost information associated with your resource labels in your AWS billing reports, follow these steps:</p> <ol> <li> <p>You need to activate the associated tags in the AWS Billing and Cost Management console. Note that newly-applied tags may take up to 24 hours to appear on your cost allocation tags page.</p> </li> <li> <p>Once your tags are activated and displayed on your Cost allocation tags page in the Billing and Cost Management console, you can apply those tags when creating cost allocation reports.</p> </li> </ol>","title":"AWS"},{"location":"resource-labels/overview/#aws-limits","tags":["resource labels","labels"],"text":"<ul> <li> <p>Resource label keys and values must contain a minimum of 2 and a maximum of 39 alphanumeric characters (each), separated by dashes or underscores.</p> </li> <li> <p>The key and value cannot begin or end with dashes <code>-</code> or underscores <code>_</code>.</p> </li> <li> <p>The key and value cannot contain a consecutive combination of <code>-</code> or <code>_</code> characters (<code>--</code>, <code>__</code>, <code>-_</code>, etc.)</p> </li> <li> <p>A maximum of 25 resource labels can be applied to each resource.</p> </li> <li> <p>A maximum of 100 resource labels can be used in each workspace.</p> </li> <li> <p>Keys and values cannot start with <code>aws</code> or <code>user</code>, as these are reserved prefixes appended to tags by AWS.</p> </li> <li> <p>Keys and values are case-sensitive in AWS.</p> </li> </ul> <p>See here for more information on AWS resource tagging.</p>","title":"AWS limits"},{"location":"resource-labels/overview/#google-batch-and-google-life-sciences","tags":["resource labels","labels"],"text":"<p>When the compute environment is created with Forge, the following resources will be tagged using the labels associated with the compute environment:</p> <p>Submission time</p> <ul> <li>Job (Batch)</li> <li>RunPipeline (Life Sciences)</li> </ul> <p>Execution time</p> <ul> <li>AllocationPolicy (Batch)</li> <li>VirtualMachine (Life Sciences)</li> <li>RunPipeline (Life Sciences)</li> </ul>","title":"Google Batch and Google Life Sciences"},{"location":"resource-labels/overview/#gcp-limits","tags":["resource labels","labels"],"text":"<ul> <li> <p>Resource label keys and values must contain a minimum of 2 and a maximum of 39 alphanumeric characters (each), separated by dashes or underscores.</p> </li> <li> <p>The key and value cannot begin or end with dashes <code>-</code> or underscores <code>_</code>.</p> </li> <li> <p>The key and value cannot contain a consecutive combination of <code>-</code> or <code>_</code> characters (<code>--</code>, <code>__</code>, <code>-_</code>, etc.)</p> </li> <li> <p>A maximum of 25 resource labels can be applied to each resource.</p> </li> <li> <p>A maximum of 100 resource labels can be used in each workspace.</p> </li> <li> <p>Keys and values in Google Cloud Resource Manager may contain only lowercase letters. Resource labels created with uppercase characters in Tower are changed to lowercase before propagating to Google Cloud.</p> </li> </ul> <p>See here for more information on Google Cloud Resource Manager labeling.</p>","title":"GCP limits"},{"location":"resource-labels/overview/#azure","tags":["resource labels","labels"],"text":"<p>Note</p> <p>The labeling system on Azure Cloud uses the term metadata to refer to resource and other labels</p>  <p>When creating an Azure Compute Environment through Forge, resource labels are added to the Pool parameters \u2014 this will add a set of <code>key=value</code> metadata pairs to the Azure Batch Pool.</p>","title":"Azure"},{"location":"resource-labels/overview/#azure-limits","tags":["resource labels","labels"],"text":"<ul> <li> <p>Resource label keys and values must contain a minimum of 2 and a maximum of 39 alphanumeric characters (each), separated by dashes or underscores.</p> </li> <li> <p>The key and value cannot begin or end with dashes <code>-</code> or underscores <code>_</code>.</p> </li> <li> <p>The key and value cannot contain a consecutive combination of <code>-</code> or <code>_</code> characters (<code>--</code>, <code>__</code>, <code>-_</code>, etc.)</p> </li> <li> <p>A maximum of 25 resource labels can be applied to each resource.</p> </li> <li> <p>A maximum of 100 resource labels can be used in each workspace.</p> </li> <li> <p>Keys are case-insensitive, but values are case-sensitive.</p> </li> <li> <p>Microsoft advises against using a non-English language in your resource labels, as this can lead to decoding progress failure while loading your VM's metadata.</p> </li> </ul> <p>See here for more information on Azure Resource Manager tagging.</p>","title":"Azure limits"},{"location":"resource-labels/overview/#kubernetes","tags":["resource labels","labels"],"text":"<p>Both the Head pod and Work pod specs will contain the set of labels associated with the compute environment in addition to the standard labels applied by Tower and Nextflow.</p>   <p>Warning</p> <p>Currently, tagging with resource labels is not available for the files created during a workflow execution. The cloud instances are the elements being tagged.</p>  <p>The following resources will be tagged using the labels associated with the compute environment:</p> <p>Forge creation time</p> <ul> <li>Deployment</li> <li>PodTemplate</li> </ul> <p>Submission time</p> <ul> <li>Head Pod Metadata</li> </ul> <p>Execution time</p> <ul> <li>Run Pod Metadata</li> </ul>","title":"Kubernetes"},{"location":"resource-labels/overview/#kubernetes-limits","tags":["resource labels","labels"],"text":"<ul> <li> <p>Resource label keys and values must contain a minimum of 2 and a maximum of 39 alphanumeric characters (each), separated by dashes or underscores.</p> </li> <li> <p>The key and value cannot begin or end with dashes <code>-</code> or underscores <code>_</code>.</p> </li> <li> <p>The key and value cannot contain a consecutive combination of <code>-</code> or <code>_</code> characters (<code>--</code>, <code>__</code>, <code>-_</code>, etc.)</p> </li> <li> <p>A maximum of 25 resource labels can be applied to each resource.</p> </li> <li> <p>A maximum of 100 resource labels can be used in each workspace.</p> </li> </ul> <p>See here for more information on Kubernetes object labeling.</p>","title":"Kubernetes limits"},{"location":"secrets/overview/","tags":["pipeline","secrets"],"text":"","title":"Pipeline secrets"},{"location":"secrets/overview/#overview","tags":["pipeline","secrets"],"text":"<p>Tower uses the concept of Secrets to store the keys and tokens used by workflow tasks to interact with external systems e.g. a password to connect to an external database or an API token. Tower relies on third-party secret manager services in order to maintain security between the workflow execution context and the secret container. This means that no secure data is transmitted from Tower to the Compute Environment.</p>   <p>Note</p> <p>Currently only AWS Batch or HPC batch schedulers are supported. Please read more about the AWS Secret Manager here</p>","title":"Overview"},{"location":"secrets/overview/#pipeline-secrets","tags":["pipeline","secrets"],"text":"<p>To create a Pipeline Secret navigate to a Workspace (private or shared) and click on the Secrets tab in the top navigation pane to gain access to the Secrets management interface.</p> <p></p> <p>All of the available Secrets will be listed here and users with the appropriate permissions (maintainer, admin or owner) will be able to create or update Secret values.</p> <p></p> <p>The form for creating or updating a Secret is very similar to the one used for Credentials.</p> <p></p>","title":"Pipeline Secrets"},{"location":"secrets/overview/#pipeline-secrets-for-users","tags":["pipeline","secrets"],"text":"<p>Secrets can be defined for users by clicking on your avatar in the top right corner of the Tower interface and selecting \"Your Secrets\". Listing, creating and updating Secrets for users is the same as Secrets in a Workspace. However, Secrets defined by a user have a higher priority and will override any Secrets defined in a Workspace with the same name.</p> <p></p>   <p>Warning</p> <p>Secrets defined by a user have higher priority and will override any Secrets defined in a Workspace with the same name.</p>","title":"Pipeline Secrets for users"},{"location":"secrets/overview/#using-secrets-in-workflows","tags":["pipeline","secrets"],"text":"<p>When a new workflow is launched, all Secrets are sent to the corresponding secret manager for the Compute Environment. Nextflow will download these Secrets internally and use them when they are referenced in the pipeline code as described in the Nextflow Secrets documentation.</p> <p>Secrets will be automatically deleted from the secret manager when the Pipeline completes (successful or unsuccessful).</p>","title":"Using Secrets in workflows"},{"location":"secrets/overview/#aws-secrets-manager-integration","tags":["pipeline","secrets"],"text":"<p>If you are planning to use the Pipeline Secrets feature provided by Tower with the AWS Secrets Manager, the following IAM permissions should be provided:</p> <ol> <li> <p>Create the AWS Batch IAM Execution role as specified in the AWS documentation.</p> </li> <li> <p>Add the <code>AmazonECSTaskExecutionRolePolicy</code> policy and this custom policy to the execution role created above.</p> </li> <li> <p>Specify the execution role ARN in the Batch execution role option (under Advanced options) when creating your Compute Environment in Tower.</p> </li> <li> <p>Add this custom policy to the ECS Instance role associated with the Batch compute environment that will be used to deploy your pipelines. Replace <code>YOUR-ACCOUNT</code> and <code>YOUR-EXECUTION-ROLE-NAME</code> with the appropriate values. See here for more details about the Instance role.</p> </li> <li> <p>Add this custom policy to your Tower IAM user (the one specified in the Tower credentials).</p> </li> </ol>","title":"AWS Secrets Manager Integration"},{"location":"supported_software/dragen/overview/","tags":["dragen","integration"],"text":"","title":"Illumina DRAGEN"},{"location":"supported_software/dragen/overview/#illumina-dragen","tags":["dragen","integration"],"text":"<p>DRAGEN is a platform provided by Illumina that offers accurate, comprehensive, and efficient secondary analysis of next-generation sequencing (NGS) data with a significant speed-up over tools that are commonly used for such tasks.</p> <p>The improved performance offered by DRAGEN is possible due to the use of Illumina proprietary algorithms in conjunction with a special type of hardware accelerator called field programmable gate arrays (FPGAs). For example, when using AWS, FPGAs are available via the F1 instance type.</p>","title":"Illumina DRAGEN"},{"location":"supported_software/dragen/overview/#running-dragen-on-nextflow-tower","tags":["dragen","integration"],"text":"<p>We have extended the Tower Forge feature for AWS Batch to support DRAGEN. Tower Forge ensures that all of the appropriate components and settings are automatically provisioned when creating a Compute Environment for executing pipelines.</p> <p>When deploying data analysis workflows, some tasks will need to use normal instance types (e.g. for non-DRAGEN processing of samples) and others will need to be executed on F1 instances. If the DRAGEN feature is enabled, Tower Forge will create an additional AWS Batch compute queue which only uses F1 instances, to which DRAGEN tasks will be dispatched.</p>","title":"Running DRAGEN on Nextflow Tower"},{"location":"supported_software/dragen/overview/#getting-started","tags":["dragen","integration"],"text":"<p>To showcase the capability of this integration, we have implemented a proof of concept pipeline called nf-dragen. To run it, sign-in into Tower, navigate to the Community Showcase and select the \u201cnf-dragen\u201d pipeline.</p> <p>You can run this pipeline at your convenience without any extra setup. Note however that it will be deployed in the Compute Environment owned by the Community Showcase.</p> <p>To deploy the pipeline on your own AWS cloud infrastructure, please follow the instructions in the next section.</p>","title":"Getting started"},{"location":"supported_software/dragen/overview/#deploy-dragen-in-your-own-workspace","tags":["dragen","integration"],"text":"<p>DRAGEN is a commercial technology provided by Illumina, so you will need to purchase a license from them. To run on Tower, you will need to obtain the following information from Illumina:</p> <ol> <li>DRAGEN AWS private AMI ID</li> <li>DRAGEN license username</li> <li>DRAGEN license password</li> </ol> <p>Tower Forge automates most of the tasks required to set up an AWS Batch Compute Environment. Please follow our guide for more details.</p> <p>In order to enable support for DRAGEN acceleration, simply toggle the \u201cEnable DRAGEN\u201d option when setting up the Compute Environment via Tower Forge.</p> <p>In the \u201cDRAGEN AMI Id\u201d field, enter the AWS AMI ID provided to you by Illumina.</p> <p></p>   <p>Warning</p> <p>Please ensure that the Region you select contains DRAGEN F1 instances.</p>","title":"Deploy DRAGEN in your own workspace"},{"location":"supported_software/dragen/overview/#pipeline-implementation-deployment","tags":["dragen","integration"],"text":"<p>Please see the dragen.nf module implemented in the nf-dragen pipeline for reference. Any Nextflow processes that run DRAGEN must:</p> <ol> <li>Define <code>label \u2018dragen\u2019</code></li> </ol> <p>The <code>label</code> directive allows you to annotate a process with mnemonic identifiers of your choice. Tower will use the <code>dragen</code> label to determine which processes need to be executed on DRAGEN F1 instances.</p> <pre>1\n2\n3\n4\n5</pre><pre><code>process DRAGEN {\n    label 'dragen'\n\n    &lt;truncated&gt;\n}\n</code></pre> <p>Please refer to the Nextflow label docs for more information.</p> <ol> <li>Define Secrets</li> </ol> <p>At Seqera, we use Secrets to safely encrypt sensitive information when running licensed software via Nextflow. This enables our team to use the DRAGEN software safely via the <code>nf-dragen</code> pipeline without having to worry about the setup or safe configuration of the license key. These Secrets will be provided securely to the <code>--lic-server</code> option when running DRAGEN on the CLI to validate the license.</p> <p>In the nf-dragen pipeline, we have defined two Secrets called <code>DRAGEN_USERNAME</code> and <code>DRAGEN_PASSWORD</code>, which you can add via the Tower UI by going to \u201cSecrets -&gt; Add Pipeline Secret\u201d:</p> <p></p> <p></p> <p>Please refer to the Secrets documentation for more information about this feature.</p>","title":"Pipeline implementation &amp; deployment"},{"location":"supported_software/dragen/overview/#limitations","tags":["dragen","integration"],"text":"<p>DRAGEN integration with Tower is currently only available for use on AWS, however, we plan to extend the functionality to other supported platforms like Azure in the future.</p>","title":"Limitations"},{"location":"supported_software/fusion/fusion/","text":"","title":"Fusion file system"},{"location":"supported_software/fusion/fusion/#fusion-v2-file-system","text":"<p>Tower 22.4 adds official support for the Fusion v2 file system. </p> <p>Fusion v2 is a lightweight container-based client that enables containerized tasks to access data in Amazon S3 buckets using POSIX file access semantics. Depending on your data handling requirements, Fusion can improve pipeline throughput, which should reduce cloud computing costs. See here for more information on Fusion's features. </p> <p>Fusion relies on the Wave containers service. When Fusion v2 and Wave containers is enabled on a Tower compute environment, the corresponding configuration values are added to your Nextflow config file when running pipelines with that compute environment:</p> <pre>1\n2\n3\n4\n5\n6\n7</pre><pre><code>wave {\n  enabled = true\n    }\n\nfusion {\n  enabled = true\n    }\n</code></pre>","title":"Fusion v2 file system"},{"location":"supported_software/fusion/fusion/#fusion-performance-and-cost-considerations","text":"<p>Fusion v2 improves pipeline throughput for containerized tasks by simplifying direct access to cloud data storage. Dedicated networking and fast I/O influence pipeline performance and are important to consider when selecting compute instances.</p>","title":"Fusion performance and cost considerations"},{"location":"supported_software/fusion/fusion/#configure-tower-compute-environments-with-fusion","text":"","title":"Configure Tower compute environments with Fusion"},{"location":"supported_software/fusion/fusion/#tower-forge-aws-batch-compute-environments-with-fast-instance-storage-recommended","text":"<p>We recommend using Fusion with AWS NVMe instances (fast instance storage) as this delivers the fastest performance when compared to environments using only AWS EBS (Elastic Block Store).</p> <ol> <li>For this configuration, use Tower version 23.1 or later. </li> <li>Create an AWS Batch compute environment.</li> <li>Enable Wave containers, Fusion v2, and fast instance storage. </li> <li>Select the Batch Forge config mode.</li> <li> <p>Select your Instance types under Advanced options:</p> <ul> <li>If left unspecified, Tower will select the following NVMe-based instance type families: <code>'c5ad', 'c5d', 'c6id', 'i3', 'i4i', 'm5ad', 'm5d', 'm6id', 'r5ad', 'r5d', 'r6id'</code></li> <li>To specify an NVMe instance family type, select from the following: <ul> <li>Intel: <code>'c5ad', 'c5d', 'c6id', 'dl1', 'f1', 'g4ad', 'g4dn', 'g5', 'i3', 'i3en', 'i4i', 'm5ad', 'm5d', 'm5dn', 'm6id', 'p3dn', 'p4d', 'p4de', 'r5ad', 'r5d', 'r5dn', 'r6id', 'x2idn', 'x2iedn', 'z1d'</code></li> <li>ARM: <code>'c6gd', 'm6gd', 'r6gd', 'x2gd', 'im4gn', 'is4gen'</code></li> </ul> </li> </ul>  <p>Optimal instance type families will not work with fast instance storage</p> <p>When enabling fast instance storage, do not select the <code>optimal</code> instance type families (c4, m4, r4) for your compute environment as these are not NVMe-based instances. Specify the NVMe instance types listed above.</p>   <p>Tip</p> <p>We recommend selecting 8xlarge or above for large and long-lived production pipelines. Dedicated networking ensures a guaranteed network speed service level compared with \"burstable\" instances. See Instance network bandwidth for more information. </p>  </li> <li> <p>Use an S3 bucket as the pipeline work directory. </p> </li> </ol>","title":"Tower Forge AWS Batch compute environments with fast instance storage (recommended)"},{"location":"supported_software/fusion/fusion/#tower-forge-aws-compute-environments-with-fusion-only","text":"<p>To use Fusion in AWS environments without NVMe instances, enable Wave containers and Fusion v2 when creating a new compute environment without enabling fast instance storage. This option configures an AWS EBS (Elastic Bucket Store) disk with settings optimized for the Fusion file system. </p> <ol> <li>For this configuration, use Tower version 23.1 or later. </li> <li>Create an AWS Batch compute environment. </li> <li>Enable Wave containers and Fusion v2. </li> <li>Select the Batch Forge config mode.</li> <li> <p>When you enable Fusion v2 without fast instance storage, the following settings are applied:</p> <ul> <li><code>process.scratch = false</code> is added to the Nextflow configuration file</li> <li>EBS autoscaling is disabled</li> <li>EBS boot disk size is increased to 100GB</li> <li>EBS boot disk type is changed to GP3</li> <li>EBS boot disk throughput is increased to 325MB/s</li> </ul> </li> <li> <p>Use an S3 bucket as the pipeline work directory. </p> </li> </ol>","title":"Tower Forge AWS compute environments with Fusion only"},{"location":"supported_software/fusion/fusion/#fusion-in-manual-compute-environments","text":"<p>Tower supports Fusion v2 for both Batch Forge and manual compute environments on AWS Batch, and for Google Cloud Batch compute environments. Detailed instructions for configuration in manual AWS and Google Cloud Batch compute environments will be available soon. In the meantime, we highly recommend using Fusion v2 in Batch Forge compute environments with the default settings described above. </p>","title":"Fusion in manual compute environments"}]}